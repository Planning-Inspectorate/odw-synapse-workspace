from odw.core.io.data_io import DataIO
from pyspark.sql import DataFrame


class SynapseDataIO(DataIO):
    """
    Manages data io to/from a storage location that is linked to Synapse
    """
    def _format_to_adls_path(self, container_name: str, blob_path: str, storage_name: str = None, storage_endpoint: str = None) -> str:
        """
        Return a datalake path from the given arguments.

        **Note this function exists so that the filepath can be mocked during testing**
        
        :param str container_name: The storage container the blob exists in
        :param str blob_path: The path to the blob in the container
        :param str storage_name: The name of the storage account
        :param str storage_endpoint: The endpoint of the storage account, of the form `mystorageaccount.dfs.core.windows.net/`
        :return str: A string with the format `abfss://{container_name}@{storage_name}.dfs.core.windows.net/{blob_path}`
        """
        if not (storage_name or storage_endpoint):
            raise ValueError(f"SynapseDataIO._format_to_adls_path expected one of 'storage_name' or 'storage_endpoint' to be provided")
        if storage_name and storage_endpoint:
            raise ValueError(f"SynapseDataIO._format_to_adls_path expected only one of 'storage_name' or 'storage_endpoint' to be provided, not both")
        if storage_name:
            storage_endpoint = f"{storage_name}.dfs.core.windows.net/"
        storage_endpoint_split = storage_endpoint.split(".")
        if len(storage_endpoint_split) != 5:
            raise ValueError(
                f"The storage endpoint '{storage_endpoint}' generated by SynapseDataIO._format_to_adls_path does not "
                "conform to the format 'name.kind.core.windows.net/'"
            )
        if not storage_endpoint.endswith("/"):
            storage_endpoint += "/"
        return f"abfss://{container_name}@{storage_endpoint}{blob_path}"

    def read(self, **kwargs) -> DataFrame:
        """
        Read from the given storage location, and return the data as a pyspark DataFrame

        :param str storage_name: The name of the storage account to read from
        :param str container_name: The container to read from
        :param str blob_path: The path to the blob (in the container) to read
        :param SparkSession spark: The spark session
        
        :return DataFrame: The data 
        """
        pass
    
    def write(self, data: DataFrame, **kwargs):
        """
        Write the data to the given storage location
        
        :param DataFrame data: The data to write
        :param str storage_name: The name of the storage account to write to
        :param str container_name: The container to write to
        :param str blob_path: The path to the blob (in the container) to write
        :param SparkSession spark: The spark session
        """
        pass
