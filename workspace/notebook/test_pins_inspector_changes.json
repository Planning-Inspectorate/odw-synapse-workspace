{
	"name": "test_pins_inspector_changes",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {},
				"source": [
					"## Test Notebook: Add/Update/Delete Test Records\n",
					"\n",
					"This notebook helps test the `py_get_delta_table_changes` functionality by:\n",
					"1. Adding a new test record\n",
					"2. Updating an existing test record\n",
					"3. Deleting a test record\n",
					"\n",
					"**IMPORTANT**: Run ONE cell at a time, then run `py_get_delta_table_changes` to see the detected changes."
				]
			},
			{
				"cell_type": "code",
				"metadata": {},
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from delta.tables import DeltaTable\n",
					"\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"table_name = \"odw_curated_db.pins_inspector\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {},
				"source": [
					"### Test 1: Insert a New Test Record\n",
					"Run this cell to add a new inspector with sapId 'TEST99999'"
				]
			},
			{
				"cell_type": "code",
				"metadata": {},
				"source": [
					"# Create a test record with the same schema as pins_inspector\n",
					"test_record = [\n",
					"    {\n",
					"        \"entraId\": \"test-entra-id-12345\",\n",
					"        \"sapId\": \"TEST99999\",\n",
					"        \"firstName\": \"TestFirstName\",\n",
					"        \"lastName\": \"TestLastName\",\n",
					"        \"emailAddress\": \"test.inspector@planninginspectorate.gov.uk\",\n",
					"        \"grade\": \"APO\",\n",
					"        \"fte\": 1.0,\n",
					"        \"unit\": \"Test Unit\",\n",
					"        \"service\": \"Test Service\",\n",
					"        \"group\": \"Test Group\",\n",
					"        \"inspectorManager\": \"Test Manager\",\n",
					"        \"title\": \"Test Inspector\",\n",
					"        \"address\": {\n",
					"            \"addressLine1\": \"123 Test Street\",\n",
					"            \"addressLine2\": \"Test Building\",\n",
					"            \"townCity\": \"Test City\",\n",
					"            \"county\": \"Test County\",\n",
					"            \"postcode\": \"TE5T 1NG\"\n",
					"        },\n",
					"        \"specialism\": \"Test Specialism\",\n",
					"        \"validFrom\": \"2026-01-16\"\n",
					"    }\n",
					"]\n",
					"\n",
					"# Create DataFrame with proper schema\n",
					"address_schema = StructType([\n",
					"    StructField(\"addressLine1\", StringType(), True),\n",
					"    StructField(\"addressLine2\", StringType(), True),\n",
					"    StructField(\"townCity\", StringType(), True),\n",
					"    StructField(\"county\", StringType(), True),\n",
					"    StructField(\"postcode\", StringType(), True)\n",
					"])\n",
					"\n",
					"test_df = spark.createDataFrame(test_record)\n",
					"\n",
					"# Read current table\n",
					"current_df = spark.table(table_name)\n",
					"\n",
					"# Append the test record\n",
					"updated_df = current_df.union(test_df)\n",
					"\n",
					"# Write back (using overwrite to create a new version)\n",
					"updated_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
					"\n",
					"print(f\"✅ Added test record with sapId: TEST99999\")\n",
					"print(f\"Total records now: {updated_df.count()}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {},
				"source": [
					"### Test 2: Update the Test Record\n",
					"Run this cell to update the test record (change the firstName)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {},
				"source": [
					"# Read current table\n",
					"current_df = spark.table(table_name)\n",
					"\n",
					"# Update the test record - change firstName\n",
					"updated_df = current_df.withColumn(\n",
					"    \"firstName\",\n",
					"    when(col(\"sapId\") == \"TEST99999\", lit(\"UpdatedFirstName\"))\n",
					"    .otherwise(col(\"firstName\"))\n",
					")\n",
					"\n",
					"# Write back\n",
					"updated_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
					"\n",
					"print(f\"✅ Updated test record with sapId: TEST99999\")\n",
					"print(f\"Changed firstName from 'TestFirstName' to 'UpdatedFirstName'\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {},
				"source": [
					"### Test 3: Delete the Test Record\n",
					"Run this cell to remove the test record"
				]
			},
			{
				"cell_type": "code",
				"metadata": {},
				"source": [
					"# Read current table\n",
					"current_df = spark.table(table_name)\n",
					"\n",
					"# Remove the test record\n",
					"cleaned_df = current_df.filter(col(\"sapId\") != \"TEST99999\")\n",
					"\n",
					"# Write back\n",
					"cleaned_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
					"\n",
					"print(f\"✅ Deleted test record with sapId: TEST99999\")\n",
					"print(f\"Total records now: {cleaned_df.count()}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {},
				"source": [
					"### Verify: Check Delta History\n",
					"Run this to see the version history of the table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {},
				"source": [
					"from delta.tables import DeltaTable\n",
					"\n",
					"delta_table = DeltaTable.forName(spark, table_name)\n",
					"history_df = delta_table.history(10)  # Last 10 versions\n",
					"\n",
					"display(history_df.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {},
				"source": [
					"### Enable Change Data Feed (Optional)\n",
					"Run this to enable CDF for more efficient change tracking"
				]
			},
			{
				"cell_type": "code",
				"metadata": {},
				"source": [
					"spark.sql(f\"ALTER TABLE {table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
					"print(f\"✅ Change Data Feed enabled on {table_name}\")\n",
					"print(\"Future changes will be tracked more efficiently!\")"
				],
				"execution_count": null
			}
		]
	}
}
