{
	"name": "py_work_schedule_rule",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c6e36aa4-5a23-437d-b2e4-f23b077e80df"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table Work Schedule Rule for Absence Data.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;27-July-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Work Schedule Rule in here;"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"deleted_count\": 0,\n",
					"    \"inserted_count\": 0,\n",
					"    \"storage_account\": \"\",\n",
					"    \"operation_type\": \"work_schedule_rule_processing\",\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    logInfo(\"Starting Work Schedule Rule processing\")\n",
					"    \n",
					"    storage_account = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"    result[\"storage_account\"] = storage_account.rstrip('/')\n",
					"    logInfo(f\"Using storage account: {storage_account}\")\n",
					"    \n",
					"    logInfo(\"Validating source table: odw_standardised_db.workschedulerule_weekly\")\n",
					"    if not spark.catalog.tableExists(\"odw_standardised_db.workschedulerule_weekly\"):\n",
					"        raise Exception(\"Source table odw_standardised_db.workschedulerule_weekly does not exist\")\n",
					"    \n",
					"    logInfo(\"Validating target table: odw_harmonised_db.sap_hr_workschedulerule\")\n",
					"    if not spark.catalog.tableExists(\"odw_harmonised_db.sap_hr_workschedulerule\"):\n",
					"        raise Exception(\"Target table odw_harmonised_db.sap_hr_workschedulerule does not exist\")\n",
					"    \n",
					"    logInfo(\"Getting current record count before processing\")\n",
					"    current_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_workschedulerule\").collect()[0][0]\n",
					"    result[\"deleted_count\"] = current_count\n",
					"    logInfo(f\"Current records in target table: {current_count}\")\n",
					"    \n",
					"    logInfo(\"Validating source data quality\")\n",
					"    source_count = spark.sql(\"SELECT COUNT(*) FROM odw_standardised_db.workschedulerule_weekly\").collect()[0][0]\n",
					"    logInfo(f\"Source records available: {source_count}\")\n",
					"    \n",
					"    if source_count == 0:\n",
					"        logInfo(\"Warning: No source data found - proceeding with empty dataset\")\n",
					"    \n",
					"    logInfo(\"Starting main work schedule rule processing operation\")\n",
					"    spark.sql(\"DELETE FROM odw_harmonised_db.sap_hr_workschedulerule\")\n",
					"    logInfo(f\"Successfully deleted {current_count} existing records\")\n",
					"    \n",
					"    logInfo(\"Executing complex work schedule rule parsing and insertion\")\n",
					"    \n",
					"    # Split into dataframe operations to avoid SQL escaping issues\n",
					"    from pyspark.sql import functions as F\n",
					"    \n",
					"    # Read source data\n",
					"    df = spark.table(\"odw_standardised_db.workschedulerule_weekly\")\n",
					"    \n",
					"    # Normalize description\n",
					"    df = df.withColumn(\"clean_desc\", \n",
					"        F.regexp_replace(\n",
					"            F.regexp_replace(\n",
					"                F.regexp_replace(\n",
					"                    F.regexp_replace(\n",
					"                        F.regexp_replace(F.lower(F.col(\"wsr_description\")), \n",
					"                            'monday|mon\\\\.?', 'mon'),\n",
					"                        'tuesday|tue\\\\.?', 'tue'),\n",
					"                    'wednesday|wed\\\\.?', 'wed'),\n",
					"                'thursday|thu\\\\.?|thus', 'thu'),\n",
					"            'friday|fri\\\\.?', 'fri'))\n",
					"    \n",
					"    # Extract week sections - need to use expr for dynamic positions\n",
					"    df = df.withColumn(\"lower_desc\", F.lower(F.col(\"wsr_description\")))\n",
					"    df = df.withColumn(\"week2_pos\", \n",
					"        F.when(F.locate('week 2', F.col(\"lower_desc\")) > 0, F.locate('week 2', F.col(\"lower_desc\")))\n",
					"        .when(F.locate('wk 2', F.col(\"lower_desc\")) > 0, F.locate('wk 2', F.col(\"lower_desc\")))\n",
					"        .otherwise(F.lit(0)))\n",
					"    \n",
					"    df = df.withColumn(\"week1_section\",\n",
					"        F.when(F.col(\"week2_pos\") > 0,\n",
					"            F.trim(F.expr(\"substring(lower_desc, 1, week2_pos - 1)\")))\n",
					"        .otherwise(F.col(\"lower_desc\")))\n",
					"    \n",
					"    df = df.withColumn(\"week2_section\",\n",
					"        F.when(F.col(\"week2_pos\") > 0,\n",
					"            F.trim(F.expr(\"substring(lower_desc, week2_pos, length(lower_desc))\")))\n",
					"        .otherwise(F.col(\"lower_desc\")))\n",
					"    \n",
					"    def parse_day_hours(section_col, day_patterns, range_patterns):\n",
					"        col = F.col(section_col)\n",
					"        result = F.lit(0.0)\n",
					"        \n",
					"        all_patterns = range_patterns + day_patterns\n",
					"        \n",
					"        for pattern in all_patterns:\n",
					"            regex_with_and = f'{pattern}[^0-9]*(\\\\d+)\\\\s*hours?\\\\s+and\\\\s+(\\\\d+)\\\\s*min'\n",
					"            regex_with_min = f'{pattern}[^0-9]*(\\\\d+)\\\\s*hours?\\\\s+(\\\\d+)\\\\s*min'\n",
					"            regex_only_hours = f'{pattern}[^0-9]*(\\\\d+)\\\\s*hours?(?:\\\\s|$|[^0-9])'\n",
					"            \n",
					"            has_and = col.rlike(regex_with_and)\n",
					"            has_minutes = col.rlike(regex_with_min)\n",
					"            \n",
					"            result = F.when(\n",
					"                has_and,\n",
					"                F.regexp_extract(col, regex_with_and, 1).cast('double') +\n",
					"                (F.regexp_extract(col, regex_with_and, 2).cast('double') / 60.0)\n",
					"            ).when(\n",
					"                has_minutes,\n",
					"                F.regexp_extract(col, regex_with_min, 1).cast('double') +\n",
					"                (F.regexp_extract(col, regex_with_min, 2).cast('double') / 60.0)\n",
					"            ).when(\n",
					"                col.rlike(regex_only_hours),\n",
					"                F.regexp_extract(col, regex_only_hours, 1).cast('double')\n",
					"            ).otherwise(result)\n",
					"        \n",
					"        return result\n",
					"    \n",
					"    mon_ranges = [\n",
					"        'monday\\\\s*-\\\\s*friday', 'monday\\\\s*-\\\\s*thursday', 'monday\\\\s*-\\\\s*wednesday', 'monday\\\\s*-\\\\s*tuesday',\n",
					"        'mon\\\\s*-\\\\s*fri', 'mon\\\\s*-\\\\s*thu', 'mon\\\\s*-\\\\s*wed', 'mon\\\\s*-\\\\s*tue'\n",
					"    ]\n",
					"    tue_ranges = [\n",
					"        'tuesday\\\\s*-\\\\s*friday', 'tuesday\\\\s*-\\\\s*thursday', 'tuesday\\\\s*-\\\\s*wednesday',\n",
					"        'monday\\\\s*-\\\\s*friday', 'monday\\\\s*-\\\\s*thursday', 'monday\\\\s*-\\\\s*wednesday', 'monday\\\\s*-\\\\s*tuesday',\n",
					"        'tue\\\\s*-\\\\s*fri', 'tue\\\\s*-\\\\s*thu', 'tue\\\\s*-\\\\s*wed',\n",
					"        'mon\\\\s*-\\\\s*fri', 'mon\\\\s*-\\\\s*thu', 'mon\\\\s*-\\\\s*wed', 'mon\\\\s*-\\\\s*tue'\n",
					"    ]\n",
					"    wed_ranges = [\n",
					"        'wednesday\\\\s*-\\\\s*friday', 'wednesday\\\\s*-\\\\s*thursday',\n",
					"        'tuesday\\\\s*-\\\\s*friday', 'tuesday\\\\s*-\\\\s*thursday', 'tuesday\\\\s*-\\\\s*wednesday',\n",
					"        'monday\\\\s*-\\\\s*friday', 'monday\\\\s*-\\\\s*thursday', 'monday\\\\s*-\\\\s*wednesday',\n",
					"        'wed\\\\s*-\\\\s*fri', 'wed\\\\s*-\\\\s*thu',\n",
					"        'tue\\\\s*-\\\\s*fri', 'tue\\\\s*-\\\\s*thu', 'tue\\\\s*-\\\\s*wed',\n",
					"        'mon\\\\s*-\\\\s*fri', 'mon\\\\s*-\\\\s*thu', 'mon\\\\s*-\\\\s*wed'\n",
					"    ]\n",
					"    thu_ranges = [\n",
					"        'thursday\\\\s*-\\\\s*friday',\n",
					"        'wednesday\\\\s*-\\\\s*friday', 'wednesday\\\\s*-\\\\s*thursday',\n",
					"        'tuesday\\\\s*-\\\\s*friday', 'tuesday\\\\s*-\\\\s*thursday',\n",
					"        'monday\\\\s*-\\\\s*friday', 'monday\\\\s*-\\\\s*thursday',\n",
					"        'thu\\\\s*-\\\\s*fri',\n",
					"        'wed\\\\s*-\\\\s*fri', 'wed\\\\s*-\\\\s*thu',\n",
					"        'tue\\\\s*-\\\\s*fri', 'tue\\\\s*-\\\\s*thu',\n",
					"        'mon\\\\s*-\\\\s*fri', 'mon\\\\s*-\\\\s*thu'\n",
					"    ]\n",
					"    fri_ranges = [\n",
					"        'thursday\\\\s*-\\\\s*friday', 'wednesday\\\\s*-\\\\s*friday', 'tuesday\\\\s*-\\\\s*friday', 'monday\\\\s*-\\\\s*friday',\n",
					"        'thu\\\\s*-\\\\s*fri', 'wed\\\\s*-\\\\s*fri', 'tue\\\\s*-\\\\s*fri', 'mon\\\\s*-\\\\s*fri'\n",
					"    ]\n",
					"    \n",
					"    # Parse Week 1\n",
					"    df = df.withColumn(\"mo_wk1\", parse_day_hours(\"week1_section\", ['monday', 'mon'], mon_ranges))\n",
					"    df = df.withColumn(\"tu_wk1\", parse_day_hours(\"week1_section\", ['tuesday', 'tue'], tue_ranges))\n",
					"    df = df.withColumn(\"we_wk1\", parse_day_hours(\"week1_section\", ['wednesday', 'wed'], wed_ranges))\n",
					"    df = df.withColumn(\"th_wk1\", parse_day_hours(\"week1_section\", ['thursday', 'thu'], thu_ranges))\n",
					"    df = df.withColumn(\"fr_wk1\", parse_day_hours(\"week1_section\", ['friday', 'fri'], fri_ranges))\n",
					"    \n",
					"    # Parse Week 2\n",
					"    df = df.withColumn(\"mo_wk2\", parse_day_hours(\"week2_section\", ['monday', 'mon'], mon_ranges))\n",
					"    df = df.withColumn(\"tu_wk2\", parse_day_hours(\"week2_section\", ['tuesday', 'tue'], tue_ranges))\n",
					"    df = df.withColumn(\"we_wk2\", parse_day_hours(\"week2_section\", ['wednesday', 'wed'], wed_ranges))\n",
					"    df = df.withColumn(\"th_wk2\", parse_day_hours(\"week2_section\", ['thursday', 'thu'], thu_ranges))\n",
					"    df = df.withColumn(\"fr_wk2\", parse_day_hours(\"week2_section\", ['friday', 'fri'], fri_ranges))\n",
					"    \n",
					"    # Calculate totals - keep as double for now\n",
					"    df = df.withColumn(\"WkHrsWk1_calc\", F.round(F.col(\"mo_wk1\") + F.col(\"tu_wk1\") + F.col(\"we_wk1\") + F.col(\"th_wk1\") + F.col(\"fr_wk1\"), 2))\n",
					"    df = df.withColumn(\"WkHrsWk2_calc\", F.round(F.col(\"mo_wk2\") + F.col(\"tu_wk2\") + F.col(\"we_wk2\") + F.col(\"th_wk2\") + F.col(\"fr_wk2\"), 2))\n",
					"    df = df.withColumn(\"AvgWkHrs_calc\", F.round((F.col(\"WkHrsWk1_calc\") + F.col(\"WkHrsWk2_calc\")) / 2, 2))\n",
					"    \n",
					"    # Date conversion - handle DD/MM/YYYY format\n",
					"    df = df.withColumn(\"date_parts\",\n",
					"        F.when(F.col(\"wsr_startdate\").isNotNull(),\n",
					"            F.split(F.col(\"wsr_startdate\"), '/')\n",
					"        ).otherwise(F.array()))\n",
					"    \n",
					"    df = df.withColumn(\"WSRstart\",\n",
					"        F.when(F.size(F.col(\"date_parts\")) == 3,\n",
					"            F.to_date(\n",
					"                F.concat(\n",
					"                    F.col(\"date_parts\")[2], F.lit('-'),\n",
					"                    F.lpad(F.col(\"date_parts\")[1], 2, '0'), F.lit('-'),\n",
					"                    F.lpad(F.col(\"date_parts\")[0], 2, '0')\n",
					"                ), 'yyyy-MM-dd'\n",
					"            )\n",
					"        ).otherwise(F.lit(None)))\n",
					"    \n",
					"    # Calculate current week\n",
					"    df = df.withColumn(\"Currentweek\",\n",
					"        F.when(F.col(\"WSRstart\").isNotNull(),\n",
					"            F.when((F.datediff(F.current_date(), F.col(\"WSRstart\")) / 7 % 2) == 0, 1).otherwise(2)\n",
					"        ).otherwise(1))\n",
					"    \n",
					"    # Get target table schema to match data types\n",
					"    target_schema = spark.table(\"odw_harmonised_db.sap_hr_workschedulerule\").schema\n",
					"    \n",
					"    # Build a mapping of column names to types\n",
					"    target_types = {field.name: field.dataType.simpleString() for field in target_schema}\n",
					"    \n",
					"    logInfo(f\"Target schema types: {target_types}\")\n",
					"    \n",
					"    # Convert Lastmodified properly (DD/MM/YYYY to timestamp)\n",
					"    df = df.withColumn(\"date_parts_last\",\n",
					"        F.when(F.col(\"wsr_lastmodified\").isNotNull(),\n",
					"            F.split(F.col(\"wsr_lastmodified\"), '/')\n",
					"        ).otherwise(F.array()))\n",
					"    \n",
					"    df = df.withColumn(\"Lastmodified_converted\",\n",
					"        F.when(F.size(F.col(\"date_parts_last\")) == 3,\n",
					"            F.to_timestamp(\n",
					"                F.concat(\n",
					"                    F.col(\"date_parts_last\")[2], F.lit('-'),\n",
					"                    F.lpad(F.col(\"date_parts_last\")[1], 2, '0'), F.lit('-'),\n",
					"                    F.lpad(F.col(\"date_parts_last\")[0], 2, '0'),\n",
					"                    F.lit(' 00:00:00')\n",
					"                ), 'yyyy-MM-dd HH:mm:ss'\n",
					"            )\n",
					"        ).otherwise(F.lit(None).cast(\"timestamp\")))\n",
					"    \n",
					"    # Select final columns with proper type casting based on target schema\n",
					"    final_df = df.select(\n",
					"        F.col(\"wsr_name\").alias(\"WorkScheduleRule\"),\n",
					"        F.col(\"wsr_code\").alias(\"WorkScheduleCode\"),\n",
					"        F.round(F.col(\"mo_wk1\"), 2).cast(target_types.get(\"MoWk1\", \"string\")).alias(\"MoWk1\"),\n",
					"        F.round(F.col(\"tu_wk1\"), 2).cast(target_types.get(\"TuWk1\", \"string\")).alias(\"TuWk1\"),\n",
					"        F.round(F.col(\"we_wk1\"), 2).cast(target_types.get(\"WeWk1\", \"string\")).alias(\"WeWk1\"),\n",
					"        F.round(F.col(\"th_wk1\"), 2).cast(target_types.get(\"ThWk1\", \"string\")).alias(\"ThWk1\"),\n",
					"        F.round(F.col(\"fr_wk1\"), 2).cast(target_types.get(\"FrWk1\", \"string\")).alias(\"FrWk1\"),\n",
					"        F.round(F.col(\"mo_wk2\"), 2).cast(target_types.get(\"MoWk2\", \"string\")).alias(\"MoWk2\"),\n",
					"        F.round(F.col(\"tu_wk2\"), 2).cast(target_types.get(\"TuWk2\", \"string\")).alias(\"TuWk2\"),\n",
					"        F.round(F.col(\"we_wk2\"), 2).cast(target_types.get(\"WeWk2\", \"string\")).alias(\"WeWk2\"),\n",
					"        F.round(F.col(\"th_wk2\"), 2).cast(target_types.get(\"ThWk2\", \"string\")).alias(\"ThWk2\"),\n",
					"        F.round(F.col(\"fr_wk2\"), 2).cast(target_types.get(\"FrWk2\", \"string\")).alias(\"FrWk2\"),\n",
					"        F.col(\"WkHrsWk1_calc\").cast(target_types.get(\"WkHrsWk1\", \"double\")).alias(\"WkHrsWk1\"),\n",
					"        F.col(\"WkHrsWk2_calc\").cast(target_types.get(\"WkHrsWk2\", \"double\")).alias(\"WkHrsWk2\"),\n",
					"        F.col(\"AvgWkHrs_calc\").cast(target_types.get(\"AvgWkHrs\", \"double\")).alias(\"AvgWkHrs\"),\n",
					"        F.col(\"WSRstart\"),\n",
					"        F.col(\"Currentweek\").cast(target_types.get(\"Currentweek\", \"string\")),\n",
					"        F.col(\"Lastmodified_converted\").alias(\"Lastmodified\"),\n",
					"        F.coalesce(F.col(\"ingested_datetime\"), F.current_timestamp()).alias(\"IngestionDate\")\n",
					"    )\n",
					"    \n",
					"    # Write to target table using SQL INSERT to handle schema properly\n",
					"    final_df.createOrReplaceTempView(\"temp_work_schedule_data\")\n",
					"    \n",
					"    spark.sql(\"\"\"\n",
					"        INSERT INTO odw_harmonised_db.sap_hr_workschedulerule\n",
					"        SELECT * FROM temp_work_schedule_data\n",
					"    \"\"\")\n",
					"    \n",
					"    final_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_workschedulerule\").collect()[0][0]\n",
					"    result[\"record_count\"] = final_count\n",
					"    result[\"inserted_count\"] = final_count\n",
					"    logInfo(f\"Successfully inserted {final_count} records\")\n",
					"    \n",
					"    logInfo(\"Performing data quality validation\")\n",
					"    valid_rules_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_workschedulerule WHERE WorkScheduleRule IS NOT NULL AND TRIM(WorkScheduleRule) != ''\").collect()[0][0]\n",
					"    parsed_hours_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_workschedulerule WHERE CAST(COALESCE(MoWk1, '0') AS DOUBLE) + CAST(COALESCE(TuWk1, '0') AS DOUBLE) + CAST(COALESCE(WeWk1, '0') AS DOUBLE) + CAST(COALESCE(ThWk1, '0') AS DOUBLE) + CAST(COALESCE(FrWk1, '0') AS DOUBLE) + CAST(COALESCE(MoWk2, '0') AS DOUBLE) + CAST(COALESCE(TuWk2, '0') AS DOUBLE) + CAST(COALESCE(WeWk2, '0') AS DOUBLE) + CAST(COALESCE(ThWk2, '0') AS DOUBLE) + CAST(COALESCE(FrWk2, '0') AS DOUBLE) > 0\").collect()[0][0]\n",
					"    avg_hours_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_workschedulerule WHERE CAST(COALESCE(AvgWkHrs, '0') AS DOUBLE) > 0 AND CAST(COALESCE(AvgWkHrs, '0') AS DOUBLE) <= 80\").collect()[0][0]\n",
					"    \n",
					"    result[\"quality_metrics\"] = {\n",
					"        \"valid_rules\": valid_rules_count,\n",
					"        \"parsed_hours\": parsed_hours_count,\n",
					"        \"reasonable_avg_hours\": avg_hours_count,\n",
					"        \"quality_percentage\": round((parsed_hours_count / final_count * 100), 2) if final_count > 0 else 0\n",
					"    }\n",
					"    \n",
					"    logInfo(f\"Data quality: {result['quality_metrics']['quality_percentage']}% of records have parsed working hours\")\n",
					"    logInfo(\"Work Schedule Rule processing completed successfully\")\n",
					"    \n",
					"    # Create minimal exit value output\n",
					"    minimal_result = {\n",
					"        \"status\": result[\"status\"],\n",
					"        \"record_count\": result[\"record_count\"],\n",
					"        \"deleted_count\": result[\"deleted_count\"],\n",
					"        \"inserted_count\": result[\"inserted_count\"],\n",
					"        \"error_message\": result[\"error_message\"]\n",
					"    }\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in Work Schedule Rule processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = error_msg[:300]\n",
					"    result[\"record_count\"] = -1\n",
					"    result[\"deleted_count\"] = -1 \n",
					"    result[\"inserted_count\"] = -1\n",
					"    result[\"operation_type\"] = \"failed\"\n",
					"    try:\n",
					"        current_count_after_error = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_workschedulerule\").collect()[0][0]\n",
					"        result[\"records_after_error\"] = current_count_after_error\n",
					"    except:\n",
					"        result[\"records_after_error\"] = \"unknown\"\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output minimal result - only the 4 key fields\n",
					"    if result[\"status\"] == \"success\":\n",
					"        mssparkutils.notebook.exit(json.dumps(minimal_result, indent=2))\n",
					"    else:\n",
					"        mssparkutils.notebook.exit(json.dumps(result, indent=2))"
				],
				"execution_count": null
			}
		]
	}
}