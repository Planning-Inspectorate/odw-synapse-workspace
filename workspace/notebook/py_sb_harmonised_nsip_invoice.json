{
	"name": "py_sb_harmonised_nsip_invoice",
	"properties": {
		"description": "Process service bus data for nsip_meeting table from sb_nsip_project with nested meeting data",
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "54541486-6852-486b-a269-70f7ce9d6186"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to read service bus data from odw_harmonised_db.sb_nsip_project Delta table and process nested meeting data to odw_harmonised_db.nsip_meeting Delta table.\n",
					"\n",
					"**Description** \n",
					"The purpose of this pyspark notebook is to read service bus data with nested meeting arrays from odw_harmonised_db.sb_nsip_project Delta table and explode/transform them into odw_harmonised_db.nsip_meeting Delta table based on the existing MiPINS business logic.\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version - 3.10, Delta Lake Version - 2.4\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## update all exceptions with appropriate comments"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Import required libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.window import Window\n",
					"from datetime import datetime\n",
					"from delta.tables import DeltaTable\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import explode, col"
				],
				"execution_count": 128
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define required delta tables and variables"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 124
			},
			{
				"cell_type": "code",
				"source": [
					"# Configurable variables\n",
					"service_bus_table = \"odw_harmonised_db.sb_nsip_project\"\n",
					"db_name=\"odw_harmonised_db\"\n",
					"table_name=\"nsip_invoice\"\n",
					"spark_table_final = \"odw_harmonised_db.nsip_invoice\"\n",
					"incremental_key = \"NSIPInvoiceID\"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": 130
			},
			{
				"cell_type": "code",
				"source": [
					"# storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"# metadata_path: str = \"abfss://odw-config@\"+storage_account+\"existing-tables-metadata_invoice.json\"\n",
					"# df  = spark.read.json(metadata_path, multiLine=True)\n",
					"# harmonised_df = df.select(explode(col(\"harmonised_metadata\")).alias(\"metadata\"))\n",
					"# print(harmonised_df)\n",
					"#     print(db_name)\n",
					"#     print(table_name)\n",
					"#     filtered_df = df_exploded.filter(\n",
					"#         (col(\"database_name\") == db_name) & (col(\"table_name\") == table_name)\n",
					"#     ).select(\"table_location\", \"table_type\", \"table_format\")\n",
					"#     print(filtered_df)"
				],
				"execution_count": 131
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# from pyspark.sql.functions import explode, col\n",
					"\n",
					"# # Get storage account name\n",
					"# storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"\n",
					"# # Build metadata path\n",
					"# metadata_path: str = f\"abfss://odw-config@{storage_account}/existing-tables-metadata_invoice.json\"\n",
					"# print(metadata_path)\n",
					"\n",
					"# # Read JSON file\n",
					"# df = spark.read.json(metadata_path, multiLine=True)\n",
					"\n",
					"# # Explode harmonised_metadata\n",
					"# harmonised_df = df.select(explode(col(\"harmonised_metadata\")).alias(\"metadata\"))\n",
					"\n",
					"# # Show exploded data\n",
					"# # harmonised_df.show(truncate=False)\n",
					"\n",
					"# # Example: filter by db_name and table_name\n",
					"# db_name = \"odw_harmonised_db\"\n",
					"# table_name = \"nsip_invoice\"\n",
					"\n",
					"# # Flatten metadata for filtering\n",
					"# df_exploded = harmonised_df.select(\n",
					"#     col(\"metadata.database_name\").alias(\"database_name\"),\n",
					"#     col(\"metadata.table_name\").alias(\"table_name\"),\n",
					"#     col(\"metadata.table_location\").alias(\"table_location\"),\n",
					"#     col(\"metadata.table_type\").alias(\"table_type\"),\n",
					"#     col(\"metadata.table_format\").alias(\"table_format\")\n",
					"# )\n",
					"# print(df_exploded)\n",
					"\n",
					"# # Filter for specific table\n",
					"# filtered_df = df_exploded.filter(\n",
					"#     (col(\"database_name\") == db_name) & (col(\"table_name\") == table_name)\n",
					"# ).select(\"table_location\", \"table_type\", \"table_format\")\n",
					"# print(filtered_df)\n",
					"\n",
					"# # Show filtered result\n",
					"# filtered_df.show(truncate=False)\n",
					""
				],
				"execution_count": 140
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"try:\n",
					"    # Check if table exists\n",
					"    if spark.catalog.tableExists(spark_table_final):\n",
					"        print(f\"Table {spark_table_final} exists.\")\n",
					"\n",
					"        # Load table schema\n",
					"        existing_df = spark.table(spark_table_final)\n",
					"        existing_cols = existing_df.columns\n",
					"\n",
					"        # Check for NSIPProjectInfoInternalID\n",
					"        if \"NSIPProjectInfoInternalID\" not in existing_cols:\n",
					"            print(\"Column NSIPProjectInfoInternalID not found. Adding as LongType...\")\n",
					"            spark.sql(f\"ALTER TABLE {spark_table_final} ADD COLUMNS (NSIPProjectInfoInternalID BIGINT)\")\n",
					"        else:\n",
					"            print(\"Column NSIPProjectInfoInternalID exists.\")\n",
					"    else:\n",
					"        print(f\"Table {spark_table_final} does not exist. Please create it first.\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_message = f\"Error during schema check/update: {str(e)[:800]}\"\n",
					"    end_exec_time = datetime.now()\n",
					"    print(error_message)\n",
					""
				],
				"execution_count": 141
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.types import *\n",
					"from datetime import datetime\n",
					"\n",
					"# Initialize Spark session\n",
					"spark = SparkSession.builder.appName(\"SyncDeltaWithSQLServer\").getOrCreate()\n",
					"\n",
					"# Configurations\n",
					"jdbc_url = \"jdbc:sqlserver://<server>;databaseName=odw_harmonised_db\"\n",
					"jdbc_user = \"<username>\"\n",
					"jdbc_password = \"<password>\"\n",
					"sql_table = \"dbo.nsip_invoice\"\n",
					"spark_table_final = \"odw_harmonised_db.nsip_invoice\"  # Delta table name\n",
					"\n",
					"try:\n",
					"    start_exec_time = datetime.now()\n",
					"    print(f\"Starting sync at {start_exec_time}\")\n",
					"\n",
					"    # Step 1: Read SQL Server table schema and data\n",
					"    query = f\"(SELECT * FROM {sql_table}) AS src\"\n",
					"    sql_df = (\n",
					"        spark.read.format(\"jdbc\")\n",
					"        .option(\"url\", jdbc_url)\n",
					"        .option(\"query\", query)\n",
					"        .option(\"user\", jdbc_user)\n",
					"        .option(\"password\", jdbc_password)\n",
					"        .load()\n",
					"    )\n",
					"    sql_cols = sql_df.columns\n",
					"    print(f\"SQL Server columns: {sql_cols}\")\n",
					"\n",
					"    # Step 2: Check if Delta table exists\n",
					"    if spark.catalog.tableExists(spark_table_final):\n",
					"        print(f\"Delta table {spark_table_final} exists.\")\n",
					"        delta_df = spark.table(spark_table_final)\n",
					"        delta_cols = delta_df.columns\n",
					"        print(f\"Delta table columns: {delta_cols}\")\n",
					"\n",
					"        # Step 3: Find missing columns in Delta\n",
					"        missing_cols = [col for col in sql_cols if col not in delta_cols]\n",
					"        extra_cols = [col for col in delta_cols if col not in sql_cols]\n",
					"\n",
					"        # Add missing columns\n",
					"        for col in missing_cols:\n",
					"            col_type = [f.dataType.simpleString() for f in sql_df.schema.fields if f.name == col][0]\n",
					"            print(f\"Adding missing column: {col} ({col_type})\")\n",
					"            spark.sql(f\"ALTER TABLE {spark_table_final} ADD COLUMNS ({col} {col_type.upper()})\")\n",
					"\n",
					"        # Drop extra columns (like message_id)\n",
					"        if extra_cols:\n",
					"            print(f\"Dropping extra columns: {extra_cols}\")\n",
					"            spark.sql(f\"\"\"\n",
					"                ALTER TABLE {spark_table_final} SET TBLPROPERTIES (\n",
					"                    'delta.columnMapping.mode' = 'name',\n",
					"                    'delta.minReaderVersion' = '2',\n",
					"                    'delta.minWriterVersion' = '5'\n",
					"                )\n",
					"            \"\"\")\n",
					"            for col in extra_cols:\n",
					"                spark.sql(f\"ALTER TABLE {spark_table_final} DROP COLUMN {col}\")\n",
					"\n",
					"        print(\"Schema sync completed.\")\n",
					"    else:\n",
					"        print(f\"Delta table {spark_table_final} does not exist. Creating...\")\n",
					"        sql_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(spark_table_final)\n",
					"        print(f\"Delta table {spark_table_final} created successfully.\")\n",
					"\n",
					"    end_exec_time = datetime.now()\n",
					"    print(f\"Sync completed at {end_exec_time}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_message = f\"Error during sync: {str(e)[:800]}\"\n",
					"    print(error_message)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"if not error_message:  # Only execute if there was no previous error\n",
					"    try:\n",
					"        # Step 1: Get max IngestionDate from existing table\n",
					"        existing_df = spark.table(spark_table_final)\n",
					"        max_IngestionDate_to = existing_df.agg(F.max(\"IngestionDate\")).collect()[0][0]\n",
					"\n",
					"        if max_IngestionDate_to is None:\n",
					"            # Set default date if no records exist\n",
					"            max_IngestionDate_to = datetime(1900, 1, 1)  # or any baseline date you prefer\n",
					"\n",
					"        print(f\"✅ Max IngestionDate from existing table: {max_IngestionDate_to}\")\n",
					"\n",
					"    except Exception as e:\n",
					"        error_message = f\"Error while fetching max IngestionDate: {str(e)[:800]}\"\n",
					"        max_IngestionDate_to = datetime(1900, 1, 1)  # fallback default\n",
					"        end_exec_time = datetime.now()\n",
					"        print(error_message)"
				],
				"execution_count": 113
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"if not error_message:\n",
					"    try:\n",
					"        # Step 2: Get new data from service bus table\n",
					"        service_bus_data = spark.sql(f\"\"\"\n",
					"            SELECT \n",
					"                NSIPProjectInfoInternalID,\n",
					"                caseId,\n",
					"                caseReference,\n",
					"                invoices,\n",
					"                ODTSourceSystem,\n",
					"                SourceSystemID,\n",
					"                IngestionDate\n",
					"            FROM {service_bus_table}\n",
					"            WHERE invoices IS NOT NULL\n",
					"        \"\"\")\n",
					"\n",
					"        print(\"✅ Service bus data loaded successfully.\")\n",
					"\n",
					"    except Exception as e:\n",
					"        error_message = f\"Error while loading service bus data: {str(e)[:800]}\"\n",
					"        print(error_message)\n",
					""
				],
				"execution_count": 114
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"if not error_message:  # Only run if no previous error\n",
					"    try:\n",
					"        # Step 3: Explode invoices array\n",
					"        exploded_df = service_bus_data.select(\n",
					"            F.col(\"NSIPProjectInfoInternalID\"),\n",
					"            F.col(\"caseId\"),\n",
					"            F.col(\"caseReference\"),\n",
					"            F.explode(F.col(\"invoices\")).alias(\"invoice\"),\n",
					"            F.col(\"ODTSourceSystem\"),\n",
					"            F.col(\"SourceSystemID\"),\n",
					"            F.col(\"IngestionDate\")\n",
					"        )\n",
					"\n",
					"        # Step 4: Extract invoice fields\n",
					"        new_data = exploded_df.select(\n",
					"            F.col(\"NSIPProjectInfoInternalID\"),\n",
					"            F.col(\"caseId\"),\n",
					"            F.col(\"caseReference\"),\n",
					"            F.col(\"invoice.invoiceStage\").alias(\"invoiceStage\"),\n",
					"            F.col(\"invoice.invoiceNumber\").alias(\"invoiceNumber\"),\n",
					"            F.col(\"invoice.amountDue\").alias(\"amountDue\"),\n",
					"            F.col(\"invoice.paymentDueDate\").alias(\"paymentDueDate\"),\n",
					"            F.col(\"invoice.invoicedDate\").alias(\"invoicedDate\"),\n",
					"            F.col(\"invoice.paymentDate\").alias(\"paymentDate\"),\n",
					"            F.col(\"invoice.refundCreditNoteNumber\").alias(\"refundCreditNoteNumber\"),\n",
					"            F.col(\"invoice.refundAmount\").alias(\"refundAmount\"),\n",
					"            F.col(\"invoice.refundIssueDate\").alias(\"refundIssueDate\"),\n",
					"            F.col(\"ODTSourceSystem\"),\n",
					"            F.col(\"SourceSystemID\"),\n",
					"            F.col(\"IngestionDate\")\n",
					"        )\n",
					"\n",
					"        print(\"Explode and extract completed successfully.\")\n",
					"\n",
					"    except Exception as e:\n",
					"        error_message = f\"Error during explode/extract: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					"        print(error_message)\n",
					""
				],
				"execution_count": 115
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Step 1: Get max ID from existing table\n",
					"        existing_df = spark.table(spark_table_final)\n",
					"        max_id_row = existing_df.agg(F.max(F.col(incremental_key))).collect()[0][0]\n",
					"        if max_id_row is None:\n",
					"            max_id_row = 0  # Default if table is empty\n",
					"\n",
					"        # Step 2: Filter only records newer than max IngestionDate\n",
					"        if max_IngestionDate_to:\n",
					"            new_data = new_data.filter(F.col(\"IngestionDate\") > F.lit(max_IngestionDate_to))\n",
					"        else:\n",
					"            default_date = datetime(1900, 1, 1)\n",
					"            new_data = new_data.filter(F.col(\"IngestionDate\") > F.lit(default_date))\n",
					"\n",
					"        # Step 3: Drop old IngestionDate column\n",
					"        new_data = new_data.drop(\"IngestionDate\")\n",
					"\n",
					"        # Step 4: Add new IngestionDate column with current timestamp (formatted)\n",
					"        new_data = new_data.withColumn(\"IngestionDate\", F.date_format(F.current_timestamp(), \"yyyy-MM-dd HH:mm:ss\"))\n",
					"\n",
					"        # Step 5: Add extra columns\n",
					"        new_data = (\n",
					"            new_data.withColumn(\"ValidTo\", F.lit(None).cast(\"string\"))\n",
					"                    .withColumn(\"RowID\", F.lit(None).cast(\"string\"))\n",
					"                    .withColumn(\"IsActive\", F.lit(\"Y\").cast(\"string\"))  # Default IsActive = 'Y'\n",
					"        )\n",
					"\n",
					"        # Step 6: Add surrogate key starting from max_id_row + 1\n",
					"        window_spec = Window.orderBy(F.monotonically_increasing_id())\n",
					"        new_data = new_data.withColumn(\n",
					"            incremental_key,\n",
					"            (F.row_number().over(window_spec) + F.lit(max_id_row)).cast(\"long\")\n",
					"        )\n",
					"\n",
					"        # Step 7: Reorder columns so incremental_key is first\n",
					"        cols = new_data.columns\n",
					"        reordered_cols = [incremental_key] + [c for c in cols if c != incremental_key]\n",
					"        new_data = new_data.select(reordered_cols)\n",
					"\n",
					"        # Display final DataFrame\n",
					"        display(new_data)\n",
					"        print(\"Columns added and reordered successfully.\")\n",
					"\n",
					"    except Exception as e:\n",
					"        error_message = f\"Error during processing: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					"        print(f\"Error occurred at: {end_exec_time}\")\n",
					"        print(error_message)\n",
					""
				],
				"execution_count": 116
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Load Delta table\n",
					"        target_table = DeltaTable.forName(spark, spark_table_final)\n",
					"\n",
					"        # Step 1: Read existing data\n",
					"        df = spark.table(spark_table_final)\n",
					"\n",
					"        # Step 2: Window to determine latest record\n",
					"        window_spec = Window.partitionBy(\"caseId\", \"invoiceNumber\").orderBy(F.col(\"IngestionDate\").desc())\n",
					"\n",
					"        # Add Migrated column\n",
					"        df = df.withColumn(\n",
					"            \"Migrated\",\n",
					"            F.when(F.col(\"caseId\").isNotNull() & F.col(\"invoiceNumber\").isNotNull(), F.lit(1)).otherwise(F.lit(0))\n",
					"        )\n",
					"\n",
					"        # Add NewIsActive column (latest record = 'Y', others = 'N')\n",
					"        df = df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
					"        df = df.withColumn(\"NewIsActive\", F.when(F.col(\"row_num\") == 1, F.lit(\"Y\")).otherwise(F.lit(\"N\")))\n",
					"\n",
					"        # Add RowID column using MD5 hash\n",
					"        df = df.withColumn(\n",
					"            \"RowID\",\n",
					"            F.md5(\n",
					"                F.concat_ws(\n",
					"                    \"\",\n",
					"                    F.coalesce(F.col(\"NSIPInvoiceID\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"NSIPProjectInfoInternalID\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"caseId\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"caseReference\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"invoiceStage\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"invoiceNumber\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"amountDue\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"paymentDueDate\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"invoicedDate\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"paymentDate\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"refundCreditNoteNumber\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"refundAmount\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"refundIssueDate\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"Migrated\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"ODTSourceSystem\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"SourceSystemID\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"IngestionDate\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"ValidTo\").cast(\"string\"), F.lit(\".\")),\n",
					"                    F.coalesce(F.col(\"NewIsActive\").cast(\"string\"), F.lit(\".\"))\n",
					"                )\n",
					"            )\n",
					"        )\n",
					"\n",
					"        display(df)\n",
					"\n",
					"        # Step 3: Merge updates back into Delta table\n",
					"        target_table.alias(\"t\").merge(\n",
					"            df.alias(\"s\"),\n",
					"            \"t.caseId = s.caseId AND t.invoiceNumber = s.invoiceNumber AND t.IngestionDate = s.IngestionDate\"\n",
					"        ).whenMatchedUpdate(\n",
					"            set={\n",
					"                \"IsActive\": \"s.NewIsActive\",\n",
					"                \"ValidTo\": F.expr(\"CASE WHEN s.NewIsActive = 'N' THEN current_timestamp() ELSE t.ValidTo END\"),\n",
					"                \"Migrated\": \"s.Migrated\",\n",
					"                \"RowID\": \"s.RowID\"\n",
					"            }\n",
					"        ).execute()\n",
					"\n",
					"        print(\"✅ MERGE completed successfully. IsActive, ValidTo, Migrated, and RowID updated.\")\n",
					"\n",
					"    except Exception as e:\n",
					"        error_message = f\"⚠️ Error during MERGE operation: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					"        print(error_message)\n",
					""
				],
				"execution_count": 117
			},
			{
				"cell_type": "code",
				"source": [
					"# duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"# activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"# stage = \"Success\" if not error_message else \"Failed\"\n",
					"# status_message = (\n",
					"#     f\"Successfully loaded data into {spark_table_final} table\"\n",
					"#     if not error_message\n",
					"#     else f\"Failed to load data from {spark_table_final} table\"\n",
					"# )\n",
					"# status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"# log_telemetry_and_exit(\n",
					"#     stage,\n",
					"#     start_exec_time,\n",
					"#     end_exec_time,\n",
					"#     error_message,\n",
					"#     spark_table_final,\n",
					"#     insert_count,\n",
					"#     update_count,\n",
					"#     delete_count,\n",
					"#     PipelineName,\n",
					"#     PipelineRunID,\n",
					"#     PipelineTriggerID,\n",
					"#     PipelineTriggerName,\n",
					"#     PipelineTriggerType,\n",
					"#     PipelineTriggeredbyPipelineName,\n",
					"#     PipelineTriggeredbyPipelineRunID,\n",
					"#     activity_type,\n",
					"#     duration_seconds,\n",
					"#     status_message,\n",
					"#     status_code\n",
					"# )"
				],
				"execution_count": 121
			}
		]
	}
}