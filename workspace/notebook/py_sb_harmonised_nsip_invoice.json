{
	"name": "py_sb_harmonised_nsip_invoice",
	"properties": {
		"description": "Process service bus data for nsip_meeting table from sb_nsip_project with nested meeting data",
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "001885ec-a198-43a4-aac3-ba6bfea4b6cd"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to read service bus data from odw_harmonised_db.sb_nsip_project Delta table and process nested meeting data to odw_harmonised_db.nsip_meeting Delta table.\n",
					"\n",
					"**Description** \n",
					"The purpose of this pyspark notebook is to read service bus data with nested meeting arrays from odw_harmonised_db.sb_nsip_project Delta table and explode/transform them into odw_harmonised_db.nsip_meeting Delta table based on the existing MiPINS business logic.\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version - 3.10, Delta Lake Version - 2.4\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## update all exceptions with appropriate comments"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Import required libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.window import Window\n",
					"from datetime import datetime\n",
					"from delta.tables import DeltaTable"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					},
					"collapsed": false
				},
				"source": [
					"%%pyspark\n",
					"df = spark.sql(\"SELECT * FROM `odw_harmonised_db`.`nsip_invoice`\")\n",
					"display(df)"
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define required delta tables and variables"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"# Configurable variables\n",
					"service_bus_table = \"odw_harmonised_db.sb_nsip_project\"\n",
					"spark_table_final = \"odw_harmonised_db.nsip_invoice\"\n",
					"incremental_key = \"NSIPInvoiceID\"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"try:\n",
					"    # Check if table exists\n",
					"    if spark.catalog.tableExists(spark_table_final):\n",
					"        print(f\"✅ Table {spark_table_final} exists.\")\n",
					"\n",
					"        # Load table schema\n",
					"        existing_df = spark.table(spark_table_final)\n",
					"        existing_cols = existing_df.columns\n",
					"\n",
					"        # Check for NSIPProjectInfoInternalID\n",
					"        if \"NSIPProjectInfoInternalID\" not in existing_cols:\n",
					"            print(\"⚠️ Column NSIPProjectInfoInternalID not found. Adding as LongType...\")\n",
					"            spark.sql(f\"ALTER TABLE {spark_table_final} ADD COLUMNS (NSIPProjectInfoInternalID BIGINT)\")\n",
					"        else:\n",
					"            print(\"✅ Column NSIPProjectInfoInternalID exists.\")\n",
					"\n",
					"        # Check for message_id and drop if exists\n",
					"        if \"message_id\" in existing_cols:\n",
					"            print(\"⚠️ Column message_id found. Dropping...\")\n",
					"            # Enable Column Mapping before dropping\n",
					"            spark.sql(f\"\"\"\n",
					"                ALTER TABLE {spark_table_final} SET TBLPROPERTIES (\n",
					"                    'delta.columnMapping.mode' = 'name',\n",
					"                    'delta.minReaderVersion' = '2',\n",
					"                    'delta.minWriterVersion' = '5'\n",
					"                )\n",
					"            \"\"\")\n",
					"            spark.sql(f\"ALTER TABLE {spark_table_final} DROP COLUMN message_id\")\n",
					"            print(\"✅ Column message_id dropped successfully.\")\n",
					"        else:\n",
					"            print(\"✅ Column message_id does not exist.\")\n",
					"\n",
					"        # ✅ Reorder columns in DataFrame if new_data exists\n",
					"        if 'new_data' in locals():\n",
					"            df_cols = new_data.columns\n",
					"            if \"NSIPProjectInfoInternalID\" in df_cols:\n",
					"                reordered_cols = [df_cols[0], \"NSIPProjectInfoInternalID\"] + \\\n",
					"                                  [c for c in df_cols if c not in [df_cols[0], \"NSIPProjectInfoInternalID\"]]\n",
					"                new_data = new_data.select(reordered_cols)\n",
					"                print(\"✅ Reordered columns so NSIPProjectInfoInternalID is second.\")\n",
					"            else:\n",
					"                print(\"⚠️ NSIPProjectInfoInternalID not found in new_data columns.\")\n",
					"        else:\n",
					"            print(\"⚠️ new_data DataFrame is not defined in this block.\")\n",
					"\n",
					"    else:\n",
					"        print(f\"❌ Table {spark_table_final} does not exist. Please create it first.\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_message = f\"⚠️ Error during schema check/update: {str(e)[:800]}\"\n",
					"    print(error_message)\n",
					""
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					},
					"collapsed": false
				},
				"source": [
					"%%pyspark\n",
					"df = spark.sql(\"SELECT * FROM `odw_harmonised_db`.`nsip_invoice`\")\n",
					"display(df)"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    # Step 1: Get max IngestionDate from existing table\n",
					"    existing_df = spark.table(spark_table_final)\n",
					"    max_IngestionDate_to = existing_df.agg(F.max(\"IngestionDate\")).collect()[0][0]\n",
					"\n",
					"    if max_IngestionDate_to is None:\n",
					"        # Set default date if no records exist\n",
					"        max_IngestionDate_to = datetime(1900, 1, 1)  # or any baseline date you prefer\n",
					"\n",
					"    print(f\"✅ Max IngestionDate from existing table: {max_IngestionDate_to}\")\n",
					"\n",
					"except Exception as e:\n",
					"    print(f\"⚠️ Error while fetching max IngestionDate: {str(e)}\")\n",
					"    max_IngestionDate_to = datetime(1900, 1, 1)  # fallback default\n",
					""
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"        # Step 2: Get new data from service bus table\n",
					"        service_bus_data = spark.sql(f\"\"\"\n",
					"            SELECT \n",
					"                NSIPProjectInfoInternalID,\n",
					"                caseId,\n",
					"                caseReference,\n",
					"                invoices,\n",
					"                ODTSourceSystem,\n",
					"                SourceSystemID,\n",
					"                IngestionDate\n",
					"            FROM {service_bus_table}\n",
					"            WHERE invoices IS NOT NULL\n",
					"        \"\"\")"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    # Step 3: Explode invoices array\n",
					"    exploded_df = service_bus_data.select(\n",
					"        F.col(\"NSIPProjectInfoInternalID\"),\n",
					"        F.col(\"caseId\"),\n",
					"        F.col(\"caseReference\"),\n",
					"        F.explode(F.col(\"invoices\")).alias(\"invoice\"),\n",
					"        F.col(\"ODTSourceSystem\"),\n",
					"        F.col(\"SourceSystemID\"),\n",
					"        F.col(\"IngestionDate\")\n",
					"    )\n",
					"\n",
					"    # Step 4: Extract invoice fields\n",
					"    new_data = exploded_df.select(\n",
					"        F.col(\"NSIPProjectInfoInternalID\"),\n",
					"        F.col(\"caseId\"),\n",
					"        F.col(\"caseReference\"),\n",
					"        F.col(\"invoice.invoiceStage\").alias(\"invoiceStage\"),\n",
					"        F.col(\"invoice.invoiceNumber\").alias(\"invoiceNumber\"),\n",
					"        F.col(\"invoice.amountDue\").alias(\"amountDue\"),\n",
					"        F.col(\"invoice.paymentDueDate\").alias(\"paymentDueDate\"),\n",
					"        F.col(\"invoice.invoicedDate\").alias(\"invoicedDate\"),\n",
					"        F.col(\"invoice.paymentDate\").alias(\"paymentDate\"),\n",
					"        F.col(\"invoice.refundCreditNoteNumber\").alias(\"refundCreditNoteNumber\"),\n",
					"        F.col(\"invoice.refundAmount\").alias(\"refundAmount\"),\n",
					"        F.col(\"invoice.refundIssueDate\").alias(\"refundIssueDate\"),\n",
					"        F.col(\"ODTSourceSystem\"),\n",
					"        F.col(\"SourceSystemID\"),\n",
					"        F.col(\"IngestionDate\")\n",
					"    )\n",
					"\n",
					"    print(\"✅ Explode and extract completed successfully.\")\n",
					"\n",
					"except Exception as e:\n",
					"    print(f\"⚠️ Error during explode/extract: {str(e)}\")\n",
					""
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"try:\n",
					"    # Step 1: Get max ID from existing table\n",
					"    existing_df = spark.table(spark_table_final)\n",
					"    max_id_row = existing_df.agg(F.max(F.col(incremental_key))).collect()[0][0]\n",
					"    if max_id_row is None:\n",
					"        max_id_row = 0  # Default if table is empty\n",
					"\n",
					"    # Step 2: Filter only records newer than max IngestionDate\n",
					"    if max_IngestionDate_to:\n",
					"        new_data = new_data.filter(F.col(\"IngestionDate\") > F.lit(max_IngestionDate_to))\n",
					"    else:\n",
					"        default_date = datetime(1900, 1, 1)\n",
					"        new_data = new_data.filter(F.col(\"IngestionDate\") > F.lit(default_date))\n",
					"\n",
					"    # Step 3: Drop old IngestionDate column\n",
					"    new_data = new_data.drop(\"IngestionDate\")\n",
					"\n",
					"    # Step 4: Add new IngestionDate column with current timestamp (formatted)\n",
					"    new_data = new_data.withColumn(\"IngestionDate\", F.date_format(F.current_timestamp(), \"yyyy-MM-dd HH:mm:ss\"))\n",
					"\n",
					"    # Step 5: Add extra columns\n",
					"    new_data = new_data.withColumn(\"ValidTo\", F.lit(None).cast(\"string\")) \\\n",
					"                       .withColumn(\"RowID\", F.lit(None).cast(\"string\")) \\\n",
					"                       .withColumn(\"IsActive\", F.lit(\"Y\").cast(\"string\"))  # Default IsActive = 'Y'\n",
					"\n",
					"    # Step 6: Add surrogate key starting from max_id_row + 1\n",
					"    window_spec = Window.orderBy(F.monotonically_increasing_id())\n",
					"    new_data = new_data.withColumn(\n",
					"        incremental_key,\n",
					"        (F.row_number().over(window_spec) + F.lit(max_id_row)).cast(\"long\")\n",
					"    )\n",
					"\n",
					"    # Step 7: Reorder columns so incremental_key is first\n",
					"    cols = new_data.columns\n",
					"    reordered_cols = [incremental_key] + [c for c in cols if c != incremental_key]\n",
					"    new_data = new_data.select(reordered_cols)\n",
					"\n",
					"    # Display final DataFrame\n",
					"    display(new_data)\n",
					"    print(\"✅ Columns added and reordered successfully.\")\n",
					"\n",
					"except Exception as e:\n",
					"    print(f\"⚠️ Error during processing: {str(e)}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"    new_data.write.format(\"delta\") \\\n",
					"            .mode(\"append\") \\\n",
					"            .option(\"overwriteSchema\", \"true\") \\\n",
					"            .saveAsTable(spark_table_final)"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"target_table = DeltaTable.forName(spark, spark_table_final)"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"df = spark.table(spark_table_final)\n",
					"\n",
					"# Window to determine latest record\n",
					"window_spec = Window.partitionBy(\"caseId\", \"invoiceNumber\").orderBy(F.col(\"IngestionDate\").desc())\n",
					"df = df.withColumn(\"RowID\", F.row_number().over(window_spec))\n",
					"df = df.withColumn(\"NewIsActive\", F.when(F.col(\"RowID\") == 1, F.lit(\"Y\")).otherwise(F.lit(\"N\")))\n",
					"\n",
					"# Step 4: Merge updates back into Delta table\n",
					"# Use MERGE for atomic update of IsActive and ValidTo\n",
					"target_table.alias(\"t\").merge(\n",
					"    df.alias(\"s\"),\n",
					"    \"t.caseId = s.caseId AND t.invoiceNumber = s.invoiceNumber AND t.IngestionDate = s.IngestionDate\"\n",
					").whenMatchedUpdate(\n",
					"    set={\n",
					"        \"IsActive\": \"s.NewIsActive\",\n",
					"        \"ValidTo\": \"CASE WHEN s.NewIsActive = 'N' THEN current_timestamp() ELSE t.ValidTo END\"\n",
					"    }\n",
					")\n",
					"\n",
					"print(\"✅ Data appended and IsActive/ValidTo updated successfully.\")\n",
					""
				],
				"execution_count": 37
			}
		]
	}
}