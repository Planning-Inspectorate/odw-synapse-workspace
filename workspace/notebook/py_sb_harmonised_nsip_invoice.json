{
	"name": "py_sb_harmonised_nsip_invoice",
	"properties": {
		"description": "Process service bus data for nsip_meeting table from sb_nsip_project with nested meeting data",
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "eadcb536-1e3c-4b31-b1cd-012bed1e124a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to read service bus data from odw_harmonised_db.sb_nsip_project Delta table and process nested meeting data to odw_harmonised_db.nsip_meeting Delta table.\n",
					"\n",
					"**Description** \n",
					"The purpose of this pyspark notebook is to read service bus data with nested meeting arrays from odw_harmonised_db.sb_nsip_project Delta table and explode/transform them into odw_harmonised_db.nsip_meeting Delta table based on the existing MiPINS business logic.\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version - 3.10, Delta Lake Version - 2.4\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## update all exceptions with appropriate comments"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Import required libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import IntegerType, ArrayType, StructType, StructField\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.functions import *\n",
					"from datetime import datetime, date\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.window import Window\n",
					"from datetime import datetime\n",
					"from delta.tables import DeltaTable\n",
					""
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define required delta tables and variables"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.window import Window\n",
					"from datetime import datetime\n",
					"from delta.tables import DeltaTable\n",
					"\n",
					"# Configurable variables\n",
					"service_bus_table = \"odw_harmonised_db.sb_nsip_project\"\n",
					"spark_table_final = \"odw_harmonised_db.nsip_invoice\"\n",
					"incremental_key = \"NSIPInvoiceID\"\n",
					"\n",
					"if not error_message:\n",
					"    try:\n",
					"        # Step 1: Get max ValidTo from existing table (not service bus)\n",
					"        existing_df = spark.table(spark_table_final)\n",
					"        max_valid_to = existing_df.agg(F.max(\"ValidTo\")).collect()[0][0]\n",
					"\n",
					"        # Step 2: Get new data from service bus table\n",
					"        service_bus_data = spark.sql(f\"\"\"\n",
					"            SELECT \n",
					"                NSIPProjectInfoInternalID,\n",
					"                caseId,\n",
					"                caseReference,\n",
					"                invoices,\n",
					"                ODTSourceSystem,\n",
					"                SourceSystemID,\n",
					"                IngestionDate\n",
					"            FROM {service_bus_table}\n",
					"            WHERE invoices IS NOT NULL\n",
					"        \"\"\")\n",
					"\n",
					"        # Step 3: Explode invoices array\n",
					"        exploded_df = service_bus_data.select(\n",
					"            F.col(\"NSIPProjectInfoInternalID\"),\n",
					"            F.col(\"caseId\"),\n",
					"            F.col(\"caseReference\"),\n",
					"            F.explode(F.col(\"invoices\")).alias(\"invoice\"),\n",
					"            F.col(\"ODTSourceSystem\"),\n",
					"            F.col(\"SourceSystemID\"),\n",
					"            F.col(\"IngestionDate\")\n",
					"        )\n",
					"\n",
					"        # Step 4: Extract invoice fields\n",
					"        new_data = exploded_df.select(\n",
					"            F.col(\"NSIPProjectInfoInternalID\"),\n",
					"            F.col(\"caseId\"),\n",
					"            F.col(\"caseReference\"),\n",
					"            F.col(\"invoice.invoiceStage\").alias(\"invoiceStage\"),\n",
					"            F.col(\"invoice.invoiceNumber\").alias(\"invoiceNumber\"),\n",
					"            F.col(\"invoice.amountDue\").alias(\"amountDue\"),\n",
					"            F.col(\"invoice.paymentDueDate\").alias(\"paymentDueDate\"),\n",
					"            F.col(\"invoice.invoicedDate\").alias(\"invoicedDate\"),\n",
					"            F.col(\"invoice.paymentDate\").alias(\"paymentDate\"),\n",
					"            F.col(\"invoice.refundCreditNoteNumber\").alias(\"refundCreditNoteNumber\"),\n",
					"            F.col(\"invoice.refundAmount\").alias(\"refundAmount\"),\n",
					"            F.col(\"invoice.refundIssueDate\").alias(\"refundIssueDate\"),\n",
					"            F.col(\"ODTSourceSystem\"),\n",
					"            F.col(\"SourceSystemID\"),\n",
					"            F.col(\"IngestionDate\")\n",
					"        )\n",
					"\n",
					"        # Step 5: Filter only records newer than max ValidTo\n",
					"        if max_valid_to:\n",
					"            new_data = new_data.filter(F.col(\"IngestionDate\") > F.lit(max_valid_to))\n",
					"\n",
					"        # Step 6: Add surrogate key dynamically\n",
					"        new_data = new_data.withColumn(incremental_key, F.monotonically_increasing_id())\n",
					"\n",
					"        # Step 7: Add ValidTo as NULL for new records\n",
					"        new_data = new_data.withColumn(\"ValidTo\", F.lit(None).cast(\"timestamp\"))\n",
					"\n",
					"        # Step 8: Determine IsActive flag\n",
					"        window_spec = Window.partitionBy(\"caseId\", \"invoiceNumber\").orderBy(F.col(\"IngestionDate\").desc())\n",
					"        new_data = new_data.withColumn(\"RowID\", F.row_number().over(window_spec))\n",
					"        new_data = new_data.withColumn(\"IsActive\", F.when(F.col(\"RowID\") == 1, F.lit(\"Y\")).otherwise(F.lit(\"N\")))\n",
					"\n",
					"        # Drop helper column\n",
					"        new_data = new_data.drop(\"RowID\")\n",
					"\n",
					"        # Step 9: Append to Delta table with schema evolution and partitioning\n",
					"        new_data.write.format(\"delta\") \\\n",
					"            .mode(\"append\") \\\n",
					"            .option(\"overwriteSchema\", \"true\") \\\n",
					"            .partitionBy(\"IsActive\") \\\n",
					"            .saveAsTable(spark_table_final)\n",
					"\n",
					"        # Step 10: Update ValidTo for inactive records\n",
					"        target_table = DeltaTable.forName(spark, spark_table_final)\n",
					"        target_table.update(\n",
					"            condition=\"IsActive = 'N' AND ValidTo IS NULL\",\n",
					"            set={\"ValidTo\": \"current_timestamp()\"}\n",
					"        )\n",
					"\n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in processing invoices: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					"        print(error_message)"
				],
				"execution_count": 43
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Prepare data for intermediate nsip_meeting processing"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# if not error_message:\n",
					"#     try: \n",
					"#         results.write.format(\"delta\").mode(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(\"IsActive\").saveAsTable(f\"{spark_table_final}\")\n",
					"#     except Exception as e:\n",
					"#         error_message = f\"Error in ingesting delta table {spark_table_final}: {str(e)[:800]}\"\n",
					"#         end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Now need to sort internal ids, IsActive flags, and valid_to dates"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# -- %%sql\n",
					"# -- CREATE OR REPLACE TEMPORARY VIEW vw_nsip_meeting_calculations_base\n",
					"# -- AS\n",
					"# -- SELECT \n",
					"# --     row_number() OVER(PARTITION BY caseId, NSIPMeetingID ORDER BY IngestionDate DESC) AS ReverseOrderProcessed\n",
					"# --     ,row_number() OVER(ORDER BY IngestionDate ASC, caseId ASC, NSIPMeetingID ASC) AS NSIPMeetingInternalID\n",
					"# --     ,caseId\n",
					"# --     ,NSIPMeetingID\n",
					"# --     ,IngestionDate\n",
					"# --     ,ValidTo\n",
					"# --     ,'0' AS migrated\n",
					"# --     ,CASE row_number() OVER(PARTITION BY caseId, NSIPMeetingID ORDER BY IngestionDate DESC)\n",
					"# --         WHEN 1 THEN\n",
					"# --             'Y'\n",
					"# --         ELSE\n",
					"# --             'N'\n",
					"# --     END AS IsActive \n",
					"# -- FROM\n",
					"# --     odw_harmonised_db.nsip_meeting"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# -- %%sql\n",
					"\n",
					"# -- select * from vw_nsip_meeting_calculations_base"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Final step for nsip meeting data transformation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# if not error_message:\n",
					"#     try:\n",
					"\n",
					"#         df_calcs = spark.sql(\"\"\"\n",
					"#             SELECT\n",
					"#                 CurrentRow.NSIPMeetingInternalID\n",
					"#                 ,CurrentRow.caseId\n",
					"#                 ,CurrentRow.NSIPMeetingID\n",
					"#                 ,CurrentRow.IngestionDate\n",
					"#                 ,COALESCE(CurrentRow.ValidTo, NextRow.IngestionDate) AS ValidTo\n",
					"#                 ,CASE\n",
					"#                     WHEN raw.caseId IS NOT NULL THEN \n",
					"#                         \\\"1\\\"\n",
					"#                     ELSE \n",
					"#                         \\\"0\\\"\n",
					"#                 END AS migrated\n",
					"#                 ,CurrentRow.IsActive\n",
					"#             FROM\n",
					"#                 vw_nsip_meeting_calculations_base AS CurrentRow\n",
					"#             LEFT OUTER JOIN vw_nsip_meeting_calculations_base AS NextRow\n",
					"#                 ON CurrentRow.caseId = NextRow.caseId\n",
					"#                 AND CurrentRow.NSIPMeetingID = NextRow.NSIPMeetingID\n",
					"#                 AND CurrentRow.ReverseOrderProcessed - 1 = NextRow.ReverseOrderProcessed\n",
					"#             LEFT OUTER JOIN (SELECT DISTINCT caseId FROM odw_harmonised_db.sb_nsip_project) AS Raw\n",
					"#                 ON CurrentRow.caseId = Raw.caseId \"\"\")\n",
					"\n",
					"#         df_calcs = df_calcs.withColumnRenamed(\"caseId\", \"temp_caseId\") \\\n",
					"#                            .withColumnRenamed(\"NSIPMeetingID\", \"temp_NSIPMeetingID\") \\\n",
					"#                            .withColumnRenamed(\"IngestionDate\", \"temp_IngestionDate\")\n",
					"\n",
					"#         results = spark.sql(f\"\"\"\n",
					"#             SELECT \n",
					"#                 NSIPProjectInfoInternalID\n",
					"#                 ,NSIPMeetingID\n",
					"#                 ,CAST(caseId AS Integer) AS caseId\n",
					"#                 ,caseReference\n",
					"#                 ,meetingAgenda\n",
					"#                 ,planningInspectorateRole\n",
					"#                 ,meetingDate\n",
					"#                 ,meetingType\n",
					"#                 ,migrated\n",
					"#                 ,ODTSourceSystem\n",
					"#                 ,SourceSystemID\n",
					"#                 ,IngestionDate\n",
					"#                 ,ValidTo\n",
					"#                 ,MD5(\n",
					"#                     COALESCE(\n",
					"#                         IFNULL(CAST(NSIPProjectInfoInternalID AS String), '.')\n",
					"#                         ,IFNULL(CAST(NSIPMeetingID AS String), '.')\n",
					"#                         ,IFNULL(CAST(caseId AS String), '.')\n",
					"#                         ,IFNULL(CAST(caseReference AS String), '.')\n",
					"#                         ,IFNULL(CAST(meetingAgenda AS String), '.')\n",
					"#                         ,IFNULL(CAST(planningInspectorateRole AS String), '.')\n",
					"#                         ,IFNULL(CAST(meetingDate AS String), '.')\n",
					"#                         ,IFNULL(CAST(meetingType AS String), '.')\n",
					"#                         ,IFNULL(CAST(migrated AS String), '.')\n",
					"#                         ,IFNULL(CAST(ODTSourceSystem AS String), '.')\n",
					"#                         ,IFNULL(CAST(SourceSystemID AS String), '.')\n",
					"#                         ,IFNULL(CAST(IngestionDate AS String), '.')\n",
					"#                         ,IFNULL(CAST(ValidTo AS String), '.')\n",
					"#                         ,IFNULL(CAST(IsActive AS String), '.')\n",
					"#                     )\n",
					"#                 ) AS RowID\n",
					"#                 ,IsActive\n",
					"#             FROM \n",
					"#                 {spark_table_final}\"\"\")\n",
					"\n",
					"#         columns = results.columns\n",
					"\n",
					"#         results = results.drop(\"ValidTo\", \"migrated\", \"IsActive\")\n",
					"\n",
					"#         final_df = results.join(df_calcs, \n",
					"#                                 (df_calcs[\"temp_caseId\"] == results[\"caseId\"]) & \n",
					"#                                 (df_calcs[\"temp_NSIPMeetingID\"] == results[\"NSIPMeetingID\"]) & \n",
					"#                                 (df_calcs[\"temp_IngestionDate\"] == results[\"IngestionDate\"])).select(columns)\n",
					"#         final_df = final_df.drop_duplicates()\n",
					"#         insert_count = final_df.count()\n",
					"#         final_df.write.format(\"delta\").mode(\"Overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(\"IsActive\").saveAsTable(f\"{spark_table_final}\")\n",
					"        \n",
					"#         end_exec_time = datetime.now()\n",
					"        \n",
					"\n",
					"#     except Exception as e:\n",
					"#         error_message = f\"Error in ingesting delta table in the final step {spark_table_final}: {str(e)[:800]}\"\n",
					"#         end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"select * from odw_harmonised_db.nsip_meeting"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"# activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"# stage = \"Success\" if not error_message else \"Failed\"\n",
					"# status_message = (\n",
					"#     f\"Successfully loaded data into {spark_table_final} table\"\n",
					"#     if not error_message\n",
					"#     else f\"Failed to load data from {spark_table_final} table\"\n",
					"# )\n",
					"# status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"# log_telemetry_and_exit(\n",
					"#     stage,\n",
					"#     start_exec_time,\n",
					"#     end_exec_time,\n",
					"#     error_message,\n",
					"#     spark_table_final,\n",
					"#     insert_count,\n",
					"#     update_count,\n",
					"#     delete_count,\n",
					"#     PipelineName,\n",
					"#     PipelineRunID,\n",
					"#     PipelineTriggerID,\n",
					"#     PipelineTriggerName,\n",
					"#     PipelineTriggerType,\n",
					"#     PipelineTriggeredbyPipelineName,\n",
					"#     PipelineTriggeredbyPipelineRunID,\n",
					"#     activity_type,\n",
					"#     duration_seconds,\n",
					"#     status_message,\n",
					"#     status_code\n",
					"# )"
				],
				"execution_count": null
			}
		]
	}
}