{
	"name": "Generate all lineage",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fdabfee6-692f-4482-8383-d6252ad30710"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\"\"\"\n",
					"Orchestration-Driven Lineage Generator\n",
					"Reads orchestration.json and generates lineage events for all transformations\n",
					"\"\"\"\n",
					"\n",
					"import json\n",
					"import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"print(\"=\"*80)\n",
					"print(\"ORCHESTRATION-DRIVEN LINEAGE GENERATION\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"\n",
					"# CONFIGURATION\n",
					"\n",
					"ORCHESTRATION_PATH = \"abfss://odw-config@pinsstodwdevuksl92q7c.dfs.core.windows.net/orchestration.json\"\n",
					"\n",
					"\n",
					"# LOAD ORCHESTRATION FILE\n",
					"\n",
					"print(\"\\nStep 1: Loading orchestration configuration...\")\n",
					"\n",
					"try:\n",
					"    orch_content = mssparkutils.fs.head(ORCHESTRATION_PATH, 500000)\n",
					"    orchestration = json.loads(orch_content)\n",
					"    print(\"Orchestration loaded successfully\")\n",
					"except Exception as e:\n",
					"    print(f\" Failed to load orchestration.json: {e}\")\n",
					"    print(f\"   Make sure file exists at: {ORCHESTRATION_PATH}\")\n",
					"    raise\n",
					"\n",
					"# Extract config\n",
					"config = orchestration.get(\"config\", {})\n",
					"STORAGE_ACCOUNT = config.get(\"storage_account\", \"pinsstodwdevuksl92q7c\")\n",
					"BASE_NAMESPACE = config.get(\"base_namespace\", \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net\")\n",
					"EVENTS_PATH = f\"abfss://{config.get('events_container', 'openlineage-events')}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
					"SCHEMA_BASE = f\"abfss://{config.get('schema_container', 'odw-config')}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
					"\n",
					"print(f\"   Storage Account: {STORAGE_ACCOUNT}\")\n",
					"print(f\"   Namespace: {BASE_NAMESPACE}\")\n",
					"print(f\"   Events Path: {EVENTS_PATH}\")\n",
					"\n",
					"\n",
					"# HELPER FUNCTIONS\n",
					"\n",
					"\n",
					"def load_schema(schema_path):\n",
					"    \"\"\"Load schema JSON file\"\"\"\n",
					"    if not schema_path:\n",
					"        return None\n",
					"    \n",
					"    try:\n",
					"        full_path = f\"{SCHEMA_BASE}/{schema_path}\"\n",
					"        content = mssparkutils.fs.head(full_path, 100000)\n",
					"        schema = json.loads(content)\n",
					"        return schema\n",
					"    except Exception as e:\n",
					"        print(f\" Could not load schema {schema_path}: {e}\")\n",
					"        return None\n",
					"\n",
					"def build_qualified_name(database, schema, table):\n",
					"    \"\"\"Build Purview-compatible qualified name\"\"\"\n",
					"    return f\"{BASE_NAMESPACE}/{database}/{schema}/{table}\"\n",
					"\n",
					"def create_dataset(database, schema_name, table, schema_file=None):\n",
					"    \"\"\"Create dataset with optional schema facet\"\"\"\n",
					"    qualified_name = build_qualified_name(database, schema_name, table)\n",
					"    \n",
					"    dataset = {\n",
					"        \"namespace\": BASE_NAMESPACE,\n",
					"        \"name\": qualified_name\n",
					"    }\n",
					"    \n",
					"    # Load and add schema if available\n",
					"    if schema_file:\n",
					"        schema = load_schema(schema_file)\n",
					"        if schema and \"fields\" in schema:\n",
					"            dataset[\"facets\"] = {\n",
					"                \"schema\": {\n",
					"                    \"fields\": [\n",
					"                        {\n",
					"                            \"name\": field.get(\"name\"),\n",
					"                            \"type\": field.get(\"type\", \"string\")\n",
					"                        }\n",
					"                        for field in schema[\"fields\"]\n",
					"                    ]\n",
					"                }\n",
					"            }\n",
					"    \n",
					"    return dataset\n",
					"\n",
					"def create_lineage_event(transformation):\n",
					"    \"\"\"Create OpenLineage event from transformation definition\"\"\"\n",
					"    \n",
					"    job_name = transformation.get(\"job_name\", \"unknown\")\n",
					"    run_id = f\"{job_name}-{datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')}\"\n",
					"    \n",
					"    # Handle single source or multiple sources\n",
					"    sources = transformation.get(\"sources\", [transformation.get(\"source\")] if \"source\" in transformation else [])\n",
					"    \n",
					"    # Build inputs\n",
					"    inputs = []\n",
					"    for source in sources:\n",
					"        if source:\n",
					"            input_dataset = create_dataset(\n",
					"                source.get(\"database\"),\n",
					"                source.get(\"schema\", \"dbo\"),\n",
					"                source.get(\"table\"),\n",
					"                source.get(\"schema_file\")\n",
					"            )\n",
					"            inputs.append(input_dataset)\n",
					"    \n",
					"    # Build output\n",
					"    target = transformation.get(\"target\", {})\n",
					"    output_dataset = create_dataset(\n",
					"        target.get(\"database\"),\n",
					"        target.get(\"schema\", \"dbo\"),\n",
					"        target.get(\"table\"),\n",
					"        target.get(\"schema_file\")\n",
					"    )\n",
					"    \n",
					"    # Create event\n",
					"    event = {\n",
					"        \"eventType\": \"COMPLETE\",\n",
					"        \"eventTime\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
					"        \"producer\": \"orchestration-lineage-generator\",\n",
					"        \"job\": {\n",
					"            \"namespace\": BASE_NAMESPACE,\n",
					"            \"name\": job_name,\n",
					"            \"facets\": {\n",
					"                \"documentation\": {\n",
					"                    \"description\": transformation.get(\"description\", \"\")\n",
					"                }\n",
					"            }\n",
					"        },\n",
					"        \"run\": {\n",
					"            \"runId\": run_id\n",
					"        },\n",
					"        \"inputs\": inputs,\n",
					"        \"outputs\": [output_dataset]\n",
					"    }\n",
					"    \n",
					"    return event, run_id\n",
					"\n",
					"def write_event(event, run_id):\n",
					"    \"\"\"Write event to blob storage\"\"\"\n",
					"    now = datetime.datetime.utcnow()\n",
					"    year, month, day = now.strftime(\"%Y,%m,%d\").split(',')\n",
					"    filename = f\"y={year}/m={month}/d={day}/{run_id}.json\"\n",
					"    full_path = f\"{EVENTS_PATH}/{filename}\"\n",
					"    \n",
					"    try:\n",
					"        mssparkutils.fs.put(full_path, json.dumps(event, indent=2), True)\n",
					"        return True, filename\n",
					"    except Exception as e:\n",
					"        print(f\"     Failed to write event: {e}\")\n",
					"        return False, None\n",
					"\n",
					"\n",
					"# GENERATE ALL LINEAGE EVENTS\n",
					"\n",
					"print(\"\\nStep 2: Generating lineage events from orchestration...\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"total_transformations = 0\n",
					"successful_events = 0\n",
					"failed_events = 0\n",
					"\n",
					"# Process each transformation layer\n",
					"transformations_dict = orchestration.get(\"transformations\", {})\n",
					"\n",
					"for layer_name, transformations in transformations_dict.items():\n",
					"    print(f\"\\n{'='*80}\")\n",
					"    print(f\"LAYER: {layer_name.upper()}\")\n",
					"    print(f\"{'='*80}\")\n",
					"    \n",
					"    if not transformations:\n",
					"        print(\"   No transformations defined\")\n",
					"        continue\n",
					"    \n",
					"    for transformation in transformations:\n",
					"        total_transformations += 1\n",
					"        \n",
					"        name = transformation.get(\"name\", \"Unknown\")\n",
					"        job_name = transformation.get(\"job_name\", \"unknown\")\n",
					"        \n",
					"        print(f\"\\n   {total_transformations}. {name}\")\n",
					"        print(f\"      Job: {job_name}\")\n",
					"        \n",
					"        # Get source/target info for display\n",
					"        source = transformation.get(\"source\") or (transformation.get(\"sources\", [{}])[0] if transformation.get(\"sources\") else {})\n",
					"        target = transformation.get(\"target\", {})\n",
					"        \n",
					"        source_name = f\"{source.get('database', '')}.{source.get('table', '')}\"\n",
					"        target_name = f\"{target.get('database', '')}.{target.get('table', '')}\"\n",
					"        \n",
					"        print(f\"      Source: {source_name}\")\n",
					"        print(f\"      Target: {target_name}\")\n",
					"        \n",
					"        try:\n",
					"            # Create event\n",
					"            event, run_id = create_lineage_event(transformation)\n",
					"            \n",
					"            # Write event\n",
					"            success, filename = write_event(event, run_id)\n",
					"            \n",
					"            if success:\n",
					"                print(f\" Event created: {filename}\")\n",
					"                successful_events += 1\n",
					"            else:\n",
					"                print(f\" Event creation failed\")\n",
					"                failed_events += 1\n",
					"                \n",
					"        except Exception as e:\n",
					"            print(f\"Error: {e}\")\n",
					"            failed_events += 1\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"print(\"\\n\" + \"=\"*80)\n",
					"print(\"SUMMARY\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"print(f\"\"\"\n",
					"Transformations Processed: {total_transformations}\n",
					""
				],
				"execution_count": null
			}
		]
	}
}