{
	"name": "entraid",
	"properties": {
		"folder": {
			"name": "odw-harmonised/EntraID"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodwpr",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4ae751d9-cc55-4a35-b75d-3ed871a6235f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodwpr",
				"name": "pinssynspodwpr",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodwpr",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import sys\n",
					"from notebookutils import mssparkutils\n",
					"from datetime import datetime, timedelta, date\n",
					"from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat,max as spark_max\n",
					"from pyspark.sql.types import StringType, DateType, TimestampType, IntegerType, FloatType, StructType, StructField\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import functions as F"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialize Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def _fail_and_log(step_name: str, e: Exception):\n",
					"    global error_message\n",
					"    error_message = f\"[{step_name}]{type(e).__name__}:{str(e)}\"[:800]\n",
					"    logError(error_message)\n",
					"\n",
					"    _log_telemetry(\"Failed\", error_message, step_name)\n",
					"    raise e "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"start_exec_time = datetime.now()\n",
					"error_message = None\n",
					"def run_step(step_name: str, fn):\n",
					"    try:\n",
					"        return fn()\n",
					"    except Exception as e:\n",
					"        _fail_and_log(step_name, e)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define variables\n",
					"target_table = \"odw_harmonised_db.entraid\"\n",
					"spark_table_final = \"odw_harmonised_db.entraid\"\n",
					"primary_key = 'TEMP_PK'\n",
					"\n",
					"# Initialize tracking variables\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message = \"\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load current raw file\n",
					"This contains a snapshot of only the valid entra ids, and should not contain duplicates"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def step_load_current_raw_entraid():\n",
					"    logInfo(\"Starting: Load current raw EntraID file\")\n",
					"    raw_entraid_snapshot = datetime.now().strftime(\"%Y-%m-%d\")\n",
					"    storage_account = spark.sparkContext.environment.get(\"dataLakeAccountName\", \"get\")\n",
					"    base_file_path = f\"abfss://odw-raw@{storage_account}.dfs.core.windows.net\"\n",
					"\n",
					"    folder_names = sorted(\n",
					"        [folder.name for folder in mssparkutils.fs.ls(f\"{base_file_path}/entraid/\")],\n",
					"        reverse=True\n",
					"    )\n",
					"\n",
					"    if not folder_names:\n",
					"        msg = (\n",
					"            f\"No folders could be found under '{base_file_path}'. \"\n",
					"            \"Expected to see folders with the form YYYY-mm-dd\"\n",
					"        )\n",
					"        logError(msg)\n",
					"\n",
					"        # Build empty DF with required schema to avoid downstream failures\n",
					"        required_cols = [\"givenName\", \"surname\", \"userPrincipalName\", \"id\", \"employeeId\"]\n",
					"        schema = T.StructType([T.StructField(c, T.StringType(), True) for c in required_cols])\n",
					"        empty_df = spark.createDataFrame([], schema)\n",
					"        empty_df.createOrReplaceTempView(\"raw_entraids\")\n",
					"\n",
					"        logInfo(\"Created empty 'raw_entraids' temp view due to missing snapshot folder. row_count=0\")\n",
					"        return empty_df\n",
					"\n",
					"    snapshot_folder = folder_names[0]\n",
					"    current_entraid_file = f\"entraid/{snapshot_folder}/entraid.json\"\n",
					"    logInfo(f\"Using snapshot_folder={snapshot_folder}, file={current_entraid_file}\")\n",
					"\n",
					"    raw_entraids = (\n",
					"        spark.read\n",
					"        .option(\"multiline\", \"true\")\n",
					"        .json(f\"{base_file_path}/{current_entraid_file}\")\n",
					"        .select(\"value\")\n",
					"    )\n",
					"\n",
					"    required_cols = [\"givenName\", \"surname\", \"userPrincipalName\", \"id\", \"employeeId\"] \n",
					"    raw_entraids = raw_entraids.withColumn(\"json\", F.explode(\"value\"))\n",
					"\n",
					"    for col in required_cols:\n",
					"        raw_entraids = raw_entraids.withColumn(col, F.col(\"json\")[col])\n",
					"\n",
					"    raw_entraids = raw_entraids.select(required_cols)\n",
					"    raw_entraids.createOrReplaceTempView(\"raw_entraids\")\n",
					"\n",
					"    row_count = raw_entraids.count()\n",
					"    logInfo(f\"raw_entraids temp view created successfully. row_count={row_count}\")\n",
					"\n",
					"    return raw_entraids\n",
					"\n",
					"raw_entraids_df = run_step(\n",
					"    \"Load current raw file (EntraID) + create raw_entraids view\",\n",
					"    step_load_current_raw_entraid\n",
					")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check for new, updated or deleted data\n",
					"- This script checks for new, updated or deleted data by checking the source data (horizon tables) against the target (odw_harmonised_db.casework tables)\n",
					"- **New Data:** where an main Reference in the source does not exist in the target, then NewData flag is set to 'Y'\n",
					"- **Updated data:** Comparison occurs on Reference Fields in source and in target where the row hash is different i.e. there is a change in one of the columns. NewData flag is set to 'Y'\n",
					"- **Deleted data:** where an Reference info in the target exists but the same identifyers don't exist in the source. DeletedData flag is set to 'Y'\n",
					"\n",
					"## View entraid_new is created"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def step_build_entraid_new_candidates():\n",
					"    logInfo(\"Starting: Build entraid_new_candidates view\")\n",
					"\n",
					"    query = \"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW entraid_new_candidates AS\n",
					"    SELECT DISTINCT\n",
					"        CASE WHEN T1.id IS NULL THEN T2.EmployeeEntraId ELSE NULL END AS EmployeeEntraId,\n",
					"        T1.id AS id,\n",
					"        T1.employeeId AS employeeId,\n",
					"        T1.givenName AS givenName,\n",
					"        T1.surname AS surname,\n",
					"        T1.userPrincipalName AS userPrincipalName,\n",
					"        '0' AS Migrated,\n",
					"        'EntraID' AS ODTSourceSystem,\n",
					"        T3.SourceSystemID AS SourceSystemID,\n",
					"        to_timestamp(T1.expected_from) AS IngestionDate,\n",
					"        NULL AS ValidTo,\n",
					"        md5(concat(\n",
					"            IFNULL(T1.id,'.'),\n",
					"            IFNULL(T1.employeeId,'.'),\n",
					"            IFNULL(T1.givenName,'.'),\n",
					"            IFNULL(T1.surname,'.'),\n",
					"            IFNULL(T1.userPrincipalName,'.')\n",
					"        )) AS RowID,\n",
					"        'Y' AS IsActive,\n",
					"        T2.IsActive AS HistoricIsActive\n",
					"    FROM odw_standardised_db.entraid T1\n",
					"    LEFT JOIN odw_harmonised_db.main_sourcesystem_fact T3\n",
					"        ON 'SAP HR' = T3.Description AND T3.IsActive = 'Y'\n",
					"    FULL JOIN odw_harmonised_db.entraid T2\n",
					"        ON T1.id = T2.id AND T2.IsActive = 'Y'\n",
					"    WHERE (\n",
					"        CASE\n",
					"            WHEN T1.id = T2.id AND md5(concat(\n",
					"                IFNULL(T1.id,'.'),\n",
					"                IFNULL(T1.employeeId,'.'),\n",
					"                IFNULL(T1.givenName,'.'),\n",
					"                IFNULL(T1.surname,'.'),\n",
					"                IFNULL(T1.userPrincipalName,'.')\n",
					"            )) <> T2.RowID THEN 'Y'\n",
					"            WHEN T2.id IS NULL THEN 'Y'\n",
					"            ELSE 'N'\n",
					"        END = 'Y'\n",
					"    )\n",
					"    AND T1.id IS NOT NULL\n",
					"    AND T1.expected_from = (SELECT MAX(expected_from) FROM odw_standardised_db.entraid)\n",
					"    \"\"\"\n",
					"\n",
					"    spark.sql(query)\n",
					"\n",
					"    row_count = spark.sql(\"SELECT COUNT(*) AS cnt FROM entraid_new_candidates\").collect()[0][\"cnt\"]\n",
					"    logInfo(f\"entraid_new_candidates view created successfully. row_count={row_count}\")\n",
					"\n",
					"    return row_count\n",
					"raw_entraid_new_candidates_count = run_step(\n",
					"    \"Build entraid_new_candidates view\",\n",
					"    step_build_entraid_new_candidates\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"If there are any duplicates, then these are removed by only selecting the candidates that are in the raw data file.\n",
					"The raw entraid data is the source of truth and does not have duplicates"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def step_build_entraid_new():\n",
					"    \"\"\"\n",
					"    Creates or replaces the temporary view 'entraid_new'\n",
					"    by joining entraid_new_candidates with raw_entraids.\n",
					"    \"\"\"\n",
					"    logInfo(\"Starting: Build entraid_new view\")\n",
					"\n",
					"    query = \"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW entraid_new AS\n",
					"    SELECT\n",
					"        candidates.*\n",
					"    FROM entraid_new_candidates AS candidates\n",
					"    INNER JOIN raw_entraids\n",
					"        ON candidates.id = raw_entraids.id\n",
					"        AND candidates.employeeId = raw_entraids.employeeId\n",
					"        AND candidates.givenName = raw_entraids.givenName\n",
					"        AND candidates.userPrincipalName = raw_entraids.userPrincipalName\n",
					"    \"\"\"\n",
					"\n",
					"    spark.sql(query)\n",
					"    row_count = spark.sql(\"SELECT COUNT(*) AS cnt FROM entraid_new\").collect()[0][\"cnt\"]\n",
					"    logInfo(f\"entraid_new view created successfully. row_count={row_count}\")\n",
					"\n",
					"    return row_count\n",
					"entraid_new_count = run_step(\n",
					"    \"Build entraid_new view\",\n",
					"    step_build_entraid_new\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Dataset is created that contains changed data and corresponding target data\n",
					"- This script combines data that has been updated, Deleted or is new, with corresponding target data\n",
					"- View **entraid_new** is unioned to the target data filter to only those rows where changes have been detected\n",
					"## View entraid_changed_rows is created"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def step_build_entraid_changed_rows():\n",
					"    \"\"\"\n",
					"    Creates or replaces the temporary view 'entraid_changed_rows'\n",
					"    which combines new/updated rows and original versions for updates.\n",
					"    \"\"\"\n",
					"    logInfo(\"Starting: Build entraid_changed_rows view\")\n",
					"\n",
					"    query = \"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW entraid_changed_rows AS\n",
					"    -- gets updated, deleted and new rows\n",
					"    SELECT\n",
					"        EmployeeEntraId,\n",
					"        id,\n",
					"        employeeId,\n",
					"        givenName,\n",
					"        surname,\n",
					"        userPrincipalName,\n",
					"        Migrated,\n",
					"        ODTSourceSystem,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    FROM entraid_new\n",
					"    WHERE HistoricIsActive = 'Y' OR HistoricIsActive IS NULL\n",
					"\n",
					"    UNION ALL\n",
					"\n",
					"    -- gets original versions of updated rows so we can update EndDate and set IsActive flag to 'N'\n",
					"    SELECT\n",
					"        EmployeeEntraId,\n",
					"        id,\n",
					"        employeeId,\n",
					"        givenName,\n",
					"        surname,\n",
					"        userPrincipalName,\n",
					"        Migrated,\n",
					"        ODTSourceSystem,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    FROM odw_harmonised_db.entraid\n",
					"    WHERE id IN (SELECT id FROM entraid_new WHERE id IS NULL) AND IsActive = 'Y'\n",
					"    \"\"\"\n",
					"\n",
					"    spark.sql(query)\n",
					"    row_count = spark.sql(\"SELECT COUNT(*) AS cnt FROM entraid_changed_rows\").collect()[0][\"cnt\"]\n",
					"    logInfo(f\"entraid_changed_rows view created successfully. row_count={row_count}\")\n",
					"\n",
					"    return row_count\n",
					"entraid_changed_rows_count = run_step(\n",
					"    \"Build entraid_changed_rows view\",\n",
					"    step_build_entraid_changed_rows\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# View entraid_changed_rows is used in a merge (Upsert) statement into the target table\n",
					"- **WHEN MATCHED** ON the surrogate Key (i.e. EmployeeEntraId), EndDate is set to today -1 day and the IsActive flag is set to 'N'\n",
					"- **WHEN NOT MATCHED** ON the surrogate Key, insert rows\n",
					"## Table odw_harmonised_db.entraid is updated"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def step_build_entraid_changed_rows_final():\n",
					"    \"\"\"\n",
					"    Creates or replaces the temporary views:\n",
					"    1) Loading_month - calculates ingestion and closing dates.\n",
					"    2) entraid_changed_rows_final - joins changed rows with loading month info.\n",
					"    \"\"\"\n",
					"    logInfo(\"Starting: Build Loading_month and entraid_changed_rows_final views\")\n",
					"\n",
					"    # 1) Create Loading_month\n",
					"    spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW Loading_month AS\n",
					"        SELECT DISTINCT\n",
					"            IngestionDate AS IngestionDate,\n",
					"            to_timestamp(date_sub(IngestionDate, 1)) AS ClosingDate,\n",
					"            'Y' AS IsActive\n",
					"        FROM entraid_new\n",
					"    \"\"\")\n",
					"\n",
					"    # 2) Create entraid_changed_rows_final\n",
					"    spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW entraid_changed_rows_final AS\n",
					"        SELECT\n",
					"            EmployeeEntraId,\n",
					"            id,\n",
					"            employeeId,\n",
					"            givenName,\n",
					"            surname,\n",
					"            userPrincipalName,\n",
					"            Migrated,\n",
					"            ODTSourceSystem,\n",
					"            T1.SourceSystemID,\n",
					"            T1.IngestionDate,\n",
					"            T1.ValidTo,\n",
					"            T1.RowID,\n",
					"            T1.IsActive,\n",
					"            T2.ClosingDate\n",
					"        FROM entraid_changed_rows T1\n",
					"        FULL JOIN Loading_month T2 ON T1.IsActive = T2.IsActive\n",
					"    \"\"\")\n",
					"\n",
					"    # Validate row count\n",
					"    row_count = spark.sql(\"SELECT COUNT(*) AS cnt FROM entraid_changed_rows_final\") \\\n",
					"                     .collect()[0][\"cnt\"]\n",
					"    logInfo(f\"entraid_changed_rows_final view created successfully. row_count={row_count}\")\n",
					"    return row_count\n",
					"\n",
					"\n",
					"# Run via your step runner\n",
					"entraid_changed_rows_final_count = run_step(\n",
					"    \"Build entraid_changed_rows_final view\",\n",
					"    step_build_entraid_changed_rows_final\n",
					")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Fix the IDs\n",
					"- No auto-increment feature is available in delta tables, therefore we need to create new IDs for the inserted rows\n",
					"- This is done by select the target data and using INSERT OVERWRITE to re-insert the data is a new Row Number\n",
					"## Table odw_harmonised_db.entraid is updated"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def step_insert_overwrite_entraid():\n",
					"    \"\"\"\n",
					"    Performs INSERT OVERWRITE into odw_harmonised_db.entraid:\n",
					"    - Rebuilds the table with a new EmployeeEntraId sequence using ROW_NUMBER().\n",
					"    \"\"\"\n",
					"    logInfo(\"Starting: INSERT OVERWRITE into odw_harmonised_db.entraid\")\n",
					"\n",
					"    query = \"\"\"\n",
					"    INSERT OVERWRITE odw_harmonised_db.entraid\n",
					"    SELECT\n",
					"        ROW_NUMBER() OVER (ORDER BY EmployeeEntraId NULLS LAST) AS EmployeeEntraId,\n",
					"        employeeId,\n",
					"        id,\n",
					"        givenName,\n",
					"        surname,\n",
					"        userPrincipalName,\n",
					"        Migrated,\n",
					"        ODTSourceSystem,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    FROM odw_harmonised_db.entraid\n",
					"    \"\"\"\n",
					"\n",
					"    spark.sql(query)\n",
					"    row_count = spark.sql(\"SELECT COUNT(*) AS cnt FROM odw_harmonised_db.entraid\").collect()[0][\"cnt\"]\n",
					"    logInfo(f\"INSERT OVERWRITE completed successfully. Total rows in entraid: {row_count}\")\n",
					"\n",
					"    return row_count\n",
					"insert_overwrite_result_count = run_step(\n",
					"    \"INSERT OVERWRITE into harmonised entraid table\",\n",
					"    step_insert_overwrite_entraid\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def _log_telemetry(stage: str, error_msg: str = None, step_name: str = None):\n",
					"    end_exec_time = datetime.now()\n",
					"    duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"    activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"\n",
					"    if stage == \"Success\":\n",
					"        status_message = f\"Successfully loaded data into {target_table} table\"\n",
					"        status_code = \"200\"\n",
					"        final_error_msg = None\n",
					"    else:\n",
					"        status_message = f\"Failed to load data into {target_table} table\"\n",
					"        if step_name:\n",
					"            status_message = f\"Failed at step [{step_name}]: {status_message}\"\n",
					"        status_code = \"500\"\n",
					"        final_error_msg = error_msg\n",
					"    try:\n",
					"        log_telemetry_and_exit(\n",
					"            stage,\n",
					"            start_exec_time,\n",
					"            end_exec_time,\n",
					"            final_error_msg,\n",
					"            target_table,\n",
					"            insert_count,\n",
					"            update_count,\n",
					"            delete_count,\n",
					"            PipelineName,\n",
					"            PipelineRunID,\n",
					"            PipelineTriggerID,\n",
					"            PipelineTriggerName,\n",
					"            PipelineTriggerType,\n",
					"            PipelineTriggeredbyPipelineName,\n",
					"            PipelineTriggeredbyPipelineRunID,\n",
					"            activity_type,\n",
					"            duration_seconds,\n",
					"            status_message,\n",
					"            status_code\n",
					"        )\n",
					"    except Exception as te:\n",
					"        print(f\"Telemetry logging failed: {te}\")\n",
					" \n",
					"if not error_message:\n",
					"    _log_telemetry(\"Success\")"
				],
				"execution_count": null
			}
		]
	}
}