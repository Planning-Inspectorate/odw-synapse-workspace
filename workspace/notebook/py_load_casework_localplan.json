{
	"name": "py_load_casework_localplan",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a530e34d-184f-486c-9285-957d3c5152ac"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"PipelineName = \"pln_curated\"\n",
					"PipelineRunID = \"5b66ce96-e547-4303-954a-2bfe138924bb\"\n",
					"PipelineTriggerID = \"8eb8c2f00fa446e88812104b6c8fe493\"\n",
					"PipelineTriggerName = \"Sandbox\"\n",
					"PipelineTriggerType = \"Manual\"\n",
					"PipelineTriggeredbyPipelineName = None\n",
					"PipelineTriggeredbyPipelineRunID = None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_harmonised_db.load_casework_localplan\"\n",
					"source_table = \"odw_standardised_db.casework_localplan\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Set time parser policy\n",
					"        logInfo(\"Setting legacy time parser policy\")\n",
					"        spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"        logInfo(\"Legacy time parser policy set successfully\")\n",
					"        \n",
					"        # Delete existing data\n",
					"        logInfo(f\"Starting deletion of all rows from {spark_table_final}\")\n",
					"        spark.sql(f\"DELETE FROM {spark_table_final}\")\n",
					"        logInfo(f\"Successfully deleted all rows from {spark_table_final}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in setting policy or deleting data from {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Insert data from source table\n",
					"        logInfo(f\"Starting data insertion into {spark_table_final} from {source_table}\")\n",
					"        spark.sql(f\"\"\"\n",
					"        INSERT INTO {spark_table_final} (\n",
					"            horizonId,\n",
					"            localDevelopmentFrameworkId,\n",
					"            region,\n",
					"            pinsLpaName,\n",
					"            localPlanTitle,\n",
					"            developmentPlanDocumentType,\n",
					"            grade,\n",
					"            actualPublicationReg19Date,\n",
					"            actualPublicationNotes,\n",
					"            actualSubmissionReg22Date,\n",
					"            actualSubmissionNotes,\n",
					"            fiveYearSupplyOfLand,\n",
					"            InspectorNames,\n",
					"            mentorOrNamedContact,\n",
					"            programmeOfficer,\n",
					"            appointmentDate,\n",
					"            appointmentNotes,\n",
					"            actualHearingStartDate,\n",
					"            actualHearingStartNotes,\n",
					"            hearingsCloseDate,\n",
					"            hearingsCloseNotes,\n",
					"            prepDays,\n",
					"            hearingDays,\n",
					"            siteVisitDays,\n",
					"            travelDays,\n",
					"            reptgDays,\n",
					"            totalDays,\n",
					"            LocalDevelopmentFrameworkRef,\n",
					"            factCheckReceiptDueDate,\n",
					"            factCheckReceiptDueDateNotes,\n",
					"            draftReportSentToQAGroupDate,\n",
					"            draftReportSentToQAGroupNotes,\n",
					"            qaPanel,\n",
					"            factCheckReportReceivedFromINSPDate,\n",
					"            factCheckReportReceivedFromINSPNotes,\n",
					"            factCheckReportDispatchDate,\n",
					"            factCheckReportDispatchNotes,\n",
					"            finalReportIssuedDate,\n",
					"            finalReportIssuedNotes,\n",
					"            adoptionDate,\n",
					"            adoptionNotes,\n",
					"            withdrawnDate,\n",
					"            withdrawnNotes,\n",
					"            status,\n",
					"            totalWeeks,\n",
					"            hearingCloseToFinalReport,\n",
					"            notes,\n",
					"            onsLPACode,\n",
					"            submissionMonthYear,\n",
					"            reportsIssuedMonthYear,\n",
					"            Migrated,\n",
					"            IngestionDate,\n",
					"            ValidFrom,\n",
					"            ValidTo,\n",
					"            RowID,\n",
					"            IsActive\n",
					"        )\n",
					"        SELECT\n",
					"            horizonId,\n",
					"            CAST(localDevelopmentFrameworkId AS INT) AS localDevelopmentFrameworkId,\n",
					"            region,\n",
					"            pinsLpaName,\n",
					"            localPlanTitle,\n",
					"            developmentPlanDocumentType,\n",
					"            grade,\n",
					"            to_date(actualPublicationReg19Date, 'dd/MM/yyyy') AS actualPublicationReg19Date,\n",
					"            actualPublicationNotes,\n",
					"            to_date(actualSubmissionReg22Date, 'dd/MM/yyyy') AS actualSubmissionReg22Date,\n",
					"            actualSubmissionNotes,\n",
					"            fiveYearSupplyOfLand,\n",
					"            InspectorNames,\n",
					"            mentorOrNamedContact,\n",
					"            programmeOfficer,\n",
					"            to_date(appointmentDate, 'dd/MM/yyyy') AS appointmentDate,\n",
					"            appointmentNotes,\n",
					"            to_date(actualHearingStartDate, 'dd/MM/yyyy') AS actualHearingStartDate,\n",
					"            actualHearingStartNotes,\n",
					"            to_date(hearingsCloseDate, 'dd/MM/yyyy') AS hearingsCloseDate,\n",
					"            hearingsCloseNotes,\n",
					"            CAST(prepDays AS FLOAT) AS prepDays,\n",
					"            CAST(hearingDays AS FLOAT) AS hearingDays,\n",
					"            CAST(siteVisitDays AS FLOAT) AS siteVisitDays,\n",
					"            CAST(travelDays AS FLOAT) AS travelDays,\n",
					"            CAST(reptgDays AS FLOAT) AS reptgDays,\n",
					"            CAST(totalDays AS FLOAT) AS totalDays,\n",
					"            CAST(LocalDevelopmentFrameworkRef AS INT) AS LocalDevelopmentFrameworkRef,\n",
					"            to_date(factCheckReceiptDueDate, 'dd/MM/yyyy') AS factCheckReceiptDueDate,\n",
					"            factCheckReceiptDueDateNotes,\n",
					"            to_date(draftReportSentToQAGroupDate, 'dd/MM/yyyy') AS draftReportSentToQAGroupDate,\n",
					"            draftReportSentToQAGroupNotes,\n",
					"            qaPanel,\n",
					"            to_date(factCheckReportReceivedFromINSPDate, 'dd/MM/yyyy') AS factCheckReportReceivedFromINSPDate,\n",
					"            factCheckReportReceivedFromINSPNotes,\n",
					"            to_date(factCheckReportDispatchDate, 'dd/MM/yyyy') AS factCheckReportDispatchDate,\n",
					"            factCheckReportDispatchNotes,\n",
					"            to_date(finalReportIssuedDate, 'dd/MM/yyyy') AS finalReportIssuedDate,\n",
					"            finalReportIssuedNotes,\n",
					"            to_date(adoptionDate, 'dd/MM/yyyy') AS adoptionDate,\n",
					"            adoptionNotes,\n",
					"            to_date(withdrawnDate, 'dd/MM/yyyy') AS withdrawnDate,\n",
					"            withdrawnNotes,\n",
					"            status,\n",
					"            CAST(totalWeeks AS FLOAT) AS totalWeeks,\n",
					"            CAST(hearingCloseToFinalReport AS FLOAT) AS hearingCloseToFinalReport,\n",
					"            notes,\n",
					"            onsLPACode,\n",
					"            submissionMonthYear,\n",
					"            reportsIssuedMonthYear,\n",
					"            null AS Migrated,\n",
					"            CURRENT_DATE() AS IngestionDate,\n",
					"            CURRENT_TIMESTAMP() AS ValidFrom,\n",
					"            CURRENT_TIMESTAMP() AS ValidTo,\n",
					"            NULL AS RowID,\n",
					"            'Y' AS IsActive\n",
					"        FROM {source_table}\n",
					"        \"\"\")\n",
					"        \n",
					"        # Get count of inserted rows\n",
					"        insert_count = spark.sql(f\"SELECT COUNT(*) as count FROM {spark_table_final}\").collect()[0]['count']\n",
					"        logInfo(f\"Successfully inserted {insert_count} rows into {spark_table_final}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in inserting data into {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Update RowID with MD5 hash\n",
					"        logInfo(\"Starting RowID update with MD5 hash values\")\n",
					"        spark.sql(f\"\"\"\n",
					"        UPDATE {spark_table_final}\n",
					"        SET RowID = md5(concat_ws('|', \n",
					"            IFNULL(CAST(horizonId AS STRING), '.'),\n",
					"            IFNULL(CAST(localDevelopmentFrameworkId AS STRING), '.'),\n",
					"            IFNULL(CAST(region AS STRING), '.'),\n",
					"            IFNULL(CAST(pinsLpaName AS STRING), '.'),\n",
					"            IFNULL(CAST(localPlanTitle AS STRING), '.'),\n",
					"            IFNULL(CAST(developmentPlanDocumentType AS STRING), '.'),\n",
					"            IFNULL(CAST(grade AS STRING), '.'),\n",
					"            IFNULL(CAST(actualPublicationReg19Date AS STRING), '.'),\n",
					"            IFNULL(CAST(actualPublicationNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(actualSubmissionReg22Date AS STRING), '.'),\n",
					"            IFNULL(CAST(actualSubmissionNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(fiveYearSupplyOfLand AS STRING), '.'),\n",
					"            IFNULL(CAST(InspectorNames AS STRING), '.'),\n",
					"            IFNULL(CAST(mentorOrNamedContact AS STRING), '.'),\n",
					"            IFNULL(CAST(programmeOfficer AS STRING), '.'),\n",
					"            IFNULL(CAST(appointmentDate AS STRING), '.'),\n",
					"            IFNULL(CAST(appointmentNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(actualHearingStartDate AS STRING), '.'),\n",
					"            IFNULL(CAST(actualHearingStartNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(hearingsCloseDate AS STRING), '.'),\n",
					"            IFNULL(CAST(hearingsCloseNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(prepDays AS STRING), '.'),\n",
					"            IFNULL(CAST(hearingDays AS STRING), '.'),\n",
					"            IFNULL(CAST(siteVisitDays AS STRING), '.'),\n",
					"            IFNULL(CAST(travelDays AS STRING), '.'),\n",
					"            IFNULL(CAST(reptgDays AS STRING), '.'),\n",
					"            IFNULL(CAST(totalDays AS STRING), '.'),\n",
					"            IFNULL(CAST(LocalDevelopmentFrameworkRef AS STRING), '.'),\n",
					"            IFNULL(CAST(factCheckReceiptDueDate AS STRING), '.'),\n",
					"            IFNULL(CAST(factCheckReceiptDueDateNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(draftReportSentToQAGroupDate AS STRING), '.'),\n",
					"            IFNULL(CAST(draftReportSentToQAGroupNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(qaPanel AS STRING), '.'),\n",
					"            IFNULL(CAST(factCheckReportReceivedFromINSPDate AS STRING), '.'),\n",
					"            IFNULL(CAST(factCheckReportReceivedFromINSPNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(factCheckReportDispatchDate AS STRING), '.'),\n",
					"            IFNULL(CAST(factCheckReportDispatchNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(finalReportIssuedDate AS STRING), '.'),\n",
					"            IFNULL(CAST(finalReportIssuedNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(adoptionDate AS STRING), '.'),\n",
					"            IFNULL(CAST(adoptionNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(withdrawnDate AS STRING), '.'),\n",
					"            IFNULL(CAST(withdrawnNotes AS STRING), '.'),\n",
					"            IFNULL(CAST(status AS STRING), '.'),\n",
					"            IFNULL(CAST(totalWeeks AS STRING), '.'),\n",
					"            IFNULL(CAST(hearingCloseToFinalReport AS STRING), '.'),\n",
					"            IFNULL(CAST(notes AS STRING), '.'),\n",
					"            IFNULL(CAST(onsLPACode AS STRING), '.'),\n",
					"            IFNULL(CAST(submissionMonthYear AS STRING), '.'),\n",
					"            IFNULL(CAST(reportsIssuedMonthYear AS STRING), '.'),\n",
					"            IFNULL(CAST(Migrated AS STRING), '.')\n",
					"        ))\n",
					"        \"\"\")\n",
					"        logInfo(\"Successfully updated all RowID values with MD5 hashes\")\n",
					"        \n",
					"        # Verify data quality\n",
					"        null_rowid_count = spark.sql(f\"SELECT COUNT(*) as count FROM {spark_table_final} WHERE RowID IS NULL\").collect()[0]['count']\n",
					"        if null_rowid_count > 0:\n",
					"            logError(f\"Data quality issue: {null_rowid_count} rows have NULL RowID values\")\n",
					"        else:\n",
					"            logInfo(\"Data quality check passed: No NULL RowID values found\")\n",
					"        \n",
					"        # Final success message\n",
					"        logInfo(\"Casework local plan data processing completed successfully\")\n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in updating RowID for {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"Successfully loaded data into {spark_table_final} table\"\n",
					"    if not error_message\n",
					"    else f\"Failed to load data into {spark_table_final} table\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    spark_table_final,\n",
					"    insert_count,\n",
					"    update_count,\n",
					"    delete_count,\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")"
				],
				"execution_count": null
			}
		]
	}
}