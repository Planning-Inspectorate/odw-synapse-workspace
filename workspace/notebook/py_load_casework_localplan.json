{
	"name": "py_load_casework_localplan",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a78e629d-ef14-4c37-aa61-204216c14e7f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"PipelineName = \"pln_curated\"\n",
					"PipelineRunID = \"5b66ce96-e547-4303-954a-2bfe138924bb\"\n",
					"PipelineTriggerID = \"8eb8c2f00fa446e88812104b6c8fe493\"\n",
					"PipelineTriggerName = \"Sandbox\"\n",
					"PipelineTriggerType = \"Manual\"\n",
					"PipelineTriggeredbyPipelineName = None\n",
					"PipelineTriggeredbyPipelineRunID = None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_harmonised_db.load_casework_localplan\"\n",
					"source_table = \"odw_standardised_db.casework_localplan\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Set time parser policy\n",
					"        logInfo(\"Setting legacy time parser policy\")\n",
					"        spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"        logInfo(\"Legacy time parser policy set successfully\")\n",
					"        \n",
					"        # Delete existing data\n",
					"        logInfo(f\"Starting deletion of all rows from {spark_table_final}\")\n",
					"        spark.sql(f\"DELETE FROM {spark_table_final}\")\n",
					"        logInfo(f\"Successfully deleted all rows from {spark_table_final}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in setting policy or deleting data from {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Insert data from source table\n",
					"        logInfo(f\"Starting data insertion into {spark_table_final} from {source_table}\")\n",
					"        spark.sql(f\"\"\"\n",
					"        INSERT INTO {spark_table_final} (\n",
					"            casework_specialist_id,\n",
					"            greenCaseType,\n",
					"            greenCaseId,\n",
					"            caseReference,\n",
					"            horizonId,\n",
					"            linkedGreenCaseId,\n",
					"            caseOfficerName,\n",
					"            caseOfficerEmail,\n",
					"            appealType,\n",
					"            procedure,\n",
					"            processingState,\n",
					"            pinsLpaCode,\n",
					"            pinsLpaName,\n",
					"            appellantName,\n",
					"            agentName,\n",
					"            SiteAddressDescription,\n",
					"            sitePostcode,\n",
					"            otherPartyName,\n",
					"            receiptDate,\n",
					"            validDate,\n",
					"            startDate,\n",
					"            lpaQuestionnaireDue,\n",
					"            lpaQuestionnaireReceived,\n",
					"            week6Date,\n",
					"            week8Date,\n",
					"            week9Date,\n",
					"            eventType,\n",
					"            eventDate,\n",
					"            eventTime,\n",
					"            inspectorName,\n",
					"            inspectorStaffNumber,\n",
					"            decision,\n",
					"            decisionDate,\n",
					"            withdrawnOrTurnedAwayDate,\n",
					"            comments,\n",
					"            active,\n",
					"            Migrated,\n",
					"            IngestionDate,\n",
					"            ValidFrom,\n",
					"            ValidTo,\n",
					"            RowID,\n",
					"            IsActive\n",
					"        )\n",
					"        SELECT\n",
					"            ROW_NUMBER() OVER (ORDER BY greenCaseId, receiptDate) AS casework_specialist_id,\n",
					"            greenCaseType,\n",
					"            greenCaseId,\n",
					"            FullReference as caseReference,\n",
					"            horizonId,\n",
					"            linkedGreenCaseId,\n",
					"            caseOfficerName,\n",
					"            caseOfficerEmail,\n",
					"            appealType,\n",
					"            procedure,\n",
					"            processingState,\n",
					"            LPACode as pinsLpaCode,\n",
					"            LPAName as pinsLpaName,\n",
					"            appellantName,\n",
					"            agentName,\n",
					"            SiteAddressDescription,\n",
					"            sitePostcode,\n",
					"            otherPartyName,\n",
					"            to_date(receiptDate, 'dd/MM/yyyy') AS receiptDate,\n",
					"            to_date(validDate, 'dd/MM/yyyy') AS validDate,\n",
					"            to_date(startDate, 'dd/MM/yyyy') AS startDate,\n",
					"            to_date(QuDate, 'dd/MM/yyyy') AS lpaQuestionnaireDue,\n",
					"            to_date(QuRecDate, 'dd/MM/yyyy') AS lpaQuestionnaireReceived,\n",
					"            to_date(`6Weeks`, 'dd/MM/yyyy') AS week6Date,\n",
					"            to_date(`8Weeks`, 'dd/MM/yyyy') AS week8Date,\n",
					"            to_date(`9Weeks`, 'dd/MM/yyyy') AS week9Date,\n",
					"            eventType,\n",
					"            to_date(eventDate, 'dd/MM/yyyy') AS eventDate,\n",
					"            eventTime,\n",
					"            inspectorName,\n",
					"            inspectorStaffNumber,\n",
					"            decision,\n",
					"            to_date(decisionDate, 'dd/MM/yyyy') AS decisionDate,\n",
					"            to_date(DateWithdrawnorTurnedAway, 'dd/MM/yyyy') AS withdrawnOrTurnedAwayDate,\n",
					"            comments,\n",
					"            active,\n",
					"            null as Migrated,\n",
					"            CURRENT_DATE() AS IngestionDate,\n",
					"            CURRENT_TIMESTAMP() AS ValidFrom,\n",
					"            CURRENT_TIMESTAMP() AS ValidTo,\n",
					"            NULL AS RowID,\n",
					"            'Y' AS IsActive\n",
					"        FROM {source_table}\n",
					"        --where active ='Y'\n",
					"        \"\"\")\n",
					"        \n",
					"        # Get count of inserted rows\n",
					"        insert_count = spark.sql(f\"SELECT COUNT(*) as count FROM {spark_table_final}\").collect()[0]['count']\n",
					"        logInfo(f\"Successfully inserted {insert_count} rows into {spark_table_final}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in inserting data into {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Update RowID with MD5 hash\n",
					"        logInfo(\"Starting RowID update with MD5 hash values\")\n",
					"        spark.sql(f\"\"\"\n",
					"        UPDATE {spark_table_final}\n",
					"        SET RowID = md5(concat_ws('|', \n",
					"            IFNULL(CAST(casework_specialist_id AS STRING), '.'),\n",
					"            IFNULL(CAST(greenCaseType AS STRING), '.'),\n",
					"            IFNULL(CAST(greenCaseId AS STRING), '.'),\n",
					"            IFNULL(CAST(caseReference AS STRING), '.'),\n",
					"            IFNULL(CAST(horizonId AS STRING), '.'),\n",
					"            IFNULL(CAST(linkedGreenCaseId AS STRING), '.'),\n",
					"            IFNULL(CAST(caseOfficerName AS STRING), '.'),\n",
					"            IFNULL(CAST(caseOfficerEmail AS STRING), '.'),\n",
					"            IFNULL(CAST(appealType AS STRING), '.'),\n",
					"            IFNULL(CAST(procedure AS STRING), '.'),\n",
					"            IFNULL(CAST(processingState AS STRING), '.'),\n",
					"            IFNULL(CAST(pinsLpaCode AS STRING), '.'),\n",
					"            IFNULL(CAST(pinsLpaName AS STRING), '.'),\n",
					"            IFNULL(CAST(appellantName AS STRING), '.'),\n",
					"            IFNULL(CAST(agentName AS STRING), '.'),\n",
					"            IFNULL(CAST(SiteAddressDescription AS STRING), '.'),\n",
					"            IFNULL(CAST(sitePostcode AS STRING), '.'),\n",
					"            IFNULL(CAST(otherPartyName AS STRING), '.'),\n",
					"            IFNULL(CAST(receiptDate AS STRING), '.'),\n",
					"            IFNULL(CAST(validDate AS STRING), '.'),\n",
					"            IFNULL(CAST(startDate AS STRING), '.'),\n",
					"            IFNULL(CAST(lpaQuestionnaireDue AS STRING), '.'),\n",
					"            IFNULL(CAST(lpaQuestionnaireReceived AS STRING), '.'),\n",
					"            IFNULL(CAST(week6Date AS STRING), '.'),\n",
					"            IFNULL(CAST(week8Date AS STRING), '.'),\n",
					"            IFNULL(CAST(week9Date AS STRING), '.'),\n",
					"            IFNULL(CAST(eventType AS STRING), '.'),\n",
					"            IFNULL(CAST(eventDate AS STRING), '.'),\n",
					"            IFNULL(CAST(eventTime AS STRING), '.'),\n",
					"            IFNULL(CAST(inspectorName AS STRING), '.'),\n",
					"            IFNULL(CAST(inspectorStaffNumber AS STRING), '.'),\n",
					"            IFNULL(CAST(decision AS STRING), '.'),\n",
					"            IFNULL(CAST(decisionDate AS STRING), '.'),\n",
					"            IFNULL(CAST(withdrawnOrTurnedAwayDate AS STRING), '.'),\n",
					"            IFNULL(CAST(comments AS STRING), '.'),\n",
					"            IFNULL(CAST(active AS STRING), '.'),\n",
					"            IFNULL(CAST(Migrated AS STRING), '.')\n",
					"        ))\n",
					"        \"\"\")\n",
					"        logInfo(\"Successfully updated all RowID values with MD5 hashes\")\n",
					"        \n",
					"        # Verify data quality\n",
					"        null_rowid_count = spark.sql(f\"SELECT COUNT(*) as count FROM {spark_table_final} WHERE RowID IS NULL\").collect()[0]['count']\n",
					"        if null_rowid_count > 0:\n",
					"            logError(f\"Data quality issue: {null_rowid_count} rows have NULL RowID values\")\n",
					"        else:\n",
					"            logInfo(\"Data quality check passed: No NULL RowID values found\")\n",
					"        \n",
					"        # Final success message\n",
					"        logInfo(\"Casework specialist data processing completed successfully\")\n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in updating RowID for {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"Successfully loaded data into {spark_table_final} table\"\n",
					"    if not error_message\n",
					"    else f\"Failed to load data into {spark_table_final} table\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    spark_table_final,\n",
					"    insert_count,\n",
					"    update_count,\n",
					"    delete_count,\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")"
				],
				"execution_count": null
			}
		]
	}
}