{
	"name": "py_load_casework_localplan",
	"properties": {
		"folder": {
			"name": "odw-harmonised/casework_local_plan"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5a3a5cb3-5416-4b80-8099-dba16d31bf64"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Harmonized Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 02-Dec-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate local plans in harmonized layer. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that local plans data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import sys\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from datetime import datetime, timedelta"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_harmonised_db.load_casework_local_plan\"\n",
					"source_table = \"odw_standardised_db.casework_local_plan\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Set time parser policy\n",
					"        logInfo(\"Setting legacy time parser policy\")\n",
					"        spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"        logInfo(\"Legacy time parser policy set successfully\")\n",
					"        \n",
					"        # Delete existing data\n",
					"        logInfo(f\"Starting deletion of all rows from {spark_table_final}\")\n",
					"        spark.sql(f\"DELETE FROM {spark_table_final}\")\n",
					"        logInfo(f\"Successfully deleted all rows from {spark_table_final}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in setting policy or deleting data from {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Insert data from source table\n",
					"        logInfo(f\"Starting data insertion into {spark_table_final} from {source_table}\")\n",
					"        spark.sql(f\"\"\"\n",
					"            INSERT INTO {spark_table_final} (\n",
					"                horizonId,\n",
					"                localDevelopmentFrameworkId,\n",
					"                region,\n",
					"                pinsLpaName,\n",
					"                localPlanTitle,\n",
					"                developmentPlanDocumentType,\n",
					"                grade,\n",
					"                actualPublicationReg19Date,\n",
					"                actualPublicationNotes,\n",
					"                actualSubmissionReg22Date,\n",
					"                actualSubmissionNotes,\n",
					"                fiveYearSupplyOfLand,\n",
					"                inspectorNames,\n",
					"                mentorOrNamedContact,\n",
					"                programmeOfficer,\n",
					"                appointmentDate,\n",
					"                appointmentNotes,\n",
					"                actualHearingStartDate,\n",
					"                actualHearingStartNotes,\n",
					"                hearingsCloseDate,\n",
					"                hearingsCloseNotes,\n",
					"                prepDays,\n",
					"                hearingDays,\n",
					"                siteVisitDays,\n",
					"                travelDays,\n",
					"                reptgDays,\n",
					"                totalDays,\n",
					"                localDevelopmentFrameworkRef,\n",
					"                factCheckReceiptDueDate,\n",
					"                factCheckReceiptDueDateNotes,\n",
					"                draftReportSentToQAGroupDate,\n",
					"                draftReportSentToQAGroupNotes,\n",
					"                qaPanel,\n",
					"                factCheckReportReceivedFromINSPDate,\n",
					"                factCheckReportReceivedFromINSPNotes,\n",
					"                factCheckReportDispatchDate,\n",
					"                factCheckReportDispatchNotes,\n",
					"                finalReportIssuedDate,\n",
					"                finalReportIssuedNotes,\n",
					"                adoptionDate,\n",
					"                adoptionNotes,\n",
					"                withdrawnDate,\n",
					"                withdrawnNotes,\n",
					"                status,\n",
					"                totalWeeks,\n",
					"                hearingCloseToFinalReport,\n",
					"                notes,\n",
					"                onsLPACode,\n",
					"                submissionMonthYear,\n",
					"                reportsIssuedMonthYear,\n",
					"                Migrated,\n",
					"                IngestionDate,\n",
					"                ValidFrom,\n",
					"                ValidTo,\n",
					"                RowID,\n",
					"                IsActive\n",
					"            )\n",
					"            SELECT\n",
					"                TRIM(HZRef) AS horizonId,\n",
					"                CAST(TRIM(LDFNo) AS INT)                                 AS localDevelopmentFrameworkId,\n",
					"                TRIM(Region)                                             AS region,\n",
					"                TRIM(LPA)                                                AS pinsLpaName,\n",
					"                TRIM(Title)                                              AS localPlanTitle,\n",
					"                TRIM(DPDType)                                            AS developmentPlanDocumentType,\n",
					"                TRIM(Grade)                                              AS grade,\n",
					"                TO_DATE(TRIM(ActualPublicationDate_Reg19), 'dd/MM/yyyy') AS actualPublicationReg19Date,\n",
					"                TRIM(ActualPublicationNotes)                             AS actualPublicationNotes,\n",
					"                TO_DATE(TRIM(ActualSubmissionDate_Reg22), 'dd/MM/yyyy')  AS actualSubmissionReg22Date,\n",
					"                TRIM(ActualSubmissionNotes)                              AS actualSubmissionNotes,\n",
					"                TRIM(FIVEYEARSUPPLYOFLAND)                               AS fiveYearSupplyOfLand,\n",
					"                TRIM(INSPECTORS)                                         AS inspectorNames,\n",
					"                TRIM(Mentornamedcontact)                                 AS mentorOrNamedContact,\n",
					"                TRIM(ProgrammeOfficer)                                   AS programmeOfficer,\n",
					"                TO_DATE(TRIM(DateofAppointment), 'dd/MM/yyyy')           AS appointmentDate,\n",
					"                TRIM(DateofAppointmentNotes)                             AS appointmentNotes,\n",
					"                TO_DATE(TRIM(ActualHearingStartDate), 'dd/MM/yyyy')      AS actualHearingStartDate,\n",
					"                TRIM(ActualHearingStartNotes)                            AS actualHearingStartNotes,\n",
					"                TO_DATE(TRIM(HearingsCloseDate), 'dd/MM/yyyy')           AS hearingsCloseDate,\n",
					"                TRIM(HearingsCloseNotes)                                 AS hearingsCloseNotes,\n",
					"                CAST(TRIM(Prepdays)      AS FLOAT)                       AS prepDays,\n",
					"                CAST(TRIM(Hearingdays)   AS FLOAT)                       AS hearingDays,\n",
					"                CAST(TRIM(SVdays)        AS FLOAT)                       AS siteVisitDays,\n",
					"                CAST(TRIM(Traveldays)    AS FLOAT)                       AS travelDays,\n",
					"                CAST(TRIM(Reptgdays)     AS FLOAT)                       AS reptgDays,\n",
					"                CAST(TRIM(TotalDays)     AS FLOAT)                       AS totalDays,\n",
					"                CAST(TRIM(DaysLDFRef)    AS INT)                         AS localDevelopmentFrameworkRef,\n",
					"                TO_DATE(\n",
					"                    TRIM(DateGivenToLPAForReceiptofFactCheck),\n",
					"                    'dd/MM/yyyy'\n",
					"                )                                                        AS factCheckReceiptDueDate,\n",
					"                TRIM(DateGivenToLPAForReceiptofFactCheckNotes)           AS factCheckReceiptDueDateNotes,\n",
					"                TO_DATE(\n",
					"                    TRIM(DraftReportSenttoQAGroupDate),\n",
					"                    'dd/MM/yyyy'\n",
					"                )                                                        AS draftReportSentToQAGroupDate,\n",
					"                TRIM(DraftReportSenttoQAGroupNotes)                      AS draftReportSentToQAGroupNotes,\n",
					"                TRIM(QAPanel)                                            AS qaPanel,\n",
					"                TO_DATE(\n",
					"                    TRIM(FactCheckReportReceivedFromINSPDate),\n",
					"                    'dd/MM/yyyy'\n",
					"                )                                                        AS factCheckReportReceivedFromINSPDate,\n",
					"                TRIM(FactCheckReportReceivedFromINSPNotes)               AS factCheckReportReceivedFromINSPNotes,\n",
					"                TO_DATE(\n",
					"                    TRIM(FactCheckReportDispatchDate),\n",
					"                    'dd/MM/yyyy'\n",
					"                )                                                        AS factCheckReportDispatchDate,\n",
					"                TRIM(FactCheckReportDispatchNotes)                       AS factCheckReportDispatchNotes,\n",
					"                TO_DATE(TRIM(FinalReportIssuedDate), 'dd/MM/yyyy')       AS finalReportIssuedDate,\n",
					"                TRIM(FinalReportIssuedNotes)                             AS finalReportIssuedNotes,\n",
					"                TO_DATE(TRIM(AdoptionDate), 'dd/MM/yyyy')                AS adoptionDate,\n",
					"                TRIM(AdoptionNotes)                                      AS adoptionNotes,\n",
					"                TO_DATE(TRIM(WithdrawnDate), 'dd/MM/yyyy')               AS withdrawnDate,\n",
					"                TRIM(WithdrawnNotes)                                     AS withdrawnNotes,\n",
					"                TRIM(Status)                                             AS status,\n",
					"                CAST(TRIM(TotalWeeks)                  AS FLOAT)         AS totalWeeks,\n",
					"                CAST(TRIM(HearingCloseto_final_report) AS FLOAT)         AS hearingCloseToFinalReport,\n",
					"                TRIM(NOTES)                                              AS notes,\n",
					"                TRIM(LPALookup)                                          AS onsLPACode,\n",
					"                TRIM(SubmissionDate)                                     AS submissionMonthYear,\n",
					"                TRIM(ReportsIssued)                                      AS reportsIssuedMonthYear,\n",
					"                'N'                  AS Migrated,\n",
					"                CURRENT_DATE()       AS IngestionDate,\n",
					"                CURRENT_TIMESTAMP()  AS ValidFrom,\n",
					"                CURRENT_TIMESTAMP()  AS ValidTo,\n",
					"                NULL                 AS RowID,\n",
					"                'Y'                  AS IsActive\n",
					"            FROM {source_table}\n",
					"        \"\"\")\n",
					"        \n",
					"        # Get count of inserted rows\n",
					"        insert_count = (\n",
					"            spark.sql(f\"SELECT COUNT(*) as count FROM {spark_table_final}\")\n",
					"                 .collect()[0]['count']\n",
					"        )\n",
					"        logInfo(f\"Successfully inserted {insert_count} rows into {spark_table_final}\")\n",
					"    \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in inserting data into {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Update RowID with MD5 hash\n",
					"        logInfo(\"Starting RowID update with MD5 hash values\")\n",
					"        spark.sql(f\"\"\"\n",
					"            UPDATE {spark_table_final}\n",
					"            SET RowID = md5(\n",
					"                concat_ws('|', \n",
					"                    COALESCE(TRIM(CAST(horizonId                         AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(localDevelopmentFrameworkId       AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(region                           AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(pinsLpaName                      AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(localPlanTitle                   AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(developmentPlanDocumentType      AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(grade                            AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(actualPublicationReg19Date       AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(actualPublicationNotes           AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(actualSubmissionReg22Date        AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(actualSubmissionNotes            AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(fiveYearSupplyOfLand             AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(inspectorNames                   AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(mentorOrNamedContact             AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(programmeOfficer                 AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(appointmentDate                  AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(appointmentNotes                 AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(actualHearingStartDate           AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(actualHearingStartNotes          AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(hearingsCloseDate                AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(hearingsCloseNotes               AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(prepDays                         AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(hearingDays                      AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(siteVisitDays                    AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(travelDays                       AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(reptgDays                        AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(totalDays                        AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(localDevelopmentFrameworkRef     AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(factCheckReceiptDueDate          AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(factCheckReceiptDueDateNotes     AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(draftReportSentToQAGroupDate     AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(draftReportSentToQAGroupNotes    AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(qaPanel                          AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(factCheckReportReceivedFromINSPDate  AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(factCheckReportReceivedFromINSPNotes AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(factCheckReportDispatchDate      AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(factCheckReportDispatchNotes     AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(finalReportIssuedDate            AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(finalReportIssuedNotes           AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(adoptionDate                     AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(adoptionNotes                    AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(withdrawnDate                    AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(withdrawnNotes                   AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(status                           AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(totalWeeks                       AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(hearingCloseToFinalReport       AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(notes                            AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(onsLPACode                       AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(submissionMonthYear              AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(reportsIssuedMonthYear           AS STRING)), '.'),\n",
					"                    COALESCE(TRIM(CAST(Migrated                         AS STRING)), '.')\n",
					"                )\n",
					"            )\n",
					"            WHERE RowID IS NULL\n",
					"        \"\"\")\n",
					"        logInfo(\"Successfully updated all RowID values with MD5 hashes\")\n",
					"        \n",
					"        # Verify data quality\n",
					"        null_rowid_count = spark.sql(\n",
					"            f\"SELECT COUNT(*) as count FROM {spark_table_final} WHERE RowID IS NULL\"\n",
					"        ).collect()[0]['count']\n",
					"        if null_rowid_count > 0:\n",
					"            logError(f\"Data quality issue: {null_rowid_count} rows have NULL RowID values\")\n",
					"        else:\n",
					"            logInfo(\"Data quality check passed: No NULL RowID values found\")\n",
					"        \n",
					"        logInfo(\"Local plan data processing completed successfully\")\n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in updating RowID for {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"Successfully loaded data into {spark_table_final} table\"\n",
					"    if not error_message\n",
					"    else f\"Failed to load data into {spark_table_final} table\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    spark_table_final,\n",
					"    insert_count,\n",
					"    update_count,\n",
					"    delete_count,\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")"
				],
				"execution_count": null
			}
		]
	}
}