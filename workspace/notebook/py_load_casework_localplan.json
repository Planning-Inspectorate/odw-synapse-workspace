{
	"name": "py_load_casework_localplan",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "db3f2856-0a7e-4e9c-afa2-82ee83056ed8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Harmonized Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 02-Dec-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate local plans in harmonized layer. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that local plans data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"PipelineName = \"pln_curated\"\n",
					"PipelineRunID = \"5b66ce96-e547-4303-954a-2bfe138924bb\"\n",
					"PipelineTriggerID = \"8eb8c2f00fa446e88812104b6c8fe493\"\n",
					"PipelineTriggerName = \"Sandbox\"\n",
					"PipelineTriggerType = \"Manual\"\n",
					"PipelineTriggeredbyPipelineName = None\n",
					"PipelineTriggeredbyPipelineRunID = None"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_logging_decorator"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_harmonised_db.load_casework_local_plan\"\n",
					"source_table = \"odw_standardised_db.casework_local_plan\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Set time parser policy\n",
					"        logInfo(\"Setting legacy time parser policy\")\n",
					"        spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"        logInfo(\"Legacy time parser policy set successfully\")\n",
					"        \n",
					"        # Delete existing data\n",
					"        logInfo(f\"Starting deletion of all rows from {spark_table_final}\")\n",
					"        spark.sql(f\"DELETE FROM {spark_table_final}\")\n",
					"        logInfo(f\"Successfully deleted all rows from {spark_table_final}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in setting policy or deleting data from {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Insert data from source table\n",
					"        logInfo(f\"Starting data insertion into {spark_table_final} from {source_table}\")\n",
					"        spark.sql(f\"\"\"\n",
					"        INSERT INTO {spark_table_final} (\n",
					"            horizonId,\n",
					"            localDevelopmentFrameworkId,\n",
					"            region,\n",
					"            pinsLpaName,\n",
					"            localPlanTitle,\n",
					"            developmentPlanDocumentType,\n",
					"            grade,\n",
					"            actualPublicationReg19Date,\n",
					"            actualPublicationNotes,\n",
					"            actualSubmissionReg22Date,\n",
					"            actualSubmissionNotes,\n",
					"            fiveYearSupplyOfLand,\n",
					"            inspectorNames,\n",
					"            mentorOrNamedContact,\n",
					"            programmeOfficer,\n",
					"            appointmentDate,\n",
					"            appointmentNotes,\n",
					"            actualHearingStartDate,\n",
					"            actualHearingStartNotes,\n",
					"            hearingsCloseDate,\n",
					"            hearingsCloseNotes,\n",
					"            prepDays,\n",
					"            hearingDays,\n",
					"            siteVisitDays,\n",
					"            travelDays,\n",
					"            reptgDays,\n",
					"            totalDays,\n",
					"            localDevelopmentFrameworkRef,\n",
					"            factCheckReceiptDueDate,\n",
					"            factCheckReceiptDueDateNotes,\n",
					"            draftReportSentToQAGroupDate,\n",
					"            draftReportSentToQAGroupNotes,\n",
					"            qaPanel,\n",
					"            factCheckReportReceivedFromINSPDate,\n",
					"            factCheckReportReceivedFromINSPNotes,\n",
					"            factCheckReportDispatchDate,\n",
					"            factCheckReportDispatchNotes,\n",
					"            finalReportIssuedDate,\n",
					"            finalReportIssuedNotes,\n",
					"            adoptionDate,\n",
					"            adoptionNotes,\n",
					"            withdrawnDate,\n",
					"            withdrawnNotes,\n",
					"            status,\n",
					"            totalWeeks,\n",
					"            hearingCloseToFinalReport,\n",
					"            notes,\n",
					"            onsLPACode,\n",
					"            submissionMonthYear,\n",
					"            reportsIssuedMonthYear,\n",
					"            Migrated,\n",
					"            IngestionDate,\n",
					"            ValidFrom,\n",
					"            ValidTo,\n",
					"            RowID,\n",
					"            IsActive\n",
					"        )\n",
					"        SELECT\n",
					"            HZRef AS horizonId,\n",
					"            CAST(LDFNo AS INT) AS localDevelopmentFrameworkId,\n",
					"            Region AS region,\n",
					"            LPA AS pinsLpaName,\n",
					"            Title AS localPlanTitle,\n",
					"            DPDType AS developmentPlanDocumentType,\n",
					"            Grade AS grade,\n",
					"            to_date(ActualPublicationDate_Reg19, 'dd/MM/yyyy') AS actualPublicationReg19Date,\n",
					"            ActualPublicationNotes AS actualPublicationNotes,\n",
					"            to_date(ActualSubmissionDate_Reg22, 'dd/MM/yyyy') AS actualSubmissionReg22Date,\n",
					"            ActualSubmissionNotes AS actualSubmissionNotes,\n",
					"            FIVEYEARSUPPLYOFLAND AS fiveYearSupplyOfLand,\n",
					"            INSPECTORS AS inspectorNames,\n",
					"            Mentornamedcontact AS mentorOrNamedContact,\n",
					"            ProgrammeOfficer AS programmeOfficer,\n",
					"            to_date(DateofAppointment, 'dd/MM/yyyy') AS appointmentDate,\n",
					"            DateofAppointmentNotes AS appointmentNotes,\n",
					"            to_date(ActualHearingStartDate, 'dd/MM/yyyy') AS actualHearingStartDate,\n",
					"            ActualHearingStartNotes AS actualHearingStartNotes,\n",
					"            to_date(HearingsCloseDate, 'dd/MM/yyyy') AS hearingsCloseDate,\n",
					"            HearingsCloseNotes AS hearingsCloseNotes,\n",
					"            CAST(Prepdays AS FLOAT) AS prepDays,\n",
					"            CAST(Hearingdays AS FLOAT) AS hearingDays,\n",
					"            CAST(SVdays AS FLOAT) AS siteVisitDays,\n",
					"            CAST(Traveldays AS FLOAT) AS travelDays,\n",
					"            CAST(Reptgdays AS FLOAT) AS reptgDays,\n",
					"            CAST(TotalDays AS FLOAT) AS totalDays,\n",
					"            CAST(DaysLDFRef AS INT) AS localDevelopmentFrameworkRef,\n",
					"            to_date(DateGivenToLPAForReceiptofFactCheck, 'dd/MM/yyyy') AS factCheckReceiptDueDate,\n",
					"            DateGivenToLPAForReceiptofFactCheckNotes AS factCheckReceiptDueDateNotes,\n",
					"            to_date(DraftReportSenttoQAGroupDate, 'dd/MM/yyyy') AS draftReportSentToQAGroupDate,\n",
					"            DraftReportSenttoQAGroupNotes AS draftReportSentToQAGroupNotes,\n",
					"            QAPanel AS qaPanel,\n",
					"            to_date(FactCheckReportReceivedFromINSPDate, 'dd/MM/yyyy') AS factCheckReportReceivedFromINSPDate,\n",
					"            FactCheckReportReceivedFromINSPNotes AS factCheckReportReceivedFromINSPNotes,\n",
					"            to_date(FactCheckReportDispatchDate, 'dd/MM/yyyy') AS factCheckReportDispatchDate,\n",
					"            FactCheckReportDispatchNotes AS factCheckReportDispatchNotes,\n",
					"            to_date(FinalReportIssuedDate, 'dd/MM/yyyy') AS finalReportIssuedDate,\n",
					"            FinalReportIssuedNotes AS finalReportIssuedNotes,\n",
					"            to_date(AdoptionDate, 'dd/MM/yyyy') AS adoptionDate,\n",
					"            AdoptionNotes AS adoptionNotes,\n",
					"            to_date(WithdrawnDate, 'dd/MM/yyyy') AS withdrawnDate,\n",
					"            WithdrawnNotes AS withdrawnNotes,\n",
					"            Status AS status,\n",
					"            CAST(TotalWeeks AS FLOAT) AS totalWeeks,\n",
					"            CAST(HearingCloseto_final_report AS FLOAT) AS hearingCloseToFinalReport,\n",
					"            NOTES AS notes,\n",
					"            LPALookup AS onsLPACode,\n",
					"            SubmissionDate AS submissionMonthYear,\n",
					"            ReportsIssued AS reportsIssuedMonthYear,\n",
					"            'N' AS Migrated,\n",
					"            CURRENT_DATE() AS IngestionDate,\n",
					"            CURRENT_TIMESTAMP() AS ValidFrom,\n",
					"            CURRENT_TIMESTAMP() AS ValidTo,\n",
					"            NULL AS RowID,\n",
					"            'Y' AS IsActive\n",
					"        FROM {source_table}\n",
					"        \"\"\")\n",
					"        \n",
					"        # Get count of inserted rows\n",
					"        insert_count = spark.sql(f\"SELECT COUNT(*) as count FROM {spark_table_final}\").collect()[0]['count']\n",
					"        logInfo(f\"Successfully inserted {insert_count} rows into {spark_table_final}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in inserting data into {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Update RowID with MD5 hash\n",
					"        logInfo(\"Starting RowID update with MD5 hash values\")\n",
					"        spark.sql(f\"\"\"\n",
					"        UPDATE {spark_table_final}\n",
					"        SET RowID = md5(concat_ws('|', \n",
					"            COALESCE(CAST(horizonId AS STRING), '.'),\n",
					"            COALESCE(CAST(localDevelopmentFrameworkId AS STRING), '.'),\n",
					"            COALESCE(CAST(region AS STRING), '.'),\n",
					"            COALESCE(CAST(pinsLpaName AS STRING), '.'),\n",
					"            COALESCE(CAST(localPlanTitle AS STRING), '.'),\n",
					"            COALESCE(CAST(developmentPlanDocumentType AS STRING), '.'),\n",
					"            COALESCE(CAST(grade AS STRING), '.'),\n",
					"            COALESCE(CAST(actualPublicationReg19Date AS STRING), '.'),\n",
					"            COALESCE(CAST(actualPublicationNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(actualSubmissionReg22Date AS STRING), '.'),\n",
					"            COALESCE(CAST(actualSubmissionNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(fiveYearSupplyOfLand AS STRING), '.'),\n",
					"            COALESCE(CAST(inspectorNames AS STRING), '.'),\n",
					"            COALESCE(CAST(mentorOrNamedContact AS STRING), '.'),\n",
					"            COALESCE(CAST(programmeOfficer AS STRING), '.'),\n",
					"            COALESCE(CAST(appointmentDate AS STRING), '.'),\n",
					"            COALESCE(CAST(appointmentNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(actualHearingStartDate AS STRING), '.'),\n",
					"            COALESCE(CAST(actualHearingStartNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(hearingsCloseDate AS STRING), '.'),\n",
					"            COALESCE(CAST(hearingsCloseNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(prepDays AS STRING), '.'),\n",
					"            COALESCE(CAST(hearingDays AS STRING), '.'),\n",
					"            COALESCE(CAST(siteVisitDays AS STRING), '.'),\n",
					"            COALESCE(CAST(travelDays AS STRING), '.'),\n",
					"            COALESCE(CAST(reptgDays AS STRING), '.'),\n",
					"            COALESCE(CAST(totalDays AS STRING), '.'),\n",
					"            COALESCE(CAST(localDevelopmentFrameworkRef AS STRING), '.'),\n",
					"            COALESCE(CAST(factCheckReceiptDueDate AS STRING), '.'),\n",
					"            COALESCE(CAST(factCheckReceiptDueDateNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(draftReportSentToQAGroupDate AS STRING), '.'),\n",
					"            COALESCE(CAST(draftReportSentToQAGroupNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(qaPanel AS STRING), '.'),\n",
					"            COALESCE(CAST(factCheckReportReceivedFromINSPDate AS STRING), '.'),\n",
					"            COALESCE(CAST(factCheckReportReceivedFromINSPNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(factCheckReportDispatchDate AS STRING), '.'),\n",
					"            COALESCE(CAST(factCheckReportDispatchNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(finalReportIssuedDate AS STRING), '.'),\n",
					"            COALESCE(CAST(finalReportIssuedNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(adoptionDate AS STRING), '.'),\n",
					"            COALESCE(CAST(adoptionNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(withdrawnDate AS STRING), '.'),\n",
					"            COALESCE(CAST(withdrawnNotes AS STRING), '.'),\n",
					"            COALESCE(CAST(status AS STRING), '.'),\n",
					"            COALESCE(CAST(totalWeeks AS STRING), '.'),\n",
					"            COALESCE(CAST(hearingCloseToFinalReport AS STRING), '.'),\n",
					"            COALESCE(CAST(notes AS STRING), '.'),\n",
					"            COALESCE(CAST(onsLPACode AS STRING), '.'),\n",
					"            COALESCE(CAST(submissionMonthYear AS STRING), '.'),\n",
					"            COALESCE(CAST(reportsIssuedMonthYear AS STRING), '.'),\n",
					"            COALESCE(CAST(Migrated AS STRING), '.')\n",
					"        ))\n",
					"        WHERE RowID IS NULL\n",
					"        \"\"\")\n",
					"        logInfo(\"Successfully updated all RowID values with MD5 hashes\")\n",
					"        \n",
					"        # Verify data quality\n",
					"        null_rowid_count = spark.sql(f\"SELECT COUNT(*) as count FROM {spark_table_final} WHERE RowID IS NULL\").collect()[0]['count']\n",
					"        if null_rowid_count > 0:\n",
					"            logError(f\"Data quality issue: {null_rowid_count} rows have NULL RowID values\")\n",
					"        else:\n",
					"            logInfo(\"Data quality check passed: No NULL RowID values found\")\n",
					"        \n",
					"        # Final success message\n",
					"        logInfo(\"Local plan data processing completed successfully\")\n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in updating RowID for {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"source": [
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"Successfully loaded data into {spark_table_final} table\"\n",
					"    if not error_message\n",
					"    else f\"Failed to load data into {spark_table_final} table\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    spark_table_final,\n",
					"    insert_count,\n",
					"    update_count,\n",
					"    delete_count,\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")"
				],
				"execution_count": 41
			}
		]
	}
}