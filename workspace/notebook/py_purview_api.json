{
	"name": "py_purview_api",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ca4868dd-971c-4b4f-8fd5-e09fd4d1e455"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# py_purview_apiInteracts with Azure Purview to resolve assets, retrieve classifications and apply anonymisation rules to Spark DataFrames.Sections: Imports, Parameters, Preview, Purview lookup, Anonymisation, Display."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import os\n",
					"import json\n",
					"import requests\n",
					"import random as _rand\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.functions import regexp_extract, max\n",
					"from pyspark.sql.types import Row, StructType, StructField, StringType, BooleanType, TimestampType\n",
					"from pyspark.sql import DataFrame\n",
					""
				],
				"execution_count": 243
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Parameters\n",
					"\n",
					"- entity_name: Service Bus entity name (e.g. 'service-user').\n",
					"- file_name: Source filename when not using Service Bus schema.\n",
					"- is_servicebus_schema: Set True when reading from Service Bus paths."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"entity_name = 'service-user'\n",
					"file_name = 'HorizonContactInformation.csv'\n",
					"is_servicebus_schema = True "
				],
				"execution_count": 251
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Test import of a Dataframe"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run service-bus/py_sb_raw_to_std"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"def get_max_file_date(df: DataFrame) -> datetime:\n",
					"    \"\"\"\n",
					"    Gets the maximum date from a file path field in a DataFrame.\n",
					"    E.g. if the input_file field contained paths such as this:\n",
					"    abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ServiceBus/appeal-has/2024-12-02/appeal-has_2024-12-02T16:54:35.214679+0000.json\n",
					"    It extracts the date from the string for each row and gets the maximum date.\n",
					"\n",
					"    Args:\n",
					"        df: a spark DataFrame\n",
					"\n",
					"    Returns:\n",
					"        formatted_timestamp: a string of the maximum file date\n",
					"    \"\"\"\n",
					"    try:\n",
					"        date_pattern: str = r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}\\+\\d{4})'\n",
					"        df: DataFrame = df.withColumn(\"file_date\", regexp_extract(df[\"input_file\"], date_pattern, 1))\n",
					"        df: DataFrame = df.withColumn(\"file_date\", df[\"file_date\"].cast(TimestampType()))\n",
					"        max_timestamp: list = df.agg(max(\"file_date\")).collect()[0][0]\n",
					"        formatted_timestamp: str = max_timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")+\"+0000\"\n",
					"        return formatted_timestamp\n",
					"    except Exception as e:\n",
					"        error_message = f\"Error extracting maximum file date from DataFrame: {str(e)}\"\n",
					"        logError(error_message)\n",
					"        raise"
				],
				"execution_count": 252
			},
			{
				"cell_type": "code",
				"source": [
					"table_name: str = f\"odw_standardised_db.sb_{entity_name.replace('-', '_')}\"\n",
					"table_df = spark.table(table_name)\n",
					"max_extracted_date = get_max_file_date(df=table_df)"
				],
				"execution_count": 253
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Load Info from Purview"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')"
				],
				"execution_count": 254
			},
			{
				"cell_type": "code",
				"source": [
					"if is_servicebus_schema:\n",
					"    source_path: str = f\"https://{storage_account}odw-raw/ServiceBus/{entity_name}/\"+ \"{Year}-{Month}-{Day}\" + \"/\" + entity_name + \".csv\"\n",
					"else:\n",
					"    source_folder = 'Horizon'\n",
					"    source_path: str = f\"https://{storage_account}odw-raw/Horizon/\" + \"{Year}-{Month}-{Day}\" + \"/\" + f\"{file_name}\""
				],
				"execution_count": 255
			},
			{
				"cell_type": "code",
				"source": [
					"source_path"
				],
				"execution_count": 228
			},
			{
				"cell_type": "code",
				"source": [
					"# Config\n",
					"\n",
					"TENANT_ID       = \"5878df98-6f88-48ab-9322-998ce557088d\"\n",
					"CLIENT_ID       = \"5750ab9b-597c-4b0d-b0f0-f4ef94e91fc0\"\n",
					"SECRET_NAME     = \"application-insights-reader\"\n",
					"PURVIEW_NAME    = \"pins-pview\"\n",
					"API_VERSION     = \"2023-09-01\"\n",
					"\n",
					"ASSET_GUID              = \"\" \n",
					"ASSET_TYPE_NAME         = \"azure_datalake_gen2_resource_set\"\n",
					"# ASSET_QUALIFIED_NAME    = \"https://pinsstodwprodukson83nw.dfs.core.windows.net/odw-raw/Horizon/{Year}-{Month}-{Day}/NSIPData.csv\"\n",
					"# ASSET_QUALIFIED_NAME    = \"https://pinsstodwdevuks9h80mb.dfs.core.windows.net/odw-raw/ServiceBus/nsip-document/{Year}-{Month}-{Day}/nsip-document.csv\"\n",
					"ASSET_QUALIFIED_NAME = source_path\n",
					"CLASSIFIED_FIELDS = (\"PotentialID\", \"NI Number\", \"Email Address\", \"First Name\", \"Last Name\", \"Birth Date\", \"Annual Salary\", \"Person's Age\", \"MICROSOFT.PERSONAL.EMAIL\", \"MICROSOFT.PERSONAL.NAME\")"
				],
				"execution_count": 262
			},
			{
				"cell_type": "code",
				"source": [
					"VAULT_NAME = spark.sparkContext.environment.get('keyVaultName', 'get')\n",
					"if not VAULT_NAME:\n",
					"    raise ValueError(f\"Cannot determine Key Vault linked service for environment: {env}\")\n",
					"\n",
					"CLIENT_SECRET = mssparkutils.credentials.getSecret(VAULT_NAME, SECRET_NAME)"
				],
				"execution_count": 263
			},
			{
				"cell_type": "code",
				"source": [
					"# AUTH\n",
					"token_url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n",
					"data = {\n",
					"   \"grant_type\": \"client_credentials\",\n",
					"   \"client_id\": CLIENT_ID,\n",
					"   \"client_secret\": CLIENT_SECRET,\n",
					"   \"scope\": \"https://purview.azure.net/.default\"\n",
					"}\n",
					"resp = requests.post(token_url, data=data, timeout=60)\n",
					"if not resp.ok:\n",
					"   raise Exception(f\"Token request failed [{resp.status_code}]: {resp.text}\")\n",
					"ACCESS_TOKEN = resp.json()[\"access_token\"]\n",
					"\n",
					"BASE = f\"https://{PURVIEW_NAME}.catalog.purview.azure.com/api/atlas/v2\"\n",
					"HDRS = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
					""
				],
				"execution_count": 264
			},
			{
				"cell_type": "code",
				"source": [
					"# HTTP helpers\n",
					"\n",
					"from urllib.parse import quote\n",
					"\n",
					"def _get(url, headers=HDRS, timeout=60):\n",
					"   r = requests.get(url, headers=headers, timeout=timeout)\n",
					"   if not r.ok:\n",
					"       raise Exception(f\"HTTP {r.status_code} â€“ {r.text[:800]}\")\n",
					"   return r.json()\n",
					"\n",
					"def get_entity_with_refs(guid: str):\n",
					"   url = (\n",
					"       f\"{BASE}/entity/guid/{guid}\"\n",
					"       f\"?minExtInfo=true&ignoreRelationships=false&api-version={API_VERSION}\"\n",
					"   )\n",
					"   return _get(url)\n",
					"\n",
					"def get_entity_classifications(guid: str):\n",
					"   url = f\"{BASE}/entity/guid/{guid}/classifications?api-version={API_VERSION}\"\n",
					"   try:\n",
					"       data = _get(url)\n",
					"       return data.get(\"list\", []) or []\n",
					"   except Exception:\n",
					"       return []\n",
					"\n",
					"def get_guid_by_unique_attrs(type_name: str, qualified_name: str) -> str:\n",
					"\n",
					"   qn = quote(qualified_name, safe=\"\")\n",
					"   url = (f\"{BASE}/entity/uniqueAttribute/type/{quote(type_name, safe='')}\"\n",
					"          f\"?attr%3AqualifiedName={qn}&api-version={API_VERSION}\")\n",
					"   data = _get(url)\n",
					"   # Atlas may return either 'entity' or the flattened attributes + guid\n",
					"   if \"entity\" in data and \"guid\" in data[\"entity\"]:\n",
					"       return data[\"entity\"][\"guid\"]\n",
					"   if \"guid\" in data:\n",
					"       return data[\"guid\"]\n",
					"   raise Exception(f\"Could not resolve GUID for {type_name} :: {qualified_name}\")\n",
					""
				],
				"execution_count": 265
			},
			{
				"cell_type": "code",
				"source": [
					"# Resolve asset GUID from provided inputs\n",
					"resource_guid = ASSET_GUID\n",
					"if not resource_guid:\n",
					"    if ASSET_TYPE_NAME and ASSET_QUALIFIED_NAME:\n",
					"        resource_guid = get_guid_by_unique_attrs(ASSET_TYPE_NAME, ASSET_QUALIFIED_NAME)\n",
					"    else:\n",
					"        raise Exception(\"Provide ASSET_GUID or (ASSET_TYPE_NAME and ASSET_QUALIFIED_NAME)\")\n",
					"\n",
					"entity_data = get_entity_with_refs(resource_guid)"
				],
				"execution_count": 266
			},
			{
				"cell_type": "code",
				"source": [
					"# Defensive extraction of relationshipAttributes and attachedSchema\n",
					"rel_attrs = entity_data.get(\"entity\", {}).get(\"relationshipAttributes\", {})\n",
					"attached_schema = rel_attrs.get(\"attachedSchema\", [])\n",
					"\n",
					"print(f\"Attached schema count: {len(attached_schema)}\")\n",
					"for schema in attached_schema:\n",
					"    print(f\"- GUID: {schema.get('guid')} | Name: {schema.get('displayText')} | Type: {schema.get('typeName')}\")"
				],
				"execution_count": 267
			},
			{
				"cell_type": "code",
				"source": [
					"schema_guid = attached_schema[0][\"guid\"] if attached_schema else None\n",
					"if not schema_guid:\n",
					"    raise ValueError(\"No attached schema found.\")\n",
					"\n",
					"schema_entity = get_entity_with_refs(schema_guid)\n",
					"columns = schema_entity.get(\"referredEntities\", {})\n",
					"\n",
					"classified_cols = []\n",
					"for guid, col in columns.items():\n",
					"    classifications = col.get(\"classifications\", [])\n",
					"    if classifications:  # Filter only those with classification\n",
					"        classified_cols.append({\n",
					"            \"column_guid\": guid,\n",
					"            \"column_name\": col[\"attributes\"].get(\"name\"),\n",
					"            \"column_type\": col.get(\"typeName\"),\n",
					"            \"classifications\": [c[\"typeName\"] for c in classifications],\n",
					"        })\n",
					"\n",
					"print(json.dumps(classified_cols, indent=2))"
				],
				"execution_count": 268
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Execute Anonymisation"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"filt_classified_cols =  [col for col in classified_cols if any(c in CLASSIFIED_FIELDS for c in col['classifications'])]\n",
					"cols_to_mask = [c['column_name'] for c in filt_classified_cols]\n",
					"classification = [c['classifications']for c in filt_classified_cols]\n",
					"anonym_table_df = apply_anonymisation_rules(table_df, cols_to_mask, classification)"
				],
				"execution_count": 284
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Anonymisation Rules"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# display(anonym_table_df)\n",
					"display(table_df)"
				],
				"execution_count": 285
			},
			{
				"cell_type": "code",
				"source": [
					"def _seed_col(df):\n",
					"    cols = [c for c in ['Staff Number','PersNo','PersNo.','Personnel Number','Employee ID','EmployeeID'] if c in df.columns]\n",
					"    return F.coalesce(*[F.col(c).cast('string') for c in cols]) if cols else F.lit('seed')\n",
					"\n",
					"def mask_after_first_two_col(col):\n",
					"    return F.when(col.isNull(), None).otherwise(F.regexp_replace(col.cast('string'), r'(?<=..).', '*'))\n",
					"\n",
					"@F.udf('string')\n",
					"def mask_fullname_udf(v):\n",
					"    if v is None:\n",
					"        return None\n",
					"    parts = [p for p in str(v).split() if p]\n",
					"    def m(p): return p if len(p) <= 2 else p[:2] + '*' * (len(p) - 2)\n",
					"    return ' '.join(m(p) for p in parts)\n",
					"\n",
					"def random_int_from_seed(seed, min_value, max_value):\n",
					"    return (F.abs(F.hash(seed)) % (max_value - min_value + 1)) + F.lit(min_value)\n",
					"\n",
					"def random_date_from_seed(seed, start='1955-01-01', end='2005-12-31'):\n",
					"    start_date = F.to_date(F.lit(start))\n",
					"    end_date = F.to_date(F.lit(end))\n",
					"    days = F.datediff(end_date, start_date)\n",
					"    offset = F.abs(F.hash(seed)) % days\n",
					"    return F.date_add(start_date, offset.cast('int'))\n",
					"\n",
					"@F.udf('string')\n",
					"def mask_email_with_persno_udf(email, pers_no, is_lm):\n",
					"    try:\n",
					"        base_identifier = 'UNKNOWN'\n",
					"        if pers_no is not None:\n",
					"            s = str(pers_no).strip()\n",
					"            if s and s.lower() not in ('', 'none', 'null', 'n/a', 'nan'):\n",
					"                base_identifier = s\n",
					"        domain = '#PINS.com'\n",
					"        return (f\"{base_identifier}_LM@{domain}\" if bool(is_lm) else f\"{base_identifier}@{domain}\")\n",
					"    except Exception:\n",
					"        return 'UNKNOWN_LM@#PINS.com' if bool(is_lm) else 'UNKNOWN@#PINS.com'\n",
					"\n",
					"@F.udf('string')\n",
					"def generate_random_ni_number_udf(seed_str):\n",
					"    letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'\n",
					"    first = _rand.choice(letters)\n",
					"    second = _rand.choice(letters)\n",
					"    digits = ''.join(str(_rand.randint(0,9)) for _ in range(6))\n",
					"    last = _rand.choice('ABCD')\n",
					"    return f\"{first}{second}{digits}{last}\"\n",
					"\n",
					"def apply_anonymisation_rules(df, sensitive_cols, classification):\n",
					"    seed = _seed_col(df)\n",
					"    col_names = [d['column_name'] for d in sensitive_cols] if (isinstance(sensitive_cols, list) and sensitive_cols and isinstance(sensitive_cols[0], dict) and 'column_name' in sensitive_cols[0]) else (sensitive_cols if isinstance(sensitive_cols, list) else [])\n",
					"    out = df\n",
					"    exists = set(df.columns)\n",
					"    staff_col_name = next((c for c in ['Staff Number','PersNo','PersNo.','Personnel Number','Employee ID','EmployeeID'] if c in df.columns), None)\n",
					"    staff_col = F.col(staff_col_name) if staff_col_name else seed\n",
					"    if 'First Name' in classification and 'First Name' in exists:\n",
					"        out = out.withColumn('First Name', mask_after_first_two_col(F.col('First Name')))\n",
					"    if 'Last Name' in classification and 'Last Name' in exists:\n",
					"        out = out.withColumn('Last Name', mask_after_first_two_col(F.col('Last Name')))\n",
					"    if 'Date of Birth' in classification and 'Date of Birth' in exists:\n",
					"        out = out.withColumn('Date of Birth', random_date_from_seed(seed))\n",
					"    if 'Email address' in classification and 'Email address' in exists:\n",
					"        out = out.withColumn('Email address', mask_email_with_persno_udf(F.col('Email address'), staff_col.cast('string'), F.lit(False)))\n",
					"    if 'Line Manager Name' in classification and 'Line Manager Name' in exists:\n",
					"        out = out.withColumn('Line Manager Name', mask_fullname_udf(F.col('Line Manager Name')))\n",
					"    if 'Line Manager Email address' in classification and 'Line Manager Email address' in exists:\n",
					"        out = out.withColumn('Line Manager Email address', mask_email_with_persno_udf(F.col('Line Manager Email address'), staff_col.cast('string'), F.lit(True)))\n",
					"    if 'Annual Salary' in classification and 'Annual Salary' in exists:\n",
					"        out = out.withColumn('Annual Salary', random_int_from_seed(seed, 20000, 100000).cast('int'))\n",
					"    if 'Employee Age' in classification and 'Employee Age' in exists:\n",
					"        out = out.withColumn('Employee Age', random_int_from_seed(seed, 18, 70).cast('int'))\n",
					"    if 'NI Number' in classification and 'NI Number' in exists:\n",
					"        out = out.withColumn('NI Number', generate_random_ni_number_udf(staff_col.cast('string')))\n",
					"    if 'Potential ID' in classification and 'Potential ID' in exists:\n",
					"        out = out.withColumn('NI Number', generate_random_ni_number_udf(staff_col.cast('string')))\n",
					"    return out\n",
					""
				],
				"execution_count": 282
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# #DF\n",
					"\n",
					"# schema = StructType([\n",
					"#    StructField(\"asset_guid\",           StringType(), True),\n",
					"#    StructField(\"asset_name\",           StringType(), True),\n",
					"#    StructField(\"asset_fqn\",            StringType(), True),\n",
					"#    StructField(\"asset_type\",           StringType(), True),\n",
					"#    StructField(\"column_guid\",          StringType(), True),\n",
					"#    StructField(\"column_name\",          StringType(), True),\n",
					"#    StructField(\"column_entity_type\",   StringType(), True),\n",
					"#    StructField(\"column_data_type\",     StringType(), True),\n",
					"#    StructField(\"classification_name\",  StringType(), True),\n",
					"#    StructField(\"classification_state\", StringType(), True),\n",
					"#    StructField(\"propagate\",            BooleanType(), True),\n",
					"# ])\n",
					"\n",
					"# df = spark.createDataFrame(rows, schema) if rows else spark.createDataFrame([], schema)\n",
					"# df.createOrReplaceTempView(\"columns_with_classification\")\n",
					"\n",
					"# display(df.orderBy(\"column_name\"))\n",
					"\n",
					""
				],
				"execution_count": 241
			}
		]
	}
}