{
	"name": "py_purview_api",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5755bf8e-bdef-4243-8cdc-ea1a804efdc3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import os\n",
					"import requests, json\n",
					"from functools import reduce\n",
					"from azure.identity import ClientSecretCredential\n",
					"from notebookutils import mssparkutils\n",
					"import requests\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.types import Row, StructType, StructField, StringType, BooleanType"
				],
				"execution_count": 197
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Parameters from SB or Horizon raw to std"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"entity_name = 'service-user'\n",
					"file_name = 'HorizonContactInformation.csv'\n",
					"is_servicebus_schema = True"
				],
				"execution_count": 223
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Test import of a Dataframe"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"def get_max_file_date(df: DataFrame) -> datetime:\n",
					"    \"\"\"\n",
					"    Gets the maximum date from a file path field in a DataFrame.\n",
					"    E.g. if the input_file field contained paths such as this:\n",
					"    abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ServiceBus/appeal-has/2024-12-02/appeal-has_2024-12-02T16:54:35.214679+0000.json\n",
					"    It extracts the date from the string for each row and gets the maximum date.\n",
					"\n",
					"    Args:\n",
					"        df: a spark DataFrame\n",
					"\n",
					"    Returns:\n",
					"        formatted_timestamp: a string of the maximum file date\n",
					"    \"\"\"\n",
					"    try:\n",
					"        date_pattern: str = r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}\\+\\d{4})'\n",
					"        df: DataFrame = df.withColumn(\"file_date\", regexp_extract(df[\"input_file\"], date_pattern, 1))\n",
					"        df: DataFrame = df.withColumn(\"file_date\", df[\"file_date\"].cast(TimestampType()))\n",
					"        max_timestamp: list = df.agg(max(\"file_date\")).collect()[0][0]\n",
					"        formatted_timestamp: str = max_timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")+\"+0000\"\n",
					"        return formatted_timestamp\n",
					"    except Exception as e:\n",
					"        error_message = f\"Error extracting maximum file date from DataFrame: {str(e)}\"\n",
					"        logError(error_message)\n",
					"        raise"
				],
				"execution_count": 224
			},
			{
				"cell_type": "code",
				"source": [
					"table_name: str = f\"odw_standardised_db.sb_{entity_name.replace('-', '_')}\"\n",
					"table_df = spark.table(table_name)\n",
					"max_extracted_date = get_max_file_date(df=table_df)"
				],
				"execution_count": 225
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Load Info from Purview"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')"
				],
				"execution_count": 226
			},
			{
				"cell_type": "code",
				"source": [
					"if is_servicebus_schema:\n",
					"    source_path: str = f\"https://{storage_account}odw-raw/ServiceBus/{entity_name}/\"+ \"{Year}-{Month}-{Day}\" + \"/\" + entity_name + \".csv\"\n",
					"else:\n",
					"    source_folder = 'Horizon'\n",
					"    source_path: str = f\"https://{storage_account}odw-raw/Horizon/\" + \"{Year}-{Month}-{Day}\" + \"/\" + f\"{file_name}\""
				],
				"execution_count": 227
			},
			{
				"cell_type": "code",
				"source": [
					"source_path"
				],
				"execution_count": 228
			},
			{
				"cell_type": "code",
				"source": [
					"# Config\n",
					"\n",
					"TENANT_ID       = \"5878df98-6f88-48ab-9322-998ce557088d\"\n",
					"CLIENT_ID       = \"5750ab9b-597c-4b0d-b0f0-f4ef94e91fc0\"\n",
					"SECRET_NAME     = \"application-insights-reader\"\n",
					"PURVIEW_NAME    = \"pins-pview\"\n",
					"API_VERSION     = \"2023-09-01\"\n",
					"\n",
					"ASSET_GUID              = \"\" \n",
					"ASSET_TYPE_NAME         = \"azure_datalake_gen2_resource_set\"\n",
					"# ASSET_QUALIFIED_NAME    = \"https://pinsstodwprodukson83nw.dfs.core.windows.net/odw-raw/Horizon/{Year}-{Month}-{Day}/NSIPData.csv\"\n",
					"# ASSET_QUALIFIED_NAME    = \"https://pinsstodwdevuks9h80mb.dfs.core.windows.net/odw-raw/ServiceBus/nsip-document/{Year}-{Month}-{Day}/nsip-document.csv\"\n",
					"ASSET_QUALIFIED_NAME = source_path\n",
					"CLASSIFIED_FIELDS = (\"PotentialID\", \"NI Number\", \"Email Address\", \"First Name\", \"Last Name\", \"Birth Date\", \"Annual Salary\", \"Person's Age\", \"MICROSOFT.PERSONAL.EMAIL\", \"MICROSOFT.PERSONAL.NAME\")"
				],
				"execution_count": 229
			},
			{
				"cell_type": "code",
				"source": [
					"VAULT_NAME =spark.sparkContext.environment.get('keyVaultName', 'get')\n",
					"if not VAULT_NAME:\n",
					"    raise ValueError(f\"Cannot determine Key Vault linked service for environment: {env}\")\n",
					"\n",
					"CLIENT_SECRET = mssparkutils.credentials.getSecret(VAULT_NAME, SECRET_NAME)"
				],
				"execution_count": 230
			},
			{
				"cell_type": "code",
				"source": [
					"# AUTH\n",
					"token_url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n",
					"data = {\n",
					"   \"grant_type\": \"client_credentials\",\n",
					"   \"client_id\": CLIENT_ID,\n",
					"   \"client_secret\": CLIENT_SECRET,\n",
					"   \"scope\": \"https://purview.azure.net/.default\"\n",
					"}\n",
					"resp = requests.post(token_url, data=data, timeout=60)\n",
					"if not resp.ok:\n",
					"   raise Exception(f\"Token request failed [{resp.status_code}]: {resp.text}\")\n",
					"ACCESS_TOKEN = resp.json()[\"access_token\"]\n",
					"\n",
					"BASE = f\"https://{PURVIEW_NAME}.catalog.purview.azure.com/api/atlas/v2\"\n",
					"HDRS = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
					""
				],
				"execution_count": 231
			},
			{
				"cell_type": "code",
				"source": [
					"# HTTP helpers\n",
					"\n",
					"from urllib.parse import quote\n",
					"\n",
					"def _get(url, headers=HDRS, timeout=60):\n",
					"   r = requests.get(url, headers=headers, timeout=timeout)\n",
					"   if not r.ok:\n",
					"       raise Exception(f\"HTTP {r.status_code} â€“ {r.text[:800]}\")\n",
					"   return r.json()\n",
					"\n",
					"def get_entity_with_refs(guid: str):\n",
					"   url = (\n",
					"       f\"{BASE}/entity/guid/{guid}\"\n",
					"       f\"?minExtInfo=true&ignoreRelationships=false&api-version={API_VERSION}\"\n",
					"   )\n",
					"   return _get(url)\n",
					"\n",
					"def get_entity_classifications(guid: str):\n",
					"   url = f\"{BASE}/entity/guid/{guid}/classifications?api-version={API_VERSION}\"\n",
					"   try:\n",
					"       data = _get(url)\n",
					"       return data.get(\"list\", []) or []\n",
					"   except Exception:\n",
					"       return []\n",
					"\n",
					"def get_guid_by_unique_attrs(type_name: str, qualified_name: str) -> str:\n",
					"\n",
					"   qn = quote(qualified_name, safe=\"\")\n",
					"   url = (f\"{BASE}/entity/uniqueAttribute/type/{quote(type_name, safe='')}\"\n",
					"          f\"?attr%3AqualifiedName={qn}&api-version={API_VERSION}\")\n",
					"   data = _get(url)\n",
					"   # Atlas may return either 'entity' or the flattened attributes + guid\n",
					"   if \"entity\" in data and \"guid\" in data[\"entity\"]:\n",
					"       return data[\"entity\"][\"guid\"]\n",
					"   if \"guid\" in data:\n",
					"       return data[\"guid\"]\n",
					"   raise Exception(f\"Could not resolve GUID for {type_name} :: {qualified_name}\")\n",
					""
				],
				"execution_count": 232
			},
			{
				"cell_type": "code",
				"source": [
					"# Resolve asset GUID from provided inputs\n",
					"resource_guid = ASSET_GUID\n",
					"if not resource_guid:\n",
					"    if ASSET_TYPE_NAME and ASSET_QUALIFIED_NAME:\n",
					"        resource_guid = get_guid_by_unique_attrs(ASSET_TYPE_NAME, ASSET_QUALIFIED_NAME)\n",
					"    else:\n",
					"        raise Exception(\"Provide ASSET_GUID or (ASSET_TYPE_NAME and ASSET_QUALIFIED_NAME)\")\n",
					"\n",
					"entity_data = get_entity_with_refs(resource_guid)"
				],
				"execution_count": 233
			},
			{
				"cell_type": "code",
				"source": [
					"# Defensive extraction of relationshipAttributes and attachedSchema\n",
					"rel_attrs = entity_data.get(\"entity\", {}).get(\"relationshipAttributes\", {})\n",
					"attached_schema = rel_attrs.get(\"attachedSchema\", [])\n",
					"\n",
					"print(f\"Attached schema count: {len(attached_schema)}\")\n",
					"for schema in attached_schema:\n",
					"    print(f\"- GUID: {schema.get('guid')} | Name: {schema.get('displayText')} | Type: {schema.get('typeName')}\")"
				],
				"execution_count": 234
			},
			{
				"cell_type": "code",
				"source": [
					"schema_guid = attached_schema[0][\"guid\"] if attached_schema else None\n",
					"if not schema_guid:\n",
					"    raise ValueError(\"No attached schema found.\")\n",
					"\n",
					"schema_entity = get_entity_with_refs(schema_guid)\n",
					"columns = schema_entity.get(\"referredEntities\", {})\n",
					"\n",
					"classified_cols = []\n",
					"for guid, col in columns.items():\n",
					"    classifications = col.get(\"classifications\", [])\n",
					"    if classifications:  # Filter only those with classification\n",
					"        classified_cols.append({\n",
					"            \"column_guid\": guid,\n",
					"            \"column_name\": col[\"attributes\"].get(\"name\"),\n",
					"            \"column_type\": col.get(\"typeName\"),\n",
					"            \"classifications\": [c[\"typeName\"] for c in classifications],\n",
					"        })\n",
					"\n",
					"print(json.dumps(classified_cols, indent=2))"
				],
				"execution_count": 235
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Execute Anonymisation"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"filt_classified_cols =  [col for col in classified_cols if any(c in CLASSIFIED_FIELDS for c in col[\"classifications\"])]\n",
					"cols_to_null = [c['column_name'] for c in filt_classified_cols]\n",
					"anonym_table_df = table_df  # start from the original\n",
					"anonym_table_df = reduce(\n",
					"    lambda df, c: df.withColumn(c, F.lit(None)) if c in df.columns else df,\n",
					"    cols_to_null,\n",
					"    table_df\n",
					")"
				],
				"execution_count": 236
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Anonymisation Rules"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(anonym_table_df)\n",
					"# display(table_df)"
				],
				"execution_count": 237
			},
			{
				"cell_type": "code",
				"source": [
					"# -----------------------------\n",
					"# Returns a \"seed\" column used for deterministic anonymisation.\n",
					"# Combines known staff identifier columns if they exist; otherwise returns a constant literal.\n",
					"# -----------------------------\n",
					"def _seed_col(df):\n",
					"    cols = [c for c in [\"Staff Number\", \"PersNo\", \"PersNo.\", \"Personnel Number\", \"Employee ID\", \"EmployeeID\"] if c in df.columns]\n",
					"    if not cols:\n",
					"        return F.lit(\"seed\")\n",
					"    return F.coalesce(*[F.col(c).cast(\"string\") for c in cols])"
				],
				"execution_count": 238
			},
			{
				"cell_type": "code",
				"source": [
					"# -----------------------------\n",
					"# Masks all characters in a string column after the first two characters with '*'.\n",
					"# If the column is null, it remains null.\n",
					"# -----------------------------\n",
					"def mask_after_first_two_col(col):\n",
					"    return F.when(col.isNull(), None).otherwise(F.regexp_replace(col.cast(\"string\"), r\"(?<=..).\", \"*\"))\n",
					""
				],
				"execution_count": 239
			},
			{
				"cell_type": "code",
				"source": [
					"# -----------------------------\n",
					"# UDF to mask a full name, keeping the first two characters of each part and masking the rest with '*'.\n",
					"# -----------------------------\n",
					"@F.udf(\"string\")\n",
					"def mask_fullname_udf(v):\n",
					"    if v is None:\n",
					"        return None\n",
					"    parts = [p for p in str(v).split() if p]\n",
					"    def m(p):\n",
					"        return p if len(p) <= 2 else p[:2] + \"*\" * (len(p) - 2)\n",
					"    return \" \".join(m(p) for p in parts)\n",
					""
				],
				"execution_count": 240
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# #DF\n",
					"\n",
					"# schema = StructType([\n",
					"#    StructField(\"asset_guid\",           StringType(), True),\n",
					"#    StructField(\"asset_name\",           StringType(), True),\n",
					"#    StructField(\"asset_fqn\",            StringType(), True),\n",
					"#    StructField(\"asset_type\",           StringType(), True),\n",
					"#    StructField(\"column_guid\",          StringType(), True),\n",
					"#    StructField(\"column_name\",          StringType(), True),\n",
					"#    StructField(\"column_entity_type\",   StringType(), True),\n",
					"#    StructField(\"column_data_type\",     StringType(), True),\n",
					"#    StructField(\"classification_name\",  StringType(), True),\n",
					"#    StructField(\"classification_state\", StringType(), True),\n",
					"#    StructField(\"propagate\",            BooleanType(), True),\n",
					"# ])\n",
					"\n",
					"# df = spark.createDataFrame(rows, schema) if rows else spark.createDataFrame([], schema)\n",
					"# df.createOrReplaceTempView(\"columns_with_classification\")\n",
					"\n",
					"# display(df.orderBy(\"column_name\"))\n",
					"\n",
					""
				],
				"execution_count": 241
			}
		]
	}
}