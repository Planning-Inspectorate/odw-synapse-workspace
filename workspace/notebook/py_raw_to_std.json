{
	"name": "py_raw_to_std",
	"properties": {
		"folder": {
			"name": "1-odw-raw-to-standardised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5367e228-3f5d-4f8f-87d2-cf0ab7feca02"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Prerequisites\n",
					"1. Make sure the new raw file's entry has been added to the Orchestration i.e `/infrastructure/configuration/data-lake/orchestration/orchestration.json`\n",
					"2. Make sure the standardised table's schema is present on the path specified in the entry added in step 1.\n",
					"3. Only if the raw file is huge (several GBs), the spark pool might need some upscaling. Hence the following cell\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Configure spark session"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"%%configure -f\n",
					"{\n",
					"    \"conf\": {\n",
					"        \"spark.kryoserializer.buffer.max\": \"2047m\",\n",
					"        \"spark.driver.maxResultSize\": \"10g\",\n",
					"        \"spark.rpc.message.maxSize\": \"1280\"\n",
					"    }\n",
					"}"
				],
				"execution_count": 58
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import Required libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os"
				],
				"execution_count": 59
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise the parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"date_folder=''\n",
					"source_folder='AIEDocumentData'\n",
					"source_frequency_folder=''\n",
					"specific_file='' # if not provided, it will ingest all files in the date_folder\n",
					"isMultiLine = True\n",
					"delete_existing_table=False\n",
					"dataAttribute = \"\"\n",
					"start_exec_time = datetime.now() \n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": 60
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Logging decorator"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 61
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 62
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Get the Storage Account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"raw_container = \"abfss://odw-raw@\" + storage_account\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message =''"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run \"1-odw-raw-to-standardised/Fileshare/SAP_HR/py_1_raw_to_standardised_hr_functions\""
				],
				"execution_count": 64
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Ingest the data from the raw/source into the standardised table. \n",
					"If the table doesn't already exist, this will create the table first and ingest the data."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"\n",
					"# ==================== Imports ====================\n",
					"from datetime import datetime, timedelta\n",
					"import json\n",
					"from concurrent.futures import ThreadPoolExecutor, as_completed\n",
					"\n",
					"# ==================== Safe getters for pipeline vars ====================\n",
					"def g(name, default=\"\"):\n",
					"    return globals().get(name, default)\n",
					"\n",
					"PipelineName = g(\"PipelineName\", mssparkutils.runtime.context.get(\"currentNotebookName\", \"Unknown\"))\n",
					"PipelineRunID = g(\"PipelineRunID\", mssparkutils.runtime.context.get(\"runId\", \"\"))\n",
					"PipelineTriggerID = g(\"PipelineTriggerID\", \"\")\n",
					"PipelineTriggerName = g(\"PipelineTriggerName\", \"\")\n",
					"PipelineTriggerType = g(\"PipelineTriggerType\", \"\")\n",
					"PipelineTriggeredbyPipelineName = g(\"PipelineTriggeredbyPipelineName\", \"\")\n",
					"PipelineTriggeredbyPipelineRunID = g(\"PipelineTriggeredbyPipelineRunID\", \"\")\n",
					"\n",
					"# ==================== ingest_adhoc with error capture ====================\n",
					"def ingest_adhoc(storage_account, definition, source_path, source_file_name,\n",
					"                 expected_from, expected_to, process_name, isMultiLine, dataAttribute):\n",
					"    try:\n",
					"        full_path = f\"{source_path}/{source_file_name}\"\n",
					"        if not mssparkutils.fs.exists(full_path):\n",
					"            msg = f\"Source file not found: {full_path}\"\n",
					"            logError(msg)\n",
					"            return True, 0, msg\n",
					"\n",
					"        logInfo(f\"[{process_name}] Reading raw file: {full_path}\")\n",
					"        ext = (source_file_name.rsplit(\".\", 1)[-1] or \"\").lower()\n",
					"\n",
					"        if ext == \"json\":\n",
					"            df_raw = spark.read.option(\"multiline\", str(isMultiLine).lower()).json(full_path)\n",
					"        elif ext in (\"csv\", \"txt\"):\n",
					"            df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(full_path)\n",
					"        elif ext == \"parquet\":\n",
					"            df_raw = spark.read.parquet(full_path)\n",
					"        else:\n",
					"            msg = f\"Unsupported file extension '{ext}' for {source_file_name}\"\n",
					"            logError(msg)\n",
					"            return True, 0, msg\n",
					"\n",
					"        row_count = df_raw.count()\n",
					"        logInfo(f\"[{process_name}] Read {row_count} rows from {source_file_name}\")\n",
					"\n",
					"        target_table = f\"odw_standardised_db.{definition['Standardised_Table_Name']}\"\n",
					"        logInfo(f\"[{process_name}] Writing to {target_table}\")\n",
					"        df_raw.write.mode(\"append\").saveAsTable(target_table)\n",
					"\n",
					"        logInfo(f\"[{process_name}] Successfully wrote {row_count} rows to {target_table}\")\n",
					"        return False, row_count, \"\"\n",
					"    except Exception as e:\n",
					"        error_message = f\"Ingestion error for {source_file_name} → {type(e).__name__}: {str(e)}\"\n",
					"        logError(error_message)\n",
					"        logException(e)\n",
					"        return True, 0, error_message\n",
					"\n",
					"# ==================== Resolve date_folder ====================\n",
					"if 'date_folder' not in globals():\n",
					"    date_folder = ''\n",
					"\n",
					"if date_folder == '':\n",
					"    date_folder = datetime.now().date()\n",
					"else:\n",
					"    date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\").date()\n",
					"\n",
					"date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"\n",
					"# ==================== Paths & orchestration ====================\n",
					"source_folder_path = source_folder if not source_frequency_folder else f\"{source_folder}/{source_frequency_folder}\"\n",
					"source_path = f\"{raw_container}{source_folder_path}/{date_folder_str}\"\n",
					"\n",
					"path_to_orchestration_file = f\"abfss://odw-config@{storage_account}/orchestration/orchestration.json\"\n",
					"df = spark.read.option(\"multiline\", \"true\").json(path_to_orchestration_file)\n",
					"definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"process_name = \"py_raw_to_std\"\n",
					"\n",
					"# ==================== Telemetry helpers ====================\n",
					"def build_params(stage, start_exec_time, end_exec_time, error_message, status_message,\n",
					"                 insert_count=0, update_count=0, delete_count=0,\n",
					"                 target_table=\"\", source_file=\"\"):\n",
					"    duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"    activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"    status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					"    return {\n",
					"        \"Stage\": stage,\n",
					"        \"PipelineName\": PipelineName,\n",
					"        \"PipelineRunID\": PipelineRunID,\n",
					"        \"StartTime\": start_exec_time.isoformat(),\n",
					"        \"EndTime\": end_exec_time.isoformat(),\n",
					"        \"Inserts\": insert_count,\n",
					"        \"Updates\": update_count,\n",
					"        \"Deletes\": delete_count,\n",
					"        \"ErrorMessage\": error_message,\n",
					"        \"StatusMessage\": status_message,\n",
					"        \"PipelineTriggerID\": PipelineTriggerID,\n",
					"        \"PipelineTriggerName\": PipelineTriggerName,\n",
					"        \"PipelineTriggerType\": PipelineTriggerType,\n",
					"        \"PipelineTriggeredbyPipelineName\": PipelineTriggeredbyPipelineName,\n",
					"        \"PipelineTriggeredbyPipelineRunID\": PipelineTriggeredbyPipelineRunID,\n",
					"        \"PipelineExecutionTimeInSec\": duration_seconds,\n",
					"        \"ActivityType\": activity_type,\n",
					"        \"DurationSeconds\": duration_seconds,\n",
					"        \"StatusCode\": status_code,\n",
					"        \"AppInsCustomEventName\": \"ODW_Master_Pipeline_Logs\",\n",
					"        \"TargetTable\": target_table,\n",
					"        \"SourceFile\": source_file\n",
					"    }\n",
					"\n",
					"def send_all_telemetry_sync(events, max_workers=4):\n",
					"    if not events:\n",
					"        return\n",
					"    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
					"        futures = [ex.submit(send_telemetry_to_app_insights, p) for p in events]\n",
					"        for f in as_completed(futures):\n",
					"            f.result()\n",
					"\n",
					"# ==================== Telemetry collection ====================\n",
					"telemetry_events = []\n",
					"any_failure = False\n",
					"\n",
					"# ==================== List files ====================\n",
					"try:\n",
					"    logInfo(f\"Reading from {source_path}\")\n",
					"    if not mssparkutils.fs.exists(source_path):\n",
					"        raise FileNotFoundError(f\"Path does not exist: {source_path}\")\n",
					"    files = mssparkutils.fs.ls(source_path)\n",
					"except Exception as e:\n",
					"    logError(f\"Raw path not found: {source_path}\")\n",
					"    logException(e)\n",
					"    start_exec_time = datetime.now()\n",
					"    end_exec_time = datetime.now()\n",
					"    telemetry_events.append(build_params(\n",
					"        stage=\"Failed\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        error_message=str(e),\n",
					"        status_message=f\"Failed to read raw files from {source_path}\"\n",
					"    ))\n",
					"    any_failure = True\n",
					"    files = []\n",
					"\n",
					"# ==================== Process files ====================\n",
					"for file in files:\n",
					"    if source_folder == 'ServiceBus' and file.name.endswith('.json'):\n",
					"        continue\n",
					"    if specific_file != '' and not file.name.startswith(specific_file + '.'):\n",
					"        continue\n",
					"\n",
					"    definition = next((d for d in definitions if\n",
					"                       (specific_file == '' or d['Source_Filename_Start'] == specific_file)\n",
					"                       and (not source_frequency_folder or d['Source_Frequency_Folder'] == source_frequency_folder)\n",
					"                       and file.name.startswith(d['Source_Filename_Start'])), None)\n",
					"    if not definition:\n",
					"        logInfo(f\"No matching definition for {file.name}, skipping.\")\n",
					"        continue\n",
					"\n",
					"    expected_from = datetime.combine(date_folder - timedelta(days=1), datetime.min.time())\n",
					"    expected_to = expected_from + timedelta(days=definition['Expected_Within_Weekdays'])\n",
					"\n",
					"    if delete_existing_table:\n",
					"        logInfo(f\"Deleting existing table odw_standardised_db.{definition['Standardised_Table_Name']}\")\n",
					"        mssparkutils.notebook.run('/utils/py_delete_table', 300,\n",
					"                                  arguments={'db_name': 'odw_standardised_db',\n",
					"                                             'table_name': definition['Standardised_Table_Name']})\n",
					"\n",
					"    start_exec_time = datetime.now()\n",
					"    logInfo(f\"Ingesting {file.name}\")\n",
					"    ingestion_failure, row_count, error_message = ingest_adhoc(\n",
					"        storage_account, definition, source_path, file.name,\n",
					"        expected_from, expected_to, process_name, isMultiLine, dataAttribute\n",
					"    )\n",
					"\n",
					"    # ✅ Guard clause fix\n",
					"    if error_message:\n",
					"        any_failure = True\n",
					"        logError(f\"Skipping downstream steps for {file.name} due to error: {error_message}\")\n",
					"        # Build failure telemetry\n",
					"        end_exec_time = datetime.now()\n",
					"        telemetry_events.append(build_params(\n",
					"            stage=\"Failed\",\n",
					"            start_exec_time=start_exec_time,\n",
					"            end_exec_time=end_exec_time,\n",
					"            error_message=error_message,\n",
					"            status_message=f\"Failed to load data from {file.name} into {definition['Standardised_Table_Name']} table\",\n",
					"            target_table=f\"odw_standardised_db.{definition['Standardised_Table_Name']}\",\n",
					"            source_file=file.name\n",
					"        ))\n",
					"        continue  # Skip downstream steps for this file\n",
					"    else:\n",
					"        logInfo(f\"Continuing downstream steps for {file.name}\")\n",
					"        # Add downstream logic here if needed (QC, transforms, etc.)\n",
					"\n",
					"    # Build success telemetry\n",
					"    end_exec_time = datetime.now()\n",
					"    telemetry_events.append(build_params(\n",
					"        stage=\"Success\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        error_message=\"\",\n",
					"        status_message=f\"Successfully loaded data from {file.name} into {definition['Standardised_Table_Name']} table\",\n",
					"        insert_count=row_count,\n",
					"        target_table=f\"odw_standardised_db.{definition['Standardised_Table_Name']}\",\n",
					"        source_file=file.name\n",
					"    ))\n",
					"    logInfo(f\"Ingested {row_count} rows from {file.name}\")\n",
					"\n",
					"# ==================== Send telemetry & exit ====================\n",
					"try:\n",
					"    send_all_telemetry_sync(telemetry_events)\n",
					"except Exception as e:\n",
					"    logError(f\"Telemetry sending failed: {e}\")\n",
					"    logException(e)\n",
					"    any_failure = True\n",
					"finally:\n",
					"    if any_failure:\n",
					"        print(\"Notebook finished with failures. Exiting with code 1.\")\n",
					"        mssparkutils.notebook.exit(\"1\")\n",
					"    else:\n",
					"        logInfo(\"Notebook finished successfully.\")\n",
					"        mssparkutils.notebook.exit(\"0\")\n",
					""
				],
				"execution_count": 65
			}
		]
	}
}