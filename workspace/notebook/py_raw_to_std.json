{
	"name": "py_raw_to_std",
	"properties": {
		"folder": {
			"name": "1-odw-raw-to-standardised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "976f2c82-a9f1-4e65-abdf-beda37829f1b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Prerequisites\n",
					"1. Make sure the new raw file's entry has been added to the Orchestration i.e `/infrastructure/configuration/data-lake/orchestration/orchestration.json`\n",
					"2. Make sure the standardised table's schema is present on the path specified in the entry added in step 1.\n",
					"3. Only if the raw file is huge (several GBs), the spark pool might need some upscaling. Hence the following cell\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Configure spark session"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"%%configure -f\n",
					"{\n",
					"    \"conf\": {\n",
					"        \"spark.kryoserializer.buffer.max\": \"2047m\",\n",
					"        \"spark.driver.maxResultSize\": \"10g\",\n",
					"        \"spark.rpc.message.maxSize\": \"1280\"\n",
					"    }\n",
					"}"
				],
				"execution_count": 181
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"PipelineName = \"pln_aie_document_data\"\n",
					"PipelineRunID = \"f96480d7-8e9d-406d-9e34-974f84b14cbb\"\n",
					"PipelineTriggerID = \"af25d427-f06a-4ef9-85fb-79363a14090b\"\n",
					"PipelineTriggerName = \"af25d427-f06a-4ef9-85fb-79363a14090b\"\n",
					"PipelineTriggerType = \"PipelineActivity\"\n",
					"PipelineTriggeredbyPipelineName = \"pln_all_horizon_data\"\n",
					"PipelineTriggeredbyPipelineRunID = \"3334ecfd-6fba-41ee-971e-5ab3f9eda134\""
				],
				"execution_count": 182
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import Required libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os"
				],
				"execution_count": 183
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise the parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"date_folder=''\n",
					"source_folder='AIEDocumentData'\n",
					"source_frequency_folder=''\n",
					"specific_file='' # if not provided, it will ingest all files in the date_folder\n",
					"isMultiLine = True\n",
					"delete_existing_table=False\n",
					"dataAttribute = \"\"\n",
					"start_exec_time = datetime.now() \n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": 184
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Logging decorator"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 185
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 186
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Get the Storage Account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"raw_container = \"abfss://odw-raw@\" + storage_account\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message =''"
				],
				"execution_count": 187
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run \"1-odw-raw-to-standardised/Fileshare/SAP_HR/py_1_raw_to_standardised_hr_functions\""
				],
				"execution_count": 188
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder.getOrCreate()\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"source_path: str = f\"abfss://odw-raw@{storage_account}{source_folder}/\"\n",
					"standardised_container = f\"abfss://odw-standardised@{storage_account}\"\n",
					"print(standardised_container)"
				],
				"execution_count": 189
			},
			{
				"cell_type": "code",
				"source": [
					"# List all items in the directory\n",
					"items = mssparkutils.fs.ls(source_path)\n",
					"# Filter for directories and get their names and modification times\n",
					"folders = [(item.name, item.modifyTime) for item in items if item.isDir]\n",
					"# Sort folders by modification time in descending order\n",
					"sorted_folders = sorted(folders, key=lambda x: x[1], reverse=True)\n",
					"# Get the name of the latest modified folder\n",
					"if sorted_folders:\n",
					"    latest_folder = sorted_folders[0][0]\n",
					"    source_path=f\"{source_path}{latest_folder}\"\n",
					"    print(f\"Latest modified folder: {latest_folder}\")\n",
					"    print(source_path)\n",
					"else:\n",
					"    print(\"No folders found in the specified directory.\")"
				],
				"execution_count": 190
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"@logging_to_appins\n",
					"def ingest_adhoc(storage_account,\n",
					"                    definition,\n",
					"                    folder_path,\n",
					"                    filename,\n",
					"                    expected_from,\n",
					"                    expected_to,\n",
					"                    process_name,\n",
					"                    isMultilineJSON=False,\n",
					"                    dataAttribute=None):\n",
					" \n",
					"    from pyspark.sql import SparkSession\n",
					"    from notebookutils import mssparkutils\n",
					"    import json\n",
					"    from datetime import datetime, timedelta, date\n",
					"    import pandas as pd\n",
					"    from pyspark.sql.types import StringType,DateType,TimestampType,IntegerType, FloatType, StructType, StructField\n",
					"    import re\n",
					"    from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat, count\n",
					"    import pyspark.sql.types as types\n",
					" \n",
					"    ingestion_failure: bool = False\n",
					"    error_message = \"\"\n",
					"    end_exec_time = None\n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
					" \n",
					"    standardised_container = \"abfss://odw-standardised@\"+storage_account\n",
					"    standardised_path = definition['Standardised_Path'] + \"/\"\n",
					"    standardised_table_name = definition['Standardised_Table_Name']\n",
					"    source_filename_start = definition['Source_Filename_Start']\n",
					" \n",
					"    logging_container = f\"abfss://logging@{storage_account}\"\n",
					"    logging_table_name = 'tables_logs'\n",
					"    ingestion_log_table_location = logging_container + logging_table_name\n",
					" \n",
					"    if 'Standardised_Table_Definition' in definition:\n",
					"        standardised_table_loc = \"abfss://odw-config@\"+storage_account + definition['Standardised_Table_Definition']\n",
					"        standardised_table_def_json = spark.read.text(standardised_table_loc, wholetext=True).first().value\n",
					"    else:\n",
					"        standardised_table_def_json = mssparkutils.notebook.run('/py_get_schema_from_url', 30, {'db_name': 'odw_standardised_db', 'entity_name': definition['Source_Frequency_Folder']})\n",
					" \n",
					"    if not any([table.name.lower() == standardised_table_name.lower() for table in spark.catalog.listTables('odw_standardised_db')]):\n",
					"        create_table_from_schema(standardised_table_def_json, \"odw_standardised_db\", standardised_table_name,standardised_container , standardised_path+standardised_table_name)  \n",
					" \n",
					"    #Check that we're dealing with delta already. If not we will convert the table first\n",
					"    table_metadata = spark.sql(f\"DESCRIBE EXTENDED odw_standardised_db.{standardised_table_name}\")\n",
					"    data_format = table_metadata.filter(table_metadata.col_name == \"Provider\").collect()[0].data_type\n",
					" \n",
					"    if data_format == \"parquet\":\n",
					"        replace = spark.sql(f\"SELECT * FROM odw_standardised_db.{standardised_table_name}\")\n",
					"        replace.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"overwrite\").saveAsTable(f\"odw_standardised_db.{standardised_table_name}_new\")\n",
					" \n",
					"        # Drop the original table\n",
					"        spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.{standardised_table_name}\")\n",
					"       \n",
					"        # Rename the temporary table to replace the original table\n",
					"        spark.sql(f\"ALTER TABLE odw_standardised_db.{standardised_table_name}_new RENAME TO odw_standardised_db.{standardised_table_name}\")\n",
					" \n",
					"    try:\n",
					"        standardised_table_location = spark.sql(f\"DESCRIBE FORMATTED odw_standardised_db.{standardised_table_name}\") \\\n",
					"                    .filter(\"col_name = 'Location'\") \\\n",
					"                    .select(\"data_type\") \\\n",
					"                    .collect()[0][0]\n",
					"    except:\n",
					"        standardised_table_location = standardised_container+standardised_path+standardised_table_name\n",
					"                   \n",
					"    standardised_table_df = spark.read.format(\"delta\").load(standardised_table_location)\n",
					"    rows = standardised_table_df.filter((standardised_table_df.expected_from == expected_from) &  (standardised_table_df.expected_to == expected_to)).count()\n",
					" \n",
					"    jobId = mssparkutils.env.getJobId()\n",
					" \n",
					"    ### mount the data lake storage in Synapse to the Synapse File Mount API\n",
					"    mount_storage(path=folder_path)\n",
					" \n",
					"    logInfo(f\"Reading {filename}\")\n",
					" \n",
					"    ### open .csv and .xlsx files using pandas                                \n",
					"    if \".xlsx\" in filename.lower():\n",
					"        sheet_name = definition['Source_Sheet_Name'] if 'Source_Sheet_Name' in definition else 0\n",
					"        df = pd.read_excel(f\"/synfs/{jobId}/temp_raw/{filename}\", dtype=str, sheet_name=sheet_name, na_filter=False)\n",
					"    elif '.csv' in filename.lower():\n",
					"        df = spark.read.options(quote='\"', escape='\\\\', encoding='utf8', header=True, multiLine=True, columnNameOfCorruptRecord='corrupted_records', mode=\"PERMISSIVE\").csv(f\"{folder_path}/{filename}\")\n",
					" \n",
					"        if \"corrupted_records\" in df.columns:\n",
					"            print(f\"Corrupted Records detected from CSV ingestion in {filename}\")\n",
					"            ingestion_failure = True\n",
					"           \n",
					" \n",
					"    elif '.json' in filename.lower():\n",
					"        if isMultilineJSON == False:\n",
					"            df = spark.read.json(f\"{folder_path}/{filename}\")\n",
					"        else:\n",
					"            logInfo(\"Reading multiline JSON\")\n",
					"            df = spark.read.option(\"multiline\", \"true\").json(f\"{folder_path}/{filename}\")\n",
					" \n",
					"            #we need to pull the data from a specific data attribute\n",
					"            if dataAttribute:\n",
					"                dfs: list = [] # an empty list to store the data frames\n",
					"                for row in df.select(dataAttribute).collect():\n",
					"                    for data in row[dataAttribute]:\n",
					"                        dfs.append(data)\n",
					"                df = spark.createDataFrame(dfs)\n",
					"    else:\n",
					"        error_message = f\"This file type for {filename} is unsupported\"\n",
					"        end_exec_time = datetime.now()\n",
					"        raise RuntimeError(f\"This file type for {filename} is unsupported\")\n",
					" \n",
					"    ### drop headerless columns\n",
					"    sparkDF = df.select([col for col in df.columns if not col.startswith('Unnamed')])\n",
					"    # rows_raw = len(df.index)\n",
					"    rows_raw = sparkDF.count()\n",
					" \n",
					"    try:\n",
					"        unmount_storage()\n",
					"    except Exception as e:\n",
					"        logInfo('Unable to unmount storage')\n",
					" \n",
					"    ### add date columns included with every standardised table\n",
					"    sparkDF = sparkDF.withColumn(\"ingested_datetime\",current_timestamp())\n",
					"    sparkDF = sparkDF.withColumn(\"ingested_by_process_name\", lit(process_name))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_from\",lit(expected_from))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_to\",lit(expected_to))\n",
					"    sparkDF = sparkDF.withColumn(\"input_file\", input_file_name())\n",
					"    sparkDF = sparkDF.withColumn(\"modified_datetime\", current_timestamp())\n",
					"    sparkDF = sparkDF.withColumn(\"modified_by_process_name\", lit(process_name))\n",
					"    sparkDF = sparkDF.withColumn(\"entity_name\", lit(source_filename_start))\n",
					"    sparkDF = sparkDF.withColumn(\"file_ID\", sha2(concat(lit(input_file_name()), current_timestamp().cast(\"string\")), 256))\n",
					" \n",
					"    ### change any array field to string\n",
					"    schema = json.loads(standardised_table_def_json)\n",
					"    for field in schema['fields']:\n",
					"        if field['type'] == 'array':\n",
					"            field['type'] = 'string'\n",
					"    schema = StructType.fromJson(schema)\n",
					" \n",
					"    ### remove characters that Delta can't allow in headers and add numbers to repeated column headers\n",
					"    cols_orig = sparkDF.schema.names\n",
					"    cols=[re.sub('[^0-9a-zA-Z]+', '_', i).lower() for i in cols_orig]\n",
					"    cols=[colm.rstrip('_') for colm in cols]\n",
					"    newlist = []\n",
					"    for i, v in enumerate(cols):\n",
					"        totalcount = cols.count(v)\n",
					"        count = cols[:i].count(v)\n",
					"        newlist.append(v + str(count + 1) if totalcount > 1 else v)\n",
					"    for colix in range(len(cols_orig)):\n",
					"        sparkDF = sparkDF.toDF(*newlist)\n",
					" \n",
					"    ### Cast any column in sparkDF with type mismatch\n",
					"    for field in sparkDF.schema:\n",
					"        table_field = next((f for f in schema if f.name.lower() == field.name.lower()), None)\n",
					"        if table_field is not None and field.dataType != table_field.dataType:\n",
					"            sparkDF = sparkDF.withColumn(field.name, col(field.name).cast(table_field.dataType))\n",
					" \n",
					"    ### writing the dataframe to the existing standardised table\n",
					"    logInfo(f\"Writing data to odw_standardised_db.{standardised_table_name}\")\n",
					"    sparkDF.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").saveAsTable(f\"odw_standardised_db.{standardised_table_name}\")\n",
					"    logInfo(f\"Written data to odw_standardised_db.{standardised_table_name}\")\n",
					" \n",
					"    standardised_table_df_new = spark.read.format(\"delta\").load(standardised_table_location)\n",
					"    rows_new = standardised_table_df.filter((standardised_table_df.expected_from == expected_from) &  (standardised_table_df.expected_to == expected_to)).count()\n",
					" \n",
					" \n",
					"    try:\n",
					"        ### Define schema for the ingestion log table with all required columns\n",
					"        ingestion_log_schema_loc = \"abfss://odw-config@\" + storage_account + \"tables_logs.json\"\n",
					"        ingestion_log_schema = spark.read.text(ingestion_log_schema_loc, wholetext=True).first().value\n",
					" \n",
					"        ### Try loading the ingestion log table, if it exists\n",
					"        try:\n",
					"            ingestion_log_df = spark.read.format(\"delta\").load(ingestion_log_table_location)\n",
					"            table_exists = True\n",
					"        except Exception as e:\n",
					"            logInfo(f\"Ingestion log table not found at {ingestion_log_table_location}. Creating a new one.\")\n",
					"            table_exists = False\n",
					" \n",
					"        ### Extract a single row with the logging columns\n",
					"        new_log_entry = sparkDF.select(\n",
					"            \"file_ID\",\n",
					"            \"ingested_datetime\",\n",
					"            \"ingested_by_process_name\",\n",
					"            \"input_file\",\n",
					"            \"modified_datetime\",\n",
					"            \"modified_by_process_name\",\n",
					"            \"entity_name\"\n",
					"        ).limit(1)\n",
					" \n",
					"        new_log_entry = new_log_entry.withColumn(\"rows_raw\", lit(sparkDF.count()))\n",
					"        new_log_entry = new_log_entry.withColumn(\"rows_new\", lit(standardised_table_df.filter((standardised_table_df.expected_from == expected_from) &  (standardised_table_df.expected_to == expected_to)).count()))\n",
					" \n",
					" \n",
					"        if not table_exists:\n",
					"            # Create the table\n",
					"            new_log_entry.write.format(\"delta\").option(\"path\", ingestion_log_table_location).saveAsTable(f\"logging.{logging_table_name}\")\n",
					"            logInfo(f\"Updating ingestion logging table {logging_table_name} with first entry.\")\n",
					"        else:\n",
					"            # Appending log entry to existing table - using the catalog directly\n",
					"            new_log_entry.write.format(\"delta\").mode(\"append\").saveAsTable(f\"logging.{logging_table_name}\")\n",
					"            logInfo(\"Appended to existing ingestion logging table with new entry\")\n",
					"   \n",
					"    except Exception as e:\n",
					"        logInfo('Logging to tables_logs failed')\n",
					"        error_message =f\"Logging to tables_logs failed\"\n",
					"        end_exec_time = datetime.now()\n",
					" \n",
					"    ### Test correct number of rows have written\n",
					"    if rows_raw <= rows_new:\n",
					"        #count up the rows which match the dates, we should at LEAST have those, multiple runs on the same day will lead to the counts not matching however\n",
					"        logInfo('All rows have successfully been written')\n",
					"    else:\n",
					"        logError(f\"All rows have NOT been successfully written. Expected {rows_raw} but {rows_new} written\")\n",
					"        error_message = f\"All rows have NOT been successfully written. Expected {rows_raw} but {rows_new} written\"\n",
					"        end_exec_time = datetime.now()\n",
					"        ingestion_failure = True\n",
					"    return ingestion_failure, rows_raw,end_exec_time,error_message\n",
					" "
				],
				"execution_count": 160
			},
			{
				"cell_type": "code",
				"source": [
					"# ==================== Imports ====================\n",
					"from datetime import datetime, timedelta\n",
					"import json\n",
					"from concurrent.futures import ThreadPoolExecutor, as_completed\n",
					"\n",
					"\n",
					"# ==================== ingest_adhoc with error capture ====================\n",
					"\n",
					"def ingest_adhoc(storage_account, definition, source_path, source_file_name,\n",
					"                 expected_from, expected_to, process_name, isMultiLine, dataAttribute):\n",
					"    try:\n",
					"        full_path = f\"{source_path}/{source_file_name}\"\n",
					"        if not mssparkutils.fs.exists(full_path):\n",
					"            msg = f\"Source file not found: {full_path}\"\n",
					"            logError(msg)\n",
					"            return True, 0, msg\n",
					"\n",
					"        logInfo(f\"[{process_name}] Reading raw file: {full_path}\")\n",
					"        ext = (source_file_name.rsplit(\".\", 1)[-1] or \"\").lower()\n",
					"\n",
					"        if ext == \"json\":\n",
					"            df_raw = spark.read.option(\"multiline\", str(isMultiLine).lower()).json(full_path)\n",
					"        elif ext in (\"csv\", \"txt\"):\n",
					"            df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(full_path)\n",
					"        elif ext == \"parquet\":\n",
					"            df_raw = spark.read.parquet(full_path)\n",
					"        else:\n",
					"            msg = f\"Unsupported file extension '{ext}' for {source_file_name}\"\n",
					"            logError(msg)\n",
					"            return True, 0, msg\n",
					"\n",
					"        df_raw = df_raw.toDF(*[c.lower() for c in df_raw.columns])\n",
					"\n",
					"        for col_name in [\"version\", \"size\", \"documentid\"]:\n",
					"            if col_name in df_raw.columns:\n",
					"                df_raw = df_raw.withColumn(col_name, df_raw[col_name].cast(\"string\"))\n",
					"\n",
					"        row_count = df_raw.count()\n",
					"        logInfo(f\"[{process_name}] Read {row_count} rows from {source_file_name}\")\n",
					"\n",
					"        target_table = f\"odw_standardised_db.{definition['Standardised_Table_Name']}\"\n",
					"        logInfo(f\"[{process_name}] Writing to {target_table}\")\n",
					"\n",
					"        df_raw.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(target_table)\n",
					"\n",
					"        logInfo(f\"[{process_name}] Successfully wrote {row_count} rows to {target_table}\")\n",
					"        return False, row_count, \"\"\n",
					"    except Exception as e:\n",
					"        error_message = f\"Ingestion error for {source_file_name} â†’ {type(e).__name__}: {str(e)}\"\n",
					"        logError(error_message)\n",
					"        logException(e)\n",
					"        return True, 0, error_message\n",
					"\n",
					"\n",
					"# ==================== Build base path (NO DATE ANYMORE) ====================\n",
					"source_folder_path = source_folder if not source_frequency_folder else f\"{source_folder}/{source_frequency_folder}\"\n",
					"source_base_path = f\"{raw_container}{source_folder_path}\"\n",
					"\n",
					"print(f\"Scanning base folder for latest modified subfolder: {source_base_path}\")\n",
					"\n",
					"# ==================== Find latest modified folder ====================\n",
					"try:\n",
					"    items = mssparkutils.fs.ls(source_base_path)\n",
					"    folders = [(item.name, item.modifyTime) for item in items if item.isDir]\n",
					"\n",
					"    if folders:\n",
					"        sorted_folders = sorted(folders, key=lambda x: x[1], reverse=True)\n",
					"        latest_folder = sorted_folders[0][0]\n",
					"        source_path = f\"{source_base_path}/{latest_folder}\"\n",
					"        print(f\"Latest modified folder: {latest_folder}\")\n",
					"        print(f\"Using source_path: {source_path}\")\n",
					"    else:\n",
					"        raise FileNotFoundError(\"No subfolders found in raw source directory.\")\n",
					"except Exception as e:\n",
					"    raise Exception(f\"Failed to locate latest modified folder under {source_base_path}: {str(e)}\")\n",
					"\n",
					"# ==================== Load orchestration ====================\n",
					"path_to_orchestration_file = f\"abfss://odw-config@{storage_account}/orchestration/orchestration.json\"\n",
					"df = spark.read.option(\"multiline\", \"true\").json(path_to_orchestration_file)\n",
					"definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"process_name = \"py_raw_to_std\"\n",
					"\n",
					"# ==================== Telemetry  ====================\n",
					"def build_params(stage, start_exec_time, end_exec_time, error_message, status_message,\n",
					"                 insert_count=0, update_count=0, delete_count=0,\n",
					"                 target_table=\"\", source_file=\"\"):\n",
					"    duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"    activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"    status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					"    return {\n",
					"        \"Stage\": stage,\n",
					"        \"PipelineName\": PipelineName,\n",
					"        \"PipelineRunID\": PipelineRunID,\n",
					"        \"StartTime\": start_exec_time.isoformat(),\n",
					"        \"EndTime\": end_exec_time.isoformat(),\n",
					"        \"Inserts\": insert_count,\n",
					"        \"Updates\": update_count,\n",
					"        \"Deletes\": delete_count,\n",
					"        \"ErrorMessage\": error_message,\n",
					"        \"StatusMessage\": status_message,\n",
					"        \"PipelineTriggerID\": PipelineTriggerID,\n",
					"        \"PipelineTriggerName\": PipelineTriggerName,\n",
					"        \"PipelineTriggerType\": PipelineTriggerType,\n",
					"        \"PipelineTriggeredbyPipelineName\": PipelineTriggeredbyPipelineName,\n",
					"        \"PipelineTriggeredbyPipelineRunID\": PipelineTriggeredbyPipelineRunID,\n",
					"        \"PipelineExecutionTimeInSec\": duration_seconds,\n",
					"        \"ActivityType\": activity_type,\n",
					"        \"DurationSeconds\": duration_seconds,\n",
					"        \"StatusCode\": status_code,\n",
					"        \"AppInsCustomEventName\": \"ODW_Master_Pipeline_Logs\",\n",
					"        \"TargetTable\": target_table,\n",
					"        \"SourceFile\": source_file\n",
					"    }\n",
					"\n",
					"def send_all_telemetry_sync(events, max_workers=4):\n",
					"    if not events:\n",
					"        return\n",
					"    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
					"        futures = [ex.submit(send_telemetry_to_app_insights, p) for p in events]\n",
					"        for f in as_completed(futures):\n",
					"            f.result()\n",
					"\n",
					"# ==================== Telemetry collection ====================\n",
					"telemetry_events = []\n",
					"any_failure = False\n",
					"\n",
					"# ==================== List files ====================\n",
					"try:\n",
					"    logInfo(f\"Reading from {source_path}\")\n",
					"    if not mssparkutils.fs.exists(source_path):\n",
					"        raise FileNotFoundError(f\"Path does not exist: {source_path}\")\n",
					"    files = mssparkutils.fs.ls(source_path)\n",
					"except Exception as e:\n",
					"    logError(f\"Raw path not found: {source_path}\")\n",
					"    logException(e)\n",
					"    start_exec_time = datetime.now()\n",
					"    end_exec_time = datetime.now()\n",
					"    telemetry_events.append(build_params(\n",
					"        stage=\"Failed\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        error_message=str(e),\n",
					"        status_message=f\"Failed to read raw files from {source_path}\"\n",
					"    ))\n",
					"    any_failure = True\n",
					"    files = []\n",
					"\n",
					"# ==================== Process files ====================\n",
					"for file in files:\n",
					"    if source_folder == 'ServiceBus' and file.name.endswith('.json'):\n",
					"        continue\n",
					"    if specific_file != '' and not file.name.startswith(specific_file + '.'):\n",
					"        continue\n",
					"\n",
					"    definition = next((d for d in definitions if\n",
					"                       (specific_file == '' or d['Source_Filename_Start'] == specific_file)\n",
					"                       and (not source_frequency_folder or d['Source_Frequency_Folder'] == source_frequency_folder)\n",
					"                       and file.name.startswith(d['Source_Filename_Start'])), None)\n",
					"    if not definition:\n",
					"        logInfo(f\"No matching definition for {file.name}, skipping.\")\n",
					"        continue\n",
					"\n",
					"    expected_from = datetime.now()\n",
					"    expected_to = expected_from\n",
					"\n",
					"    if delete_existing_table:\n",
					"        logInfo(f\"Deleting existing table odw_standardised_db.{definition['Standardised_Table_Name']}\")\n",
					"        mssparkutils.notebook.run('/utils/py_delete_table', 300,\n",
					"                                  arguments={'db_name': 'odw_standardised_db',\n",
					"                                             'table_name': definition['Standardised_Table_Name']})\n",
					"\n",
					"    start_exec_time = datetime.now()\n",
					"    logInfo(f\"Ingesting {file.name}\")\n",
					"    ingestion_failure, row_count, error_message = ingest_adhoc(\n",
					"        storage_account, definition, source_path, file.name,\n",
					"        expected_from, expected_to, process_name, isMultiLine, dataAttribute\n",
					"    )\n",
					"\n",
					"    if error_message:\n",
					"        any_failure = True\n",
					"        logError(f\"Skipping downstream steps for {file.name} due to error: {error_message}\")\n",
					"        end_exec_time = datetime.now()\n",
					"        telemetry_events.append(build_params(\n",
					"            stage=\"Failed\",\n",
					"            start_exec_time=start_exec_time,\n",
					"            end_exec_time=end_exec_time,\n",
					"            error_message=error_message,\n",
					"            status_message=f\"Failed to load data from {file.name} into {definition['Standardised_Table_Name']} table\",\n",
					"            target_table=f\"odw_standardised_db.{definition['Standardised_Table_Name']}\",\n",
					"            source_file=file.name\n",
					"        ))\n",
					"        continue\n",
					"    else:\n",
					"        logInfo(f\"Continuing downstream steps for {file.name}\")\n",
					"\n",
					"    end_exec_time = datetime.now()\n",
					"    telemetry_events.append(build_params(\n",
					"        stage=\"Success\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        error_message=\"\",\n",
					"        status_message=f\"Successfully loaded data from {file.name} into {definition['Standardised_Table_Name']} table\",\n",
					"        insert_count=row_count,\n",
					"        target_table=f\"odw_standardised_db.{definition['Standardised_Table_Name']}\",\n",
					"        source_file=file.name\n",
					"    ))\n",
					"    logInfo(f\"Ingested {row_count} rows from {file.name}\")\n",
					"\n",
					"# ==================== Send telemetry & exit ====================\n",
					"try:\n",
					"    send_all_telemetry_sync(telemetry_events)\n",
					"except Exception as e:\n",
					"    logError(f\"Telemetry sending failed: {e}\")\n",
					"    logException(e)\n",
					"    any_failure = True\n",
					"finally:\n",
					"    if any_failure:\n",
					"        print(\"Notebook finished with failures. Exiting with code 1.\")\n",
					"        mssparkutils.notebook.exit(\"1\")\n",
					"    else:\n",
					"        logInfo(\"Notebook finished successfully.\")\n",
					"        mssparkutils.notebook.exit(\"0\")\n",
					""
				],
				"execution_count": 191
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Ingest the data from the raw/source into the standardised table. \n",
					"If the table doesn't already exist, this will create the table first and ingest the data."
				]
			}
		]
	}
}