{
	"name": "py_raw_to_std",
	"properties": {
		"folder": {
			"name": "1-odw-raw-to-standardised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "9758082e-cb29-4622-a869-2c8bbf2ffee4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Prerequisites\n",
					"1. Make sure the new raw file's entry has been added to the Orchestration i.e `/infrastructure/configuration/data-lake/orchestration/orchestration.json`\n",
					"2. Make sure the standardised table's schema is present on the path specified in the entry added in step 1.\n",
					"3. Only if the raw file is huge (several GBs), the spark pool might need some upscaling. Hence the following cell\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Configure spark session"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"%%configure -f\n",
					"{\n",
					"    \"conf\": {\n",
					"        \"spark.kryoserializer.buffer.max\": \"2047m\",\n",
					"        \"spark.driver.maxResultSize\": \"10g\",\n",
					"        \"spark.rpc.message.maxSize\": \"1280\"\n",
					"    }\n",
					"}"
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import Required libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os"
				],
				"execution_count": 27
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise the parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"date_folder=''\n",
					"source_folder='AIEDocumentData'\n",
					"source_frequency_folder=''\n",
					"specific_file='' # if not provided, it will ingest all files in the date_folder\n",
					"isMultiLine = True\n",
					"delete_existing_table=False\n",
					"dataAttribute = \"\"\n",
					"start_exec_time = datetime.now() \n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": 28
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Logging decorator"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 29
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 30
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Get the Storage Account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"raw_container = \"abfss://odw-raw@\" + storage_account\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message =''"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run \"1-odw-raw-to-standardised/Fileshare/SAP_HR/py_1_raw_to_standardised_hr_functions\""
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Ingest the data from the raw/source into the standardised table. \n",
					"If the table doesn't already exist, this will create the table first and ingest the data."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"if date_folder == '':\n",
					"    date_folder = datetime.now().date()\n",
					"    print(date_folder)\n",
					"else:\n",
					"    date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\")\n",
					"\n",
					"date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"source_folder_path = source_folder if not source_frequency_folder else f\"{source_folder}/{source_frequency_folder}\"\n",
					"\n",
					"# READ ORCHESTRATION DATA\n",
					"path_to_orchestration_file = \"abfss://odw-config@\"+storage_account+\"orchestration/orchestration.json\"\n",
					"df = spark.read.option(\"multiline\",\"true\").json(path_to_orchestration_file)\n",
					"definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"process_name = \"py_raw_to_std\"\n",
					"\n",
					"source_path = f\"{raw_container}{source_folder_path}/{date_folder_str}\"\n",
					"\n",
					"try:\n",
					"    logInfo(f\"Reading from {source_path}\")\n",
					"    files = mssparkutils.fs.ls(source_path)\n",
					"\n",
					"except Exception as e:\n",
					"    logError(f\"Raw file not found at {source_path}\")\n",
					"    logException(e)\n",
					"    mssparkutils.notebook.exit(f\"Raw file not found at {source_path}\")\n",
					"    print(f\"Failed to process {file}: {e}\")\n",
					"    error_message = f\"Failed to process {file}: {str(e)}\"\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"for file in files:\n",
					"\n",
					"    # ignore json raw files if source is service bus\n",
					"    if source_folder == 'ServiceBus' and file.name.endswith('.json'):\n",
					"        continue\n",
					"\n",
					"    # ignore files other than specified file \n",
					"    if specific_file != '' and not file.name.startswith(specific_file + '.'):\n",
					"        continue\n",
					"        \n",
					"    definition = next((d for d in definitions if (specific_file == '' or d['Source_Filename_Start'] == specific_file) \n",
					"                        and (not source_frequency_folder or d['Source_Frequency_Folder'] == source_frequency_folder) \n",
					"                        and file.name.startswith(d['Source_Filename_Start'])), None)\n",
					"    \n",
					"    if definition:\n",
					"        expected_from = date_folder - timedelta(days=1)\n",
					"        expected_from = datetime.combine(expected_from, datetime.min.time())\n",
					"        expected_to = expected_from + timedelta(days=definition['Expected_Within_Weekdays']) \n",
					"\n",
					"        if delete_existing_table:\n",
					"            logInfo(f\"Deleting existing table if exists odw_standardised_db.{definition['Standardised_Table_Name']}\")\n",
					"            mssparkutils.notebook.run('/utils/py_delete_table', 300, arguments={'db_name': 'odw_standardised_db', 'table_name': definition['Standardised_Table_Name']})\n",
					"\n",
					"        logInfo(f\"Ingesting {file.name}\")\n",
					"        (ingestion_failure, row_count) = ingest_adhoc(storage_account, definition, source_path, file.name, expected_from, expected_to, process_name, isMultiLine, dataAttribute)\n",
					"\n",
					"        insert_count = row_count\n",
					"        end_exec_time = datetime.now()\n",
					"\n",
					"        \n",
					"import asyncio\n",
					"from datetime import datetime\n",
					"\n",
					"# After your ingestion logic determines these:\n",
					"# - ingestion_failure (bool)\n",
					"# - insert_count, update_count, delete_count, row_count\n",
					"# - definition['Standardised_Table_Name']\n",
					"# - PipelineName, PipelineRunID, PipelineTriggerID, PipelineTriggerName, PipelineTriggerType,\n",
					"#   PipelineTriggeredbyPipelineName, PipelineTriggeredbyPipelineRunID\n",
					"# - start_exec_time was set before, end_exec_time now:\n",
					"end_exec_time = datetime.now()\n",
					"\n",
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not ingestion_failure else \"Failed\"\n",
					"\n",
					"# Optional: if you have a `source_file` (like in the first notebook), plug it in here.\n",
					"source_file = locals().get(\"file\", None)\n",
					"\n",
					"status_message = (\n",
					"    (f\"Successfully loaded data from {source_file} into {definition['Standardised_Table_Name']} table\")\n",
					"    if (source_file and not ingestion_failure) else\n",
					"    (f\"Failed to load data from {source_file} into {definition['Standardised_Table_Name']} table\")\n",
					"    if (source_file and ingestion_failure) else\n",
					"    (f\"Successfully loaded data into {definition['Standardised_Table_Name']} table\")\n",
					"    if not ingestion_failure else\n",
					"    (f\"Failed to load data for {definition['Standardised_Table_Name']} table\")\n",
					")\n",
					"\n",
					"error_message = \"\" if not ingestion_failure else \"Errors reported during Ingestion!!\"\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					"\n",
					"# Build params to match the first notebook (field names kept identical):\n",
					"params = {\n",
					"    \"Stage\": stage,\n",
					"    \"PipelineName\": PipelineName,\n",
					"    \"PipelineRunID\": PipelineRunID,\n",
					"    \"StartTime\": start_exec_time.isoformat(),\n",
					"    \"EndTime\": end_exec_time.isoformat(),\n",
					"    \"Inserts\": insert_count,\n",
					"    \"Updates\": update_count,\n",
					"    \"Deletes\": delete_count,\n",
					"    \"ErrorMessage\": error_message,\n",
					"    \"StatusMessage\": status_message,\n",
					"    \"PipelineTriggerID\": PipelineTriggerID,\n",
					"    \"PipelineTriggerName\": PipelineTriggerName,\n",
					"    \"PipelineTriggerType\": PipelineTriggerType,\n",
					"    \"PipelineTriggeredbyPipelineName\": PipelineTriggeredbyPipelineName,\n",
					"    \"PipelineTriggeredbyPipelineRunID\": PipelineTriggeredbyPipelineRunID,\n",
					"    \"PipelineExecutionTimeInSec\": duration_seconds,\n",
					"    \"ActivityType\": activity_type,\n",
					"    \"DurationSeconds\": duration_seconds,\n",
					"    \"StatusCode\": status_code,\n",
					"    \"AppInsCustomEventName\": \"ODW_Master_Pipeline_Logs\",\n",
					"    # Optional extras: add table fully qualified name for traceability\n",
					"    \"TargetTable\": f\"odw_standardised_db.{definition['Standardised_Table_Name']}\",\n",
					"    \"SourceFile\": source_file if source_file else \"\"\n",
					"}\n",
					"\n",
					"async def send_and_maybe_exit():\n",
					"    # Send telemetry in a separate thread, like the first notebook\n",
					"    await asyncio.to_thread(send_telemetry_to_app_insights, params)\n",
					"\n",
					"    # Mirror \"stop on first error\" behavior\n",
					"    if ingestion_failure:\n",
					"        raise RuntimeError(f\"Notebook failed: {error_message}\")\n",
					"\n",
					"# Run and handle exception to quit notebook cleanly (exit code 1)\n",
					"try:\n",
					"    loop = asyncio.get_event_loop()\n",
					"    loop.run_until_complete(send_and_maybe_exit())\n",
					"    # Optional logging\n",
					"    logInfo(f\"Ingested {row_count} rows\")\n",
					"except RuntimeError as e:\n",
					"    print(str(e))\n",
					"    import sys\n",
					"    sys.exit(1)"
				],
				"execution_count": 33
			}
		]
	}
}