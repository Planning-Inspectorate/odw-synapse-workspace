{
	"name": "rel_28_0_0_fix_pins_inspector_columns",
	"properties": {
		"folder": {
			"name": "Releases"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "98bae789-0b8c-4008-84c2-96cc37e59b31"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"## Release 28.0.0 - Fix pins_inspector Column Names\n",
					"\n",
					"### Purpose\n",
					"Fix incorrect column names in pins_inspector harmonised and curated tables:\n",
					"- Rename `emailAddress` to `email`\n",
					"- Rename `specialism` to `specialisms`\n",
					"- Fix nested field `specialismValidFrom` to `validFrom` in `specialisms` array struct\n",
					"- Convert `validFrom` from date format to datetime format (add T00:00:00.000Z)\n",
					"\n",
					"### Scope\n",
					"- `odw_harmonised_db.pins_inspector`\n",
					"- `odw_curated_db.pins_inspector`\n",
					"\n",
					"### Approach\n",
					"Read tables, rename columns using withColumnRenamed(), fix nested struct fields using transform(), convert date to datetime format, and overwrite with updated schema."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import *\n",
					"from datetime import datetime\n",
					"\n",
					"start_time = datetime.now()\n",
					"print(f\"Starting column rename at {start_time}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Step 1: Rename columns in harmonised table"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    print(\"Renaming columns in odw_harmonised_db.pins_inspector...\")\n",
					"    \n",
					"    # Read existing table and cache it in memory\n",
					"    harmonised_df = spark.table(\"odw_harmonised_db.pins_inspector\")\n",
					"    harmonised_df.cache()  # Cache in memory to break the read dependency\n",
					"    \n",
					"    columns = harmonised_df.columns\n",
					"    print(f\"Current columns: {columns}\")\n",
					"    \n",
					"    needs_rename = \"emailAddress\" in columns or \"specialism\" in columns\n",
					"    renamed_df = harmonised_df\n",
					"\n",
					"    \n",
					"    # Check for nested field specialismValidFrom in specialisms array\n",
					"    needs_nested_fix = False\n",
					"    if \"specialisms\" in columns:\n",
					"        schema_json = harmonised_df.schema.json()\n",
					"        if \"specialismValidFrom\" in schema_json:\n",
					"            needs_nested_fix = True\n",
					"            print(\"Found specialismValidFrom in specialisms struct - will fix\")\n",
					"    \n",
					"    if needs_rename or needs_nested_fix:\n",
					"        renamed_df = harmonised_df\n",
					"        \n",
					"        if \"emailAddress\" in columns:\n",
					"            renamed_df = renamed_df.withColumnRenamed(\"emailAddress\", \"email\")\n",
					"            print(\"✓ Renamed emailAddress to email\")\n",
					"        \n",
					"        if \"specialism\" in columns and \"specialisms\" not in columns:\n",
					"            renamed_df = renamed_df.withColumnRenamed(\"specialism\", \"specialisms\")\n",
					"            print(\"✓ Renamed specialism to specialisms\")\n",
					"        \n",
					"        # Fix nested specialismValidFrom field\n",
					"        if needs_nested_fix:\n",
					"            renamed_df = renamed_df.withColumn(\n",
					"                \"specialisms\",\n",
					"                when(\n",
					"                    (col(\"specialisms\").isNotNull()) & (size(col(\"specialisms\")) > 0),\n",
					"                    transform(\n",
					"                        col(\"specialisms\"),\n",
					"                        lambda x: struct(\n",
					"                            x.getField(\"name\").alias(\"name\"),\n",
					"                            x.getField(\"proficiency\").alias(\"proficiency\"),\n",
					"                            x.getField(\"specialismValidFrom\").alias(\"validFrom\")\n",
					"                        )\n",
					"                    )\n",
					"                ).otherwise(col(\"specialisms\"))\n",
					"            )\n",
					"            print(\"✓ Fixed specialismValidFrom to validFrom in specialisms struct\")\n",
					"        \n",
					"    # Convert validFrom to datetime format\n",
					"    if \"validFrom\" in columns:\n",
					"        from pyspark.sql.functions import concat, lit\n",
					"        # Check if validFrom needs conversion (is date-only format)\n",
					"        sample_val = renamed_df.select(\"validFrom\").first()\n",
					"        if sample_val and sample_val[0] and \"T\" not in str(sample_val[0]):\n",
					"            renamed_df = renamed_df.withColumn(\n",
					"                \"validFrom\",\n",
					"                when(\n",
					"                    col(\"validFrom\").isNotNull(),\n",
					"                    concat(col(\"validFrom\"), lit(\"T00:00:00.000Z\"))\n",
					"                ).otherwise(col(\"validFrom\"))\n",
					"            )\n",
					"            print(\"✓ Converted validFrom to datetime format\")\n",
					"        \n",
					"        print(\"Writing updated table...\")\n",
					"        # Write to a temp location first\n",
					"        renamed_df.write.mode(\"overwrite\").saveAsTable(\"odw_harmonised_db.pins_inspector_temp\")\n",
					"        \n",
					"        # Drop the original table\n",
					"        spark.sql(\"DROP TABLE IF EXISTS odw_harmonised_db.pins_inspector\")\n",
					"        \n",
					"        # Rename temp table to original name\n",
					"        spark.sql(\"ALTER TABLE odw_harmonised_db.pins_inspector_temp RENAME TO odw_harmonised_db.pins_inspector\")\n",
					"        \n",
					"        print(\"✓ Harmonised table updated successfully\")\n",
					"        \n",
					"        # Unpersist the cached dataframe\n",
					"        harmonised_df.unpersist()\n",
					"    else:\n",
					"        print(\"  Columns already have correct names\")\n",
					"        harmonised_df.unpersist()\n",
					"    \n",
					"except Exception as e:\n",
					"    print(f\"Error renaming harmonised table columns: {str(e)}\")\n",
					"    raise"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Step 2: Rename columns in curated table"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    print(\"Renaming columns in odw_curated_db.pins_inspector...\")\n",
					"    \n",
					"    # Read existing table and cache it in memory\n",
					"    curated_df = spark.table(\"odw_curated_db.pins_inspector\")\n",
					"    curated_df.cache()  # Cache in memory to break the read dependency\n",
					"    \n",
					"    columns = curated_df.columns\n",
					"    print(f\"Current columns: {columns}\")\n",
					"    \n",
					"    needs_rename = \"emailAddress\" in columns or \"specialism\" in columns\n",
					"    renamed_df = curated_df\n",
					"    \n",
					"    # Check for nested field specialismValidFrom in specialisms array\n",
					"    needs_nested_fix = False\n",
					"    if \"specialisms\" in columns:\n",
					"        schema_json = curated_df.schema.json()\n",
					"        if \"specialismValidFrom\" in schema_json:\n",
					"            needs_nested_fix = True\n",
					"            print(\"Found specialismValidFrom in specialisms struct - will fix\")\n",
					"    \n",
					"    if needs_rename or needs_nested_fix:\n",
					"        renamed_df = curated_df\n",
					"        \n",
					"        if \"emailAddress\" in columns:\n",
					"            renamed_df = renamed_df.withColumnRenamed(\"emailAddress\", \"email\")\n",
					"            print(\"✓ Renamed emailAddress to email\")\n",
					"        \n",
					"        if \"specialism\" in columns and \"specialisms\" not in columns:\n",
					"            renamed_df = renamed_df.withColumnRenamed(\"specialism\", \"specialisms\")\n",
					"            print(\"✓ Renamed specialism to specialisms\")\n",
					"        \n",
					"        # Fix nested specialismValidFrom field\n",
					"        if needs_nested_fix:\n",
					"            renamed_df = renamed_df.withColumn(\n",
					"                \"specialisms\",\n",
					"                when(\n",
					"                    (col(\"specialisms\").isNotNull()) & (size(col(\"specialisms\")) > 0),\n",
					"                    transform(\n",
					"                        col(\"specialisms\"),\n",
					"                        lambda x: struct(\n",
					"                            x.getField(\"name\").alias(\"name\"),\n",
					"                            x.getField(\"proficiency\").alias(\"proficiency\"),\n",
					"                            x.getField(\"specialismValidFrom\").alias(\"validFrom\")\n",
					"                        )\n",
					"                    )\n",
					"                ).otherwise(col(\"specialisms\"))\n",
					"            )\n",
					"            print(\"✓ Fixed specialismValidFrom to validFrom in specialisms struct\")\n",
					"        \n",
					"    # Convert validFrom to datetime format\n",
					"    if \"validFrom\" in columns:\n",
					"        from pyspark.sql.functions import concat, lit\n",
					"        # Check if validFrom needs conversion (is date-only format)\n",
					"        sample_val = renamed_df.select(\"validFrom\").first()\n",
					"        if sample_val and sample_val[0] and \"T\" not in str(sample_val[0]):\n",
					"            renamed_df = renamed_df.withColumn(\n",
					"                \"validFrom\",\n",
					"                when(\n",
					"                    col(\"validFrom\").isNotNull(),\n",
					"                    concat(col(\"validFrom\"), lit(\"T00:00:00.000Z\"))\n",
					"                ).otherwise(col(\"validFrom\"))\n",
					"            )\n",
					"            print(\"✓ Converted validFrom to datetime format\")\n",
					"        \n",
					"        print(\"Writing updated table...\")\n",
					"        # Write to a temp location first\n",
					"        renamed_df.write.mode(\"overwrite\").saveAsTable(\"odw_curated_db.pins_inspector_temp\")\n",
					"        \n",
					"        # Drop the original table\n",
					"        spark.sql(\"DROP TABLE IF EXISTS odw_curated_db.pins_inspector\")\n",
					"        \n",
					"        # Rename temp table to original name\n",
					"        spark.sql(\"ALTER TABLE odw_curated_db.pins_inspector_temp RENAME TO odw_curated_db.pins_inspector\")\n",
					"        \n",
					"        print(\"✓ curated table updated successfully\")\n",
					"        \n",
					"        # Unpersist the cached dataframe\n",
					"        curated_df.unpersist()\n",
					"    else:\n",
					"        print(\"  Columns already have correct names\")\n",
					"        curated_df.unpersist()\n",
					"    \n",
					"except Exception as e:\n",
					"    print(f\"Error renaming curated table columns: {str(e)}\")\n",
					"    raise"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Step 3: Verify changes"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    print(\"\\n=== Verification ===\")\n",
					"    print(\"\\nHarmonised table columns:\")\n",
					"    harmonised_df = spark.table(\"odw_harmonised_db.pins_inspector\")\n",
					"    print(harmonised_df.columns)\n",
					"    \n",
					"    print(\"\\nCurated table columns:\")\n",
					"    curated_df = spark.table(\"odw_curated_db.pins_inspector\")\n",
					"    print(curated_df.columns)\n",
					"    \n",
					"    # Verify expected columns exist\n",
					"    required_columns = ['email', 'specialisms']\n",
					"    \n",
					"    harmonised_has_all = all(col in harmonised_df.columns for col in required_columns)\n",
					"    curated_has_all = all(col in curated_df.columns for col in required_columns)\n",
					"    \n",
					"    if harmonised_has_all and curated_has_all:\n",
					"        print(\"\\n✓ All required columns present in both tables\")\n",
					"    else:\n",
					"        print(\"\\n✗ Missing required columns:\")\n",
					"        if not harmonised_has_all:\n",
					"            print(f\"  Harmonised table missing: {[c for c in required_columns if c not in harmonised_df.columns]}\")\n",
					"        if not curated_has_all:\n",
					"            print(f\"  Curated table missing: {[c for c in required_columns if c not in curated_df.columns]}\")\n",
					"    \n",
					"    end_time = datetime.now()\n",
					"    duration = (end_time - start_time).total_seconds()\n",
					"    print(f\"\\nCompleted in {duration:.2f} seconds\")\n",
					"    \n",
					"except Exception as e:\n",
					"    print(f\"Error during verification: {str(e)}\")\n",
					"    raise"
				],
				"execution_count": null
			}
		]
	}
}