{
	"name": "nb_write_orchestrator_manifest",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "1c8684b4-c20e-4b76-8108-6257c6f3b4cf"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Parameters injected by pipeline - defaults for adhoc runs\n",
					"\n",
					"try:\n",
					"    runSummaryJson\n",
					"except NameError:\n",
					"    runSummaryJson = \"{}\"\n",
					"\n",
					"try:\n",
					"    runContextJson\n",
					"except NameError:\n",
					"    runContextJson = \"{}\"\n",
					"\n",
					"try:\n",
					"    manifestPath\n",
					"except NameError:\n",
					"    # relative path - pipeline will override\n",
					"    manifestPath = \"Orchestrator_Manifests/pln_orchestrator/runs/ingest_date=1970-01-01/run_id=local/manifest.json\"\n",
					"\n",
					"try:\n",
					"    status\n",
					"except NameError:\n",
					"    status = \"Succeeded\"\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import Row\n",
					"import json, datetime, uuid\n",
					"\n",
					"# Helpers\n",
					"def _to_dict(x):\n",
					"    if isinstance(x, dict):\n",
					"        return x\n",
					"    if not x:\n",
					"        return {}\n",
					"    if isinstance(x, str):\n",
					"        x = x.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
					"    try:\n",
					"        return json.loads(x)\n",
					"    except Exception:\n",
					"        return json.loads(bytes(x, \"utf-8\").decode(\"unicode_escape\").replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \"))\n",
					"\n",
					"def _clean_strings(obj):\n",
					"    if isinstance(obj, dict):\n",
					"        return {k: _clean_strings(v) for k, v in obj.items()}\n",
					"    if isinstance(obj, list):\n",
					"        return [_clean_strings(v) for v in obj]\n",
					"    if isinstance(obj, str):\n",
					"        return obj.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
					"    return obj\n",
					"\n",
					"ctx = _clean_strings(_to_dict(runContextJson))\n",
					"\n",
					"run_id   = ctx.get(\"runId\", str(uuid.uuid4()))\n",
					"env_name = ctx.get(\"environment\", \"\")\n",
					"ing_date = ctx.get(\"ingestDate\") or datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
					"\n",
					"# Storage account\n",
					"storage_account = (mssparkutils.notebook.run('/utils/py_utils_get_storage_account') or \"\").strip()\n",
					"storage_account = storage_account.replace(\"https://\", \"\").replace(\"http://\", \"\").strip(\"/\")\n",
					"\n",
					"# We store orchestrator manifests in odw-raw, same pattern as Horizon/ServiceBus\n",
					"FILE_SYSTEM = \"odw-raw\"\n",
					"\n",
					"# manifestPath is a relative path built in the pipeline, e.g.:\n",
					"# Orchestrator_Manifests/pln_orchestrator/runs/ingest_date=2025-11-26/run_id=<runId>/manifest.json\n",
					"manifest_rel = (manifestPath or \"\").lstrip(\"/\")\n",
					"assert manifest_rel, \"Missing required param: manifestPath\"\n",
					"\n",
					"manifest_full = f\"abfss://{FILE_SYSTEM}@{storage_account}/{manifest_rel}\"\n",
					"manifest_dir  = manifest_full.rsplit(\"/\", 1)[0]\n",
					"\n",
					"\n",
					"# Ensure directory exists\n",
					"mssparkutils.fs.mkdirs(manifest_dir)\n",
					"\n",
					"# Run summary JSON comes straight from pln_orchestrator\n",
					"summary_obj = _clean_strings(_to_dict(runSummaryJson))\n",
					"\n",
					"# Enrich slightly with write metadata (doesn't change rerun logic, just nicer manifest)\n",
					"summary_obj.setdefault(\"orchestrator\", \"pln_orchestrator\")\n",
					"summary_obj.setdefault(\"environment\", env_name)\n",
					"summary_obj.setdefault(\"ingestDate\", ing_date)\n",
					"summary_obj.setdefault(\"writtenAt\", datetime.datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n",
					"summary_obj.setdefault(\"status\", str(status).strip(\"'\").strip('\"'))\n",
					"\n",
					"# Write manifest\n",
					"mssparkutils.fs.put(\n",
					"    manifest_full,\n",
					"    json.dumps(summary_obj, indent=2, ensure_ascii=False),\n",
					"    True\n",
					")\n",
					"print(f\"orchestrator manifest written: {manifest_full}\")\n",
					"\n",
					"# Watermark upsert\n",
					"\n",
					"spark.sql(\"CREATE DATABASE IF NOT EXISTS odw_meta_db\")\n",
					"spark.sql(\"USE odw_meta_db\")\n",
					"\n",
					"spark.sql(\"\"\"\n",
					"CREATE TABLE IF NOT EXISTS watermarks (\n",
					"  entity            STRING,\n",
					"  source            STRING,\n",
					"  last_ingest_time  TIMESTAMP,\n",
					"  ingest_date       STRING,\n",
					"  run_id            STRING,\n",
					"  manifest_path     STRING,\n",
					"  records_ingested  BIGINT,\n",
					"  status            STRING,\n",
					"  updated_at        TIMESTAMP\n",
					")\n",
					"USING DELTA\n",
					"PARTITIONED BY (entity, ingest_date)\n",
					"\"\"\")\n",
					"\n",
					"now_ts = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
					"\n",
					"# For orchestrator we often don't have a unified \"records\" metric yet\n",
					"wm_df = spark.createDataFrame([\n",
					"    Row(\n",
					"        entity=\"pln_orchestrator\",\n",
					"        source=\"orchestrator\",\n",
					"        last_ingest_time=now_ts,\n",
					"        ingest_date=ing_date,\n",
					"        run_id=run_id,\n",
					"        manifest_path=manifest_full,\n",
					"        records_ingested=0,\n",
					"        status=str(status),\n",
					"        updated_at=now_ts\n",
					"    )\n",
					"]).selectExpr(\n",
					"    \"entity\",\n",
					"    \"source\",\n",
					"    \"to_timestamp(last_ingest_time) as last_ingest_time\",\n",
					"    \"ingest_date\",\n",
					"    \"run_id\",\n",
					"    \"manifest_path\",\n",
					"    \"cast(records_ingested as bigint) as records_ingested\",\n",
					"    \"status\",\n",
					"    \"to_timestamp(updated_at) as updated_at\"\n",
					")\n",
					"\n",
					"wm_df.createOrReplaceTempView(\"wm_upd\")\n",
					"\n",
					"spark.sql(\"\"\"\n",
					"MERGE INTO odw_meta_db.watermarks AS tgt\n",
					"USING wm_upd AS src\n",
					"ON  tgt.entity = src.entity\n",
					"AND tgt.source = src.source\n",
					"AND tgt.ingest_date = src.ingest_date\n",
					"WHEN MATCHED THEN UPDATE SET\n",
					"  tgt.last_ingest_time = src.last_ingest_time,\n",
					"  tgt.run_id           = src.run_id,\n",
					"  tgt.manifest_path    = src.manifest_path,\n",
					"  tgt.records_ingested = src.records_ingested,\n",
					"  tgt.status           = src.status,\n",
					"  tgt.updated_at       = src.updated_at\n",
					"WHEN NOT MATCHED THEN INSERT *\n",
					"\"\"\")\n",
					"\n",
					"print(f\"orchestrator watermark upserted for {ing_date} / {run_id}\")\n",
					"\n",
					"# Return values to pipeline\n",
					"mssparkutils.notebook.exit(json.dumps({\n",
					"    \"manifestPath\": manifest_rel,\n",
					"    \"fullManifestPath\": manifest_full,\n",
					"    \"status\": str(status)\n",
					"}))\n",
					""
				],
				"execution_count": null
			}
		]
	}
}