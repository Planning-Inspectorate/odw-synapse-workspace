{
	"name": "py_load_green_specialist_case",
	"properties": {
		"folder": {
			"name": "odw-harmonised/green_specialist_case"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b6f674ce-4925-4a7f-ae9d-70f2177e6eda"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Harmonized Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 02-Dec-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate green specialist cases in harmonized layer. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that green specialist case data is accurately transformed, stored, and made available for reporting and analysis.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from datetime import datetime, timedelta\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_harmonised_db.load_green_specialist_case\"\n",
					"source_table = \"odw_standardised_db.green_specialist_case\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Set time parser policy\n",
					"        logInfo(\"Setting legacy time parser policy\")\n",
					"        spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"        logInfo(\"Legacy time parser policy set successfully\")\n",
					"        \n",
					"        # Delete existing data\n",
					"        logInfo(f\"Starting deletion of all rows from {spark_table_final}\")\n",
					"        spark.sql(f\"DELETE FROM {spark_table_final}\")\n",
					"        logInfo(f\"Successfully deleted all rows from {spark_table_final}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in setting policy or deleting data from {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Insert data from source table\n",
					"        logInfo(f\"Starting data insertion into {spark_table_final} from {source_table}\")\n",
					"        spark.sql(f\"\"\"\n",
					"            INSERT INTO {spark_table_final} (\n",
					"                casework_specialist_id,\n",
					"                greenCaseType,\n",
					"                greenCaseId,\n",
					"                caseReference,\n",
					"                horizonId,\n",
					"                linkedGreenCaseId,\n",
					"                caseOfficerName,\n",
					"                caseOfficerEmail,\n",
					"                appealType,\n",
					"                procedure,\n",
					"                processingState,\n",
					"                pinsLpaCode,\n",
					"                pinsLpaName,\n",
					"                appellantName,\n",
					"                agentName,\n",
					"                SiteAddressDescription,\n",
					"                sitePostcode,\n",
					"                otherPartyName,\n",
					"                receiptDate,\n",
					"                validDate,\n",
					"                startDate,\n",
					"                lpaQuestionnaireDue,\n",
					"                lpaQuestionnaireReceived,\n",
					"                week6Date,\n",
					"                week8Date,\n",
					"                week9Date,\n",
					"                eventType,\n",
					"                eventDate,\n",
					"                eventTime,\n",
					"                inspectorName,\n",
					"                inspectorStaffNumber,\n",
					"                decision,\n",
					"                decisionDate,\n",
					"                withdrawnOrTurnedAwayDate,\n",
					"                comments,\n",
					"                active,\n",
					"                Migrated,\n",
					"                IngestionDate,\n",
					"                ValidFrom,\n",
					"                ValidTo,\n",
					"                RowID,\n",
					"                IsActive\n",
					"            )\n",
					"            SELECT\n",
					"                ROW_NUMBER() OVER (ORDER BY greenCaseId, receiptDate) AS casework_specialist_id,\n",
					"                greenCaseType,\n",
					"                TRIM(greenCaseId)                                   AS greenCaseId,\n",
					"                TRIM(FullReference)                                 AS caseReference,\n",
					"                TRIM(horizonId)                                     AS horizonId,\n",
					"                TRIM(linkedGreenCaseId)                             AS linkedGreenCaseId,\n",
					"                TRIM(caseOfficerName)                               AS caseOfficerName,\n",
					"                TRIM(caseOfficerEmail)                              AS caseOfficerEmail,\n",
					"                TRIM(appealType)                                    AS appealType,\n",
					"                TRIM(procedure)                                     AS procedure,\n",
					"                TRIM(processingState)                               AS processingState,\n",
					"                TRIM(LPACode)                                       AS pinsLpaCode,\n",
					"                TRIM(LPAName)                                       AS pinsLpaName,\n",
					"                TRIM(appellantName)                                 AS appellantName,\n",
					"                TRIM(agentName)                                     AS agentName,\n",
					"                TRIM(SiteAddressDescription)                        AS SiteAddressDescription,\n",
					"                TRIM(sitePostcode)                                  AS sitePostcode,\n",
					"                TRIM(otherPartyName)                                AS otherPartyName,\n",
					"                TO_DATE(TRIM(receiptDate), 'yyyy-MM-dd')            AS receiptDate,\n",
					"                TO_DATE(TRIM(validDate),   'yyyy-MM-dd')            AS validDate,\n",
					"                TO_DATE(TRIM(startDate),   'yyyy-MM-dd')            AS startDate,\n",
					"                TO_DATE(TRIM(QuDate),      'yyyy-MM-dd')            AS lpaQuestionnaireDue,\n",
					"                TO_DATE(TRIM(QuRecDate),   'yyyy-MM-dd')            AS lpaQuestionnaireReceived,\n",
					"                TO_DATE(TRIM(`6Weeks`),    'yyyy-MM-dd')            AS week6Date,\n",
					"                TO_DATE(TRIM(`8Weeks`),    'yyyy-MM-dd')            AS week8Date,\n",
					"                TO_DATE(TRIM(`9Weeks`),    'yyyy-MM-dd')            AS week9Date,\n",
					"                TRIM(eventType)                                     AS eventType,\n",
					"                TO_DATE(TRIM(eventDate),  'yyyy-MM-dd')             AS eventDate,\n",
					"                TRIM(eventTime)                                     AS eventTime,\n",
					"                TRIM(inspectorName)                                 AS inspectorName,\n",
					"                TRIM(inspectorStaffNumber)                          AS inspectorStaffNumber,\n",
					"                TRIM(decision)                                      AS decision,\n",
					"                TO_DATE(TRIM(decisionDate), 'yyyy-MM-dd')           AS decisionDate,\n",
					"                TO_DATE(TRIM(DateWithdrawnorTurnedAway), 'yyyy-MM-dd') AS withdrawnOrTurnedAwayDate,\n",
					"                comments,\n",
					"                active,\n",
					"                'N' AS Migrated,\n",
					"                CURRENT_DATE()      AS IngestionDate,\n",
					"                CURRENT_TIMESTAMP() AS ValidFrom,\n",
					"                CURRENT_TIMESTAMP() AS ValidTo,\n",
					"                NULL                AS RowID,\n",
					"                'Y'                 AS IsActive\n",
					"            FROM {source_table}\n",
					"            WHERE active = 'Y'\n",
					"        \"\"\")\n",
					"        \n",
					"        # Get count of inserted rows\n",
					"        insert_count = spark.sql(f\"SELECT COUNT(*) as count FROM {spark_table_final}\") \\\n",
					"                            .collect()[0]['count']\n",
					"        logInfo(f\"Successfully inserted {insert_count} rows into {spark_table_final}\")\n",
					"    \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in inserting data into {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Update RowID with MD5 hash\n",
					"        logInfo(\"Starting RowID update with MD5 hash values\")\n",
					"        spark.sql(f\"\"\"\n",
					"            UPDATE {spark_table_final}\n",
					"            SET RowID = md5(concat_ws('|', \n",
					"                IFNULL(TRIM(CAST(casework_specialist_id      AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(greenCaseType              AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(greenCaseId                AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(caseReference              AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(horizonId                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(linkedGreenCaseId          AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(caseOfficerName            AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(caseOfficerEmail           AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(appealType                 AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(procedure                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(processingState            AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(pinsLpaCode                AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(pinsLpaName                AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(appellantName              AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(agentName                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(SiteAddressDescription     AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(sitePostcode               AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(otherPartyName             AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(receiptDate                AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(validDate                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(startDate                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(lpaQuestionnaireDue        AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(lpaQuestionnaireReceived   AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(week6Date                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(week8Date                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(week9Date                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(eventType                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(eventDate                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(eventTime                  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(inspectorName              AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(inspectorStaffNumber       AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(decision                   AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(decisionDate               AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(withdrawnOrTurnedAwayDate  AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(comments                   AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(active                     AS STRING)), '.'),\n",
					"                IFNULL(TRIM(CAST(Migrated                   AS STRING)), '.')\n",
					"            ))\n",
					"        \"\"\")\n",
					"        logInfo(\"Successfully updated all RowID values with MD5 hashes\")\n",
					"\n",
					"        null_rowid_count = spark.sql(\n",
					"            f\"SELECT COUNT(*) as count FROM {spark_table_final} WHERE RowID IS NULL\"\n",
					"        ).collect()[0]['count']\n",
					"        if null_rowid_count > 0:\n",
					"            logError(f\"Data quality issue: {null_rowid_count} rows have NULL RowID values\")\n",
					"        else:\n",
					"            logInfo(\"Data quality check passed: No NULL RowID values found\")\n",
					"\n",
					"        logInfo(\"Casework specialist data processing completed successfully\")\n",
					"        end_exec_time = datetime.now()\n",
					"\n",
					"    except Exception as e:\n",
					"        error_message = (\n",
					"            f\"Error in updating RowID for {spark_table_final}: {str(e)[:800]}\"\n",
					"        )\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"Successfully loaded data into {spark_table_final} table\"\n",
					"    if not error_message\n",
					"    else f\"Failed to load data into {spark_table_final} table\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    spark_table_final,\n",
					"    insert_count,\n",
					"    update_count,\n",
					"    delete_count,\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")"
				],
				"execution_count": null
			}
		]
	}
}