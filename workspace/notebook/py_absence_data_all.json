{
	"name": "py_absence_data_all",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodwpr",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "97dd6ed5-1c5b-4543-b819-cf752b0cb8ac"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodwpr",
				"name": "pinssynspodwpr",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodwpr",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of absence data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that absence data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intializations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from datetime import datetime, timedelta\n",
					"import json\n",
					"import re\n",
					"from pyspark.sql.functions import col\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Absence Incremental load"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"inserted_count\": 0,\n",
					"    \"updated_count\": 0,\n",
					"    \"deleted_count\": 0,\n",
					"    \"cancelled_deleted_count\": 0,\n",
					"    \"skipped_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"# Enhanced UUID extraction function (handles NULL AbsType)\n",
					"def extract_uuid_from_abstype(abs_type):\n",
					"    \"\"\"\n",
					"    Extract UUID from AbsType field using simplified logic\n",
					"    Enhanced to handle NULL AbsType\n",
					"    \"\"\"\n",
					"    if abs_type is None or (isinstance(abs_type, str) and abs_type.strip() == ''):\n",
					"        # Generate synthetic UUID for NULL AbsType\n",
					"        import hashlib\n",
					"        return f\"NULL_ABSTYPE_{hashlib.md5('NULL_RECORD'.encode()).hexdigest()}\"\n",
					"    \n",
					"    # Original logic unchanged\n",
					"    # Pattern 1: Complex timestamp pattern\n",
					"    pattern1 = r'^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}[.][0-9]+-(.+)$'\n",
					"    match1 = re.match(pattern1, abs_type)\n",
					"    if match1:\n",
					"        return match1.group(1)\n",
					"    \n",
					"    # Pattern 2: 32-character hex string\n",
					"    pattern2 = r'[a-f0-9]{32}'\n",
					"    match2 = re.search(pattern2, abs_type)\n",
					"    if match2:\n",
					"        return match2.group(0)\n",
					"    \n",
					"    # Pattern 3: Suffix after last dash (4+ characters)\n",
					"    pattern3 = r'.*-([A-Za-z0-9_]{4,})$'\n",
					"    match3 = re.match(pattern3, abs_type)\n",
					"    if match3:\n",
					"        return match3.group(1)\n",
					"    \n",
					"    # Default: return MD5 hash\n",
					"    import hashlib\n",
					"    return hashlib.md5(abs_type.encode()).hexdigest()\n",
					"\n",
					"# Register UDF for use in SQL\n",
					"from pyspark.sql.types import StringType\n",
					"spark.udf.register(\"extract_uuid\", extract_uuid_from_abstype, StringType())\n",
					"\n",
					"# Set legacy time parser for compatibility\n",
					"logInfo(\"Setting legacy time parser policy\")\n",
					"spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"logInfo(\"Legacy time parser policy set successfully\")\n",
					"\n",
					"# INCREMENTAL APPROACH: Get last processing watermark\n",
					"logInfo(\"Determining incremental processing watermark\")\n",
					"\n",
					"# Get the last ingestion date from target table\n",
					"try:\n",
					"    last_processed_result = spark.sql(\"\"\"\n",
					"        SELECT COALESCE(MAX(CAST(IngestionDate AS DATE)), DATE('1900-01-01')) as last_processed_date\n",
					"        FROM odw_harmonised_db.sap_hr_absence_all\n",
					"        WHERE IsActive = 'Y'\n",
					"    \"\"\").collect()\n",
					"    \n",
					"    last_processed_date = last_processed_result[0]['last_processed_date']\n",
					"    logInfo(f\"Last processed date: {last_processed_date}\")\n",
					"    \n",
					"     \n",
					"    from datetime import datetime, timedelta\n",
					"    if last_processed_date.year > 1900:\n",
					"        safe_start_date = last_processed_date - timedelta(days=7)\n",
					"    else:\n",
					"        safe_start_date = datetime(1900, 1, 1).date()\n",
					"    \n",
					"    logInfo(f\"Processing records from: {safe_start_date} (safe start date)\")\n",
					"    \n",
					"except Exception as e:\n",
					"    # If target table is empty or doesn't exist, process all data\n",
					"    logInfo(f\"Target table empty or error getting watermark: {e}\")\n",
					"    safe_start_date = datetime(1900, 1, 1).date()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Process Absence data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    # Step 1: Create incremental source view - only process new/changed records\n",
					"    logInfo(\"Creating incremental source view with new/changed records only\")\n",
					"    \n",
					"    source_total_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_standardised_db.hr_absence_monthly\").collect()[0]['count']\n",
					"    logInfo(f\"Total source records available: {source_total_count}\")\n",
					"    \n",
					"    spark.sql(f\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW incremental_source AS\n",
					"    SELECT * FROM odw_standardised_db.hr_absence_monthly\n",
					"    WHERE \n",
					"        StaffNumber IS NOT NULL \n",
					"        AND StartDate IS NOT NULL \n",
					"        AND EndDate IS NOT NULL\n",
					"        AND (\n",
					"            -- Include records with recent start dates (new absences)\n",
					"            TO_DATE(StartDate,'dd/MM/yyyy') >= DATE('{safe_start_date}')\n",
					"            OR \n",
					"            -- Include records with recent end dates (recently ended absences)\n",
					"            TO_DATE(EndDate,'dd/MM/yyyy') >= DATE('{safe_start_date}')\n",
					"            OR\n",
					"            -- Include records with NULL AbsType (always process these as they may be corrected data)\n",
					"            AbsType IS NULL\n",
					"        )\n",
					"    \"\"\")\n",
					"    \n",
					"    incremental_source_count = spark.sql(\"SELECT COUNT(*) as count FROM incremental_source\").collect()[0]['count']\n",
					"    logInfo(f\"Incremental source records to process: {incremental_source_count}\")\n",
					"    \n",
					"    if incremental_source_count == 0:\n",
					"        logInfo(\"No new records to process. Exiting early.\")\n",
					"        result[\"skipped_count\"] = source_total_count\n",
					"        logInfo(\"No incremental processing needed\")\n",
					"    else:\n",
					"        # Step 2: Create enhanced staging view for incremental records\n",
					"        logInfo(\"Creating staging view for incremental records\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW staging_absence AS\n",
					"        SELECT  \n",
					"            StaffNumber,\n",
					"            COALESCE(AbsType, '') AS AbsType,\n",
					"            COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"            CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"            CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"            AttendanceorAbsenceType,\n",
					"            ROUND(CAST(REPLACE(Days, ',', '') AS DOUBLE), 2) AS Days,\n",
					"            ROUND(CAST(REPLACE(Hrs, ',', '') AS DOUBLE), 2) AS Hrs,\n",
					"            TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"            TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"            Caldays,\n",
					"            WorkScheduleRule,\n",
					"            ROUND(TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE), 2) AS Wkhrs,  \n",
					"            ROUND(TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE), 2) AS HrsDay,  \n",
					"            TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"            TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"            'saphr' AS SourceSystemID,\n",
					"            CURRENT_DATE() AS IngestionDate,\n",
					"            CURRENT_TIMESTAMP() AS ValidTo,\n",
					"            \n",
					"            -- Enhanced approval status handling for NULL AbsType\n",
					"            CASE \n",
					"                WHEN AbsType IS NULL OR TRIM(AbsType) = '' THEN 'NULL_ABSTYPE'\n",
					"                WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"                ELSE 'UNKNOWN'\n",
					"            END AS ApprovalStatus,\n",
					"            \n",
					"            -- Enhanced UUID generation for NULL AbsType\n",
					"            CASE\n",
					"                WHEN AbsType IS NULL OR TRIM(AbsType) = '' THEN\n",
					"                    CONCAT('NULL_', MD5(CONCAT_WS('_', StaffNumber, TO_DATE(StartDate,'dd/MM/yyyy'), TO_DATE(EndDate,'dd/MM/yyyy'))))\n",
					"                ELSE\n",
					"                    extract_uuid(AbsType)\n",
					"            END AS RecordUUID,\n",
					"            \n",
					"            -- Enhanced last modified date handling\n",
					"            CASE \n",
					"                WHEN AbsType IS NULL OR TRIM(AbsType) = '' THEN CURRENT_TIMESTAMP()\n",
					"                WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN\n",
					"                    TRY_CAST(\n",
					"                        REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                        AS TIMESTAMP\n",
					"                    )\n",
					"                ELSE CURRENT_TIMESTAMP()\n",
					"            END AS LastModifiedDate,\n",
					"            \n",
					"            MD5(CONCAT_WS('|',\n",
					"                StaffNumber,            \n",
					"                COALESCE(AbsType, 'NULL_ABSTYPE'),                \n",
					"                COALESCE(SicknessGroup, ''),          \n",
					"                TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"                TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"                AttendanceorAbsenceType,\n",
					"                REPLACE(Days, ',', ''),                   \n",
					"                REPLACE(Hrs, ',', ''),                    \n",
					"                Caldays,                \n",
					"                WorkScheduleRule,       \n",
					"                REPLACE(Wkhrs, ',', ''),                  \n",
					"                REPLACE(HrsDay, ',', ''),                 \n",
					"                REPLACE(WkDys, ',', '')\n",
					"            )) AS RowID,\n",
					"            'Y' AS IsActive\n",
					"            \n",
					"        FROM incremental_source\n",
					"        \"\"\")\n",
					"        \n",
					"        # Step 3: Deduplicate incremental staging data\n",
					"        logInfo(\"Creating deduplicated incremental records\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW latest_staging_records AS\n",
					"        SELECT *\n",
					"        FROM (\n",
					"            SELECT *,\n",
					"                ROW_NUMBER() OVER (\n",
					"                    PARTITION BY RecordUUID \n",
					"                    ORDER BY LastModifiedDate DESC, IngestionDate DESC\n",
					"                ) AS rn\n",
					"            FROM staging_absence\n",
					"            WHERE RecordUUID IS NOT NULL\n",
					"        ) ranked\n",
					"        WHERE rn = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        staging_count = spark.sql(\"SELECT COUNT(*) as count FROM latest_staging_records\").collect()[0]['count']\n",
					"        logInfo(f\"Deduplicated staging records: {staging_count}\")\n",
					"        \n",
					"        # Step 4: Filter out cancelled/rejected from staging\n",
					"        logInfo(\"Filtering out cancelled/rejected records from staging\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW final_staging_records AS\n",
					"        SELECT * FROM latest_staging_records\n",
					"        WHERE NOT (\n",
					"            UPPER(CASE \n",
					"                WHEN AbsType IS NULL OR TRIM(AbsType) = '' THEN 'NULL_ABSTYPE'\n",
					"                WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"                ELSE 'UNKNOWN'\n",
					"            END) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"        )\n",
					"        \"\"\")\n",
					"        \n",
					"        final_staging_count = spark.sql(\"SELECT COUNT(*) as count FROM final_staging_records\").collect()[0]['count']\n",
					"        cancelled_in_staging = staging_count - final_staging_count\n",
					"        \n",
					"        logInfo(f\"Final staging records (after filtering): {final_staging_count}\")\n",
					"        logInfo(f\"Cancelled/rejected records in staging: {cancelled_in_staging}\")\n",
					"    \n",
					"    logInfo(\"Incremental data preparation completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in incremental data preparation phase: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Clean up on error\n",
					"    try:\n",
					"        spark.sql(\"DROP VIEW IF EXISTS incremental_source\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_staging_records\")\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    raise e"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Final Processing and Data Loading"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count == 0:\n",
					"        logInfo(\"No incremental processing needed - skipping merge operations\")\n",
					"        final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all WHERE IsActive = 'Y'\").collect()[0]['count']\n",
					"        result[\"record_count\"] = final_record_count\n",
					"        result[\"skipped_count\"] = source_total_count\n",
					"        \n",
					"    else:\n",
					"        # Step 5: Identify records to insert vs update\n",
					"        logInfo(\"Identifying records for insert vs update operations\")\n",
					"        \n",
					"        # Records to insert (new RowIDs)\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_insert AS\n",
					"        SELECT s.* FROM final_staging_records s\n",
					"        LEFT JOIN odw_harmonised_db.sap_hr_absence_all t ON s.RowID = t.RowID\n",
					"        WHERE t.RowID IS NULL\n",
					"        \"\"\")\n",
					"        \n",
					"        # Records to update (existing RowIDs with newer data)\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_update AS\n",
					"        SELECT s.* FROM final_staging_records s\n",
					"        INNER JOIN odw_harmonised_db.sap_hr_absence_all t ON s.RowID = t.RowID\n",
					"        WHERE s.LastModifiedDate > t.ValidTo OR t.IsActive = 'N'\n",
					"        \"\"\")\n",
					"        \n",
					"        insert_count = spark.sql(\"SELECT COUNT(*) as count FROM records_to_insert\").collect()[0]['count']\n",
					"        update_count = spark.sql(\"SELECT COUNT(*) as count FROM records_to_update\").collect()[0]['count']\n",
					"        \n",
					"        logInfo(f\"Records to insert: {insert_count}\")\n",
					"        logInfo(f\"Records to update: {update_count}\")\n",
					"        \n",
					"        # Step 6: Handle cancelled/rejected records by marking as inactive\n",
					"        logInfo(\"Marking cancelled/rejected records as inactive\")\n",
					"        \n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW cancelled_records AS\n",
					"        SELECT * FROM latest_staging_records\n",
					"        WHERE UPPER(CASE \n",
					"            WHEN AbsType IS NULL OR TRIM(AbsType) = '' THEN 'NULL_ABSTYPE'\n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"        \"\"\")\n",
					"        \n",
					"        cancelled_rowids = spark.sql(\"SELECT RowID FROM cancelled_records\").collect()\n",
					"        \n",
					"        if cancelled_rowids:\n",
					"            cancelled_rowids_list = [row['RowID'] for row in cancelled_rowids]\n",
					"            logInfo(f\"Marking {len(cancelled_rowids_list)} cancelled records as inactive\")\n",
					"            \n",
					"            # Use DataFrame operations for security\n",
					"            target_df = spark.table(\"odw_harmonised_db.sap_hr_absence_all\")\n",
					"            target_df.filter(col(\"RowID\").isin(cancelled_rowids_list)).createOrReplaceTempView(\"records_to_deactivate\")\n",
					"            \n",
					"            spark.sql(\"\"\"\n",
					"            MERGE INTO odw_harmonised_db.sap_hr_absence_all AS target\n",
					"            USING records_to_deactivate AS source\n",
					"            ON target.RowID = source.RowID\n",
					"            WHEN MATCHED THEN UPDATE SET IsActive = 'N', ValidTo = CURRENT_TIMESTAMP()\n",
					"            \"\"\")\n",
					"        \n",
					"        # Step 7: Perform incremental updates\n",
					"        if update_count > 0:\n",
					"            logInfo(f\"Updating {update_count} existing records\")\n",
					"            spark.sql(\"\"\"\n",
					"            MERGE INTO odw_harmonised_db.sap_hr_absence_all AS target\n",
					"            USING records_to_update AS source\n",
					"            ON target.RowID = source.RowID\n",
					"            WHEN MATCHED THEN UPDATE SET\n",
					"                StaffNumber = source.StaffNumber,\n",
					"                AbsType = source.AbsType,\n",
					"                SicknessGroup = source.SicknessGroup,\n",
					"                StartDate = source.StartDate,\n",
					"                EndDate = source.EndDate,\n",
					"                AttendanceorAbsenceType = source.AttendanceorAbsenceType,\n",
					"                Days = source.Days,\n",
					"                Hrs = source.Hrs,\n",
					"                Start = source.Start,\n",
					"                Endtime = source.Endtime,\n",
					"                Caldays = source.Caldays,\n",
					"                WorkScheduleRule = source.WorkScheduleRule,\n",
					"                Wkhrs = source.Wkhrs,\n",
					"                HrsDay = source.HrsDay,\n",
					"                WkDys = source.WkDys,\n",
					"                AnnualLeaveStart = source.AnnualLeaveStart,\n",
					"                SourceSystemID = source.SourceSystemID,\n",
					"                IngestionDate = source.IngestionDate,\n",
					"                ValidTo = source.ValidTo,\n",
					"                IsActive = source.IsActive\n",
					"            \"\"\")\n",
					"        \n",
					"        # Step 8: Perform incremental inserts\n",
					"        if insert_count > 0:\n",
					"            logInfo(f\"Inserting {insert_count} new records\")\n",
					"            spark.sql(\"\"\"\n",
					"            INSERT INTO odw_harmonised_db.sap_hr_absence_all (\n",
					"                StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"                AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"                Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"                AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"                RowID, IsActive\n",
					"            )\n",
					"            SELECT \n",
					"                StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"                AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"                Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"                AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"                RowID, IsActive\n",
					"            FROM records_to_insert\n",
					"            \"\"\")\n",
					"        \n",
					"        # Step 9: Final counts and reporting\n",
					"        final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all WHERE IsActive = 'Y'\").collect()[0]['count']\n",
					"        total_processed = insert_count + update_count\n",
					"        \n",
					"        result[\"record_count\"] = final_record_count\n",
					"        result[\"inserted_count\"] = insert_count\n",
					"        result[\"updated_count\"] = update_count\n",
					"        result[\"deleted_count\"] = 0  # We mark as inactive, not delete\n",
					"        result[\"cancelled_deleted_count\"] = len(cancelled_rowids) if cancelled_rowids else 0\n",
					"        \n",
					"        logInfo(\"Incremental load completed successfully:\")\n",
					"        logInfo(f\"- Total source records available: {source_total_count}\")\n",
					"        logInfo(f\"- Incremental records processed: {incremental_source_count}\")\n",
					"        logInfo(f\"- New records inserted: {insert_count}\")\n",
					"        logInfo(f\"- Existing records updated: {update_count}\")\n",
					"        logInfo(f\"- Records marked as cancelled: {len(cancelled_rowids) if cancelled_rowids else 0}\")\n",
					"        logInfo(f\"- Total active records in target: {final_record_count}\")\n",
					"        logInfo(f\"- Records skipped (no changes): {source_total_count - incremental_source_count}\")\n",
					"    \n",
					"    # Clean up temporary views\n",
					"    spark.sql(\"DROP VIEW IF EXISTS incremental_source\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_staging_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS records_to_insert\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS records_to_update\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS cancelled_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS records_to_deactivate\")\n",
					"    \n",
					"    logInfo(\"Successfully completed incremental load process\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in incremental merge processing phase: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1\n",
					"    result[\"inserted_count\"] = -1\n",
					"    result[\"updated_count\"] = -1\n",
					"    result[\"deleted_count\"] = -1\n",
					"    result[\"cancelled_deleted_count\"] = -1\n",
					"    \n",
					"    # Clean up on error\n",
					"    try:\n",
					"        spark.sql(\"DROP VIEW IF EXISTS incremental_source\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_insert\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_update\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS cancelled_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_deactivate\")\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}