{
  "name": "py_inspector_delta_to_sb",
  "properties": {
    "folder": {"name": "publish"},
    "nbformat": 4,
    "nbformat_minor": 2,
    "bigDataPool": {"referenceName": "pinssynspodw34", "type": "BigDataPoolReference"},
    "sessionProperties": {
      "driverMemory": "28g",
      "driverCores": 4,
      "executorMemory": "28g",
      "executorCores": 4,
      "numExecutors": 2,
      "conf": {
        "spark.dynamicAllocation.enabled": "false",
        "spark.dynamicAllocation.minExecutors": "2",
        "spark.dynamicAllocation.maxExecutors": "2",
        "spark.autotune.trackingId": "c5f3f6a1-1f45-4c38-9f8b-5d92c0c4e24a"
      }
    },
    "metadata": {
      "saveOutput": true,
      "enableDebugMode": false,
      "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"},
      "language_info": {"name": "python"},
      "a365ComputeOptions": {
        "id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
        "name": "pinssynspodw34",
        "type": "Spark",
        "endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
        "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"},
        "sparkVersion": "3.4",
        "nodeCount": 3,
        "cores": 4,
        "memory": 28,
        "automaticScaleJobs": false
      },
      "sessionKeepAliveTimeout": 30
    },
    "cells": [
      {"cell_type": "code", "metadata": {"tags":["parameters"]}, "source": [
        "db_name='odw_curated_db'\n",
        "table_name='inspector'\n",
        "primary_key='source_id'\n",
        "enable_validation=False\n",
        "enable_sessions=False\n",
        "schema_name='PINS.Inspector'\n",
        "schema_version='v1'\n"
      ], "execution_count": null},
      {"cell_type": "code", "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when\n",
        "from notebookutils import mssparkutils\n",
        "from delta.tables import DeltaTable\n",
        "import json, hashlib\n",
        "from datetime import datetime\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n"
      ], "execution_count": null},
      {"cell_type": "code", "source": [
        "# Resolve table path and latest delta version\n",
        "table_fqn=f'{db_name}.{table_name}'\n",
        "table_path = spark.sql(f\"DESCRIBE DETAIL {table_fqn}\").select('location').first()['location']\n",
        "dt = DeltaTable.forPath(spark, table_path)\n",
        "latest_version = dt.history().select('version').orderBy('version', ascending=False).first()['version']\n",
        "\n",
        "# Read watermark (last published version)\n",
        "try:\n",
        "    wm_df = spark.sql(\"select LastRun from odw_config_db.pipeline_runs where sourceName = 'inspector-curated-version'\")\n",
        "    watermark = int(wm_df.first()['LastRun']) if wm_df.count() > 0 else -1\n",
        "except Exception:\n",
        "    watermark = -1\n",
        "start_version = watermark + 1\n",
        "\n",
        "messages = []\n",
        "change_count = 0\n",
        "if start_version <= latest_version:\n",
        "    cdf = (spark.read.format('delta')\n",
        "           .option('readChangeFeed','true')\n",
        "           .option('startingVersion', start_version)\n",
        "           .option('endingVersion', latest_version)\n",
        "           .load(table_path))\n",
        "    # keep only post-image for updates and inserts/deletes\n",
        "    cdf = cdf.where(col('_change_type').isin('insert','update_postimage','delete'))\n",
        "    # map change type to eventType\n",
        "    cdf = cdf.withColumn('eventType', when(col('_change_type')=='insert','create')\n",
        "                               .when(col('_change_type')=='update_postimage','update')\n",
        "                               .otherwise('delete'))\n",
        "    cols_to_drop=['_change_type','_commit_version','_commit_timestamp']\n",
        "    # materialize rows\n",
        "    rows = cdf.toLocalIterator()\n",
        "    run_id = mssparkutils.runtime.context.get('pipelinejobid','')\n",
        "    for r in rows:\n",
        "        rd = r.asDict()\n",
        "        event_type = rd.pop('eventType')\n",
        "        commit_version = rd.pop('_commit_version', latest_version) if '_commit_version' in rd else latest_version\n",
        "        for k in list(rd.keys()):\n",
        "            if k in cols_to_drop: rd.pop(k, None)\n",
        "        entity_id = str(rd.get(primary_key))\n",
        "        # Body as strict JSON string\n",
        "        body_json = json.dumps(rd, default=str)\n",
        "        # Deterministic MessageId\n",
        "        mid = hashlib.sha256(f\"{entity_id}-{commit_version}\".encode()).hexdigest()\n",
        "        broker = {'MessageId': mid, 'ContentType': 'application/json'}\n",
        "        if enable_sessions: broker['SessionId'] = entity_id\n",
        "        if run_id: broker['CorrelationId'] = str(run_id)\n",
        "        user = {\n",
        "            'type': event_type,\n",
        "            'entityId': entity_id,\n",
        "            'timestamp': datetime.utcnow().isoformat()+'Z',\n",
        "            'sourceSystem': 'ODW',\n",
        "            'schema': schema_name,\n",
        "            'schemaVersion': schema_version,\n",
        "            'changeVersion': int(commit_version)\n",
        "        }\n",
        "        messages.append({'Body': body_json, 'BrokerProperties': broker, 'UserProperties': user})\n",
        "    change_count = len(messages)\n",
        "\n",
        "# write audit copy\n",
        "if change_count > 0:\n",
        "    target_folder = f\"abfss://odw-curated@{storage_account}/{table_name}/curated_to_sb/{latest_version}\"\n",
        "    mssparkutils.fs.mkdirs(target_folder)\n",
        "    mssparkutils.fs.put(f\"{target_folder}/sb_messages.json\", json.dumps(messages), True)\n",
        "\n",
        "# Return composite result; messages_text is the JSON array string expected by pln_publish_to_sb\n",
        "result = {\n",
        "  'messages_text': json.dumps(messages),\n",
        "  'counts': change_count,\n",
        "  'latestVersion': int(latest_version)\n",
        "}\n",
        "mssparkutils.notebook.exit(json.dumps(result))\n"
      ], "execution_count": null}
    ]
  }
}