{
	"name": "py_load_listed_buildings_to_harmonised",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "522b760e-ef7a-472c-9fa5-a33cadb7a7d3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from odw.core.util.logging_util import LoggingUtil"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import IntegerType, ArrayType, StructType, StructField\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql import functions as F\n",
					"import json\n",
					"from datetime import datetime\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import md5, concat, coalesce, lit, col\n",
					"from delta.tables import DeltaTable"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialize Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_utils_common_logging_output"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define variables\n",
					"source_table = \"odw_standardised_db.listed_building\"\n",
					"target_table = \"odw_harmonised_db.listed_building\"\n",
					"primary_key = 'entity'\n",
					"\n",
					"# Initialize logging utility\n",
					"logging_util = LoggingUtil()\n",
					"\n",
					"# Initialize tracking variables\n",
					"start_exec_time = str(datetime.now())\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize Application Insights logger\n",
					"app_insight_logger = ProcessingLogger()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"try:\n",
					"    # Read and transform data from source\n",
					"    logging_util.log_info(f\"Reading data from {source_table}\")\n",
					"    \n",
					"    # Check if target table exists to determine if this is initial load or incremental\n",
					"    target_exists = spark._jsparkSession.catalog().tableExists('odw_harmonised_db', 'listed_building')\n",
					"    logging_util.log_info(f\"Target table exists? {target_exists}\")\n",
					"    \n",
					"    if target_exists:\n",
					"        logging_util.log_info(\"Target table exists - performing incremental processing\")\n",
					"        # Table exists, we'll use MERGE operation\n",
					"        pass\n",
					"    else:\n",
					"        logging_util.log_info(\"Target table does not exist - performing initial load\")\n",
					"    \n",
					"    # Read source data and add row numbers for ID generation\n",
					"    source_df = spark.sql(f\"\"\"\n",
					"        SELECT \n",
					"            dataset,\n",
					"            `end-date` AS endDate,\n",
					"            entity,\n",
					"            `entry-date` AS entryDate,\n",
					"            geometry,\n",
					"            `listed-building-grade` AS listedBuildingGrade,\n",
					"            name,\n",
					"            `organisation-entity` AS organisationEntity,\n",
					"            point,\n",
					"            prefix,\n",
					"            reference,\n",
					"            `start-date` AS startDate,\n",
					"            typology,\n",
					"            `documentation-url` AS documentationUrl\n",
					"        FROM {source_table}\n",
					"    \"\"\") \\\n",
					"    .withColumn(\"dateReceived\", F.current_date())\n",
					"    \n",
					"    # Count source rows for visibility\n",
					"    source_row_count = source_df.count()\n",
					"    logging_util.log_info(f\"Source rows retrieved: {source_row_count}\")\n",
					"    \n",
					"    # Add rowID calculation    \n",
					"    final_df = source_df.withColumn(\n",
					"        \"rowID\",\n",
					"        md5(concat(\n",
					"            coalesce(col(\"entity\"), lit('.')),\n",
					"            coalesce(col(\"dataset\"), lit('.')),\n",
					"            coalesce(col(\"endDate\"), lit('.')),\n",
					"            coalesce(col(\"entryDate\"), lit('.')),\n",
					"            coalesce(col(\"geometry\"), lit('.')),\n",
					"            coalesce(col(\"listedBuildingGrade\"), lit('.')),\n",
					"            coalesce(col(\"name\"), lit('.')),\n",
					"            coalesce(col(\"organisationEntity\"), lit('.')),\n",
					"            coalesce(col(\"point\"), lit('.')),\n",
					"            coalesce(col(\"prefix\"), lit('.')),\n",
					"            coalesce(col(\"reference\"), lit('.')),\n",
					"            coalesce(col(\"startDate\"), lit('.')),\n",
					"            coalesce(col(\"typology\"), lit('.')),\n",
					"            coalesce(col(\"documentationUrl\"), lit('.'))\n",
					"            ))\n",
					"    ) \\\n",
					"    .withColumn(\"validTo\", lit(None).cast(\"timestamp\")) \\\n",
					"    .withColumn(\"isActive\", lit(\"Y\"))\n",
					"\n",
					"    \n",
					"    logging_util.log_info(f\"Processing records for {target_table}\")\n",
					"    \n",
					"    # Write data to target table\n",
					"    if target_exists:\n",
					"        logging_util.log_info(\"Merging into existing harmonised table\")\n",
					"        \n",
					"        # Use Delta Lake MERGE INTO\n",
					"        deltaTable = DeltaTable.forName(spark, target_table)\n",
					"\n",
					"        logging_util.log_info(\"Starting Delta MERGE operation\")\n",
					"        \n",
					"        # Step 1: Mark old versions as inactive when rowID differs but entity matches\n",
					"        logging_util.log_info(\"Step 1: Marking old versions as inactive for changed records\")\n",
					"        (\n",
					"            deltaTable.alias(\"t\")\n",
					"            .merge(\n",
					"                final_df.alias(\"s\"),\n",
					"                \"t.entity = s.entity AND t.isActive = 'Y'\" \n",
					"            )\n",
					"            .whenMatchedUpdate(\n",
					"                condition=\"t.rowID <> s.rowID\",\n",
					"                set={\n",
					"                    \"validTo\": \"current_date()\",\n",
					"                    \"isActive\": \"'N'\"\n",
					"                }\n",
					"            )\n",
					"            .execute()\n",
					"        )\n",
					"        \n",
					"        # Step 2: Insert all records from source (new entities and new versions of existing entities)\n",
					"        # Only records that don't exactly match (same entity AND same rowID) will be inserted\n",
					"        logging_util.log_info(\"Step 2: Inserting new and updated records\")\n",
					"        (\n",
					"            deltaTable.alias(\"t\")\n",
					"            .merge(\n",
					"                final_df.alias(\"s\"),\n",
					"                \"t.entity = s.entity AND t.rowID = s.rowID AND t.isActive = 'Y'\" \n",
					"            )\n",
					"            .whenNotMatchedInsert(values={\n",
					"                \"dataset\": \"s.dataset\",\n",
					"                \"endDate\": \"s.endDate\",\n",
					"                \"entity\": \"s.entity\",\n",
					"                \"entryDate\": \"s.entryDate\",\n",
					"                \"geometry\": \"s.geometry\",\n",
					"                \"listedBuildingGrade\": \"s.listedBuildingGrade\",\n",
					"                \"name\": \"s.name\",\n",
					"                \"organisationEntity\": \"s.organisationEntity\",\n",
					"                \"point\": \"s.point\",\n",
					"                \"prefix\": \"s.prefix\",\n",
					"                \"reference\": \"s.reference\",\n",
					"                \"startDate\": \"s.startDate\",\n",
					"                \"typology\": \"s.typology\",\n",
					"                \"documentationUrl\": \"s.documentationUrl\",\n",
					"                \"dateReceived\": \"s.dateReceived\",\n",
					"                \"rowID\": \"s.rowID\",\n",
					"                \"validTo\": \"s.validTo\",\n",
					"                \"isActive\": \"s.isActive\"\n",
					"            })\n",
					"            .execute()\n",
					"        )\n",
					"        logging_util.log_info(\"Delta MERGE operation completed\")\n",
					"        \n",
					"    else:\n",
					"        logging_util.log_info(\"Creating new harmonised table\")\n",
					"        logging_util.log_info(f\"Initial load row count: {final_df.count()}\")\n",
					"        final_df.write.format(\"delta\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .saveAsTable(target_table)\n",
					"    \n",
					"    # Get record count\n",
					"    if target_exists:\n",
					"        # For incremental loads, calculate new and changed rows\n",
					"        latest_df = deltaTable.toDF().filter(\"isActive = 'Y'\")\n",
					"        new_rows = final_df.join(latest_df, \"reference\", \"left_anti\")\n",
					"        changed_rows = final_df.join(\n",
					"            latest_df, \"reference\", \"inner\"\n",
					"        ).filter(final_df.rowID != latest_df.rowID)\n",
					"        insert_count = new_rows.count()\n",
					"        update_count = changed_rows.count()\n",
					"    else:\n",
					"        # For initial loads, all records are inserts\n",
					"        insert_count = final_df.count()\n",
					"        update_count = 0\n",
					"\n",
					"    logging_util.log_info(f\"Record count summary - inserts: {insert_count}, updates: {update_count}\")\n",
					"    \n",
					"    end_exec_time = str(datetime.now())\n",
					"    logging_util.log_info(f\"Successfully processed {target_table} - {insert_count} inserts, {update_count} updates\")\n",
					"    \n",
					"    \n",
					"    # Add successful result to logger\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=insert_count,\n",
					"        update_count=update_count,\n",
					"        delete_count=delete_count,\n",
					"        table_result=\"success\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time\n",
					"    )\n",
					"    \n",
					"except Exception as e:\n",
					"    # Handle errors with proper logging\n",
					"    logging_util.log_error(f\"Error processing {target_table}: {e}\")\n",
					"    error_message = app_insight_logger.format_error_message(e, max_length=800)\n",
					"    \n",
					"    end_exec_time = str(datetime.now())\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=insert_count,\n",
					"        update_count=update_count,\n",
					"        delete_count=delete_count,\n",
					"        table_result=\"failed\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        error_message=error_message\n",
					"    )\n",
					"    \n",
					"    # Exit with the JSON result\n",
					"    mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Generate and exit with final logging results\n",
					"mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"select count(*) from odw_standardised_db.listed_building"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"select count(*) from odw_harmonised_db.listed_building"
				],
				"execution_count": null
			}
		]
	}
}