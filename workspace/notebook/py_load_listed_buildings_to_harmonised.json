{
	"name": "py_load_listed_buildings_to_harmonised",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "bcbe1a3c-f03d-46a1-a88e-a9f799846168"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from odw.core.util.logging_util import LoggingUtil"
				],
				"execution_count": 211
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import IntegerType, ArrayType, StructType, StructField\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql import functions as F\n",
					"import json\n",
					"from datetime import datetime\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import md5, concat, coalesce, lit, col\n",
					"from delta.tables import DeltaTable"
				],
				"execution_count": 212
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialize Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_utils_common_logging_output"
				],
				"execution_count": 213
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define variables\n",
					"source_table = \"odw_standardised_db.listed_building\"\n",
					"target_table = \"odw_harmonised_db.listed_building\"\n",
					"primary_key = 'entity'\n",
					"\n",
					"# Initialize logging utility\n",
					"logging_util = LoggingUtil()\n",
					"\n",
					"# Initialize tracking variables\n",
					"start_exec_time = str(datetime.now())\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0"
				],
				"execution_count": 214
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize Application Insights logger\n",
					"app_insight_logger = ProcessingLogger()"
				],
				"execution_count": 215
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"try:\n",
					"    # Read and transform data from source\n",
					"    logging_util.log_info(f\"Reading data from {source_table}\")\n",
					"    \n",
					"    # Check if target table exists to determine if this is initial load or incremental\n",
					"    target_exists = spark._jsparkSession.catalog().tableExists('odw_harmonised_db', 'listed_building')\n",
					"    logging_util.log_info(f\"Target table exists? {target_exists}\")\n",
					"    \n",
					"    if target_exists:\n",
					"        logging_util.log_info(\"Target table exists - performing incremental processing\")\n",
					"        # Table exists, we'll use MERGE operation\n",
					"        pass\n",
					"    else:\n",
					"        logging_util.log_info(\"Target table does not exist - performing initial load\")\n",
					"    \n",
					"    # Read source data and add row numbers for ID generation\n",
					"    source_df = spark.sql(f\"\"\"\n",
					"        SELECT \n",
					"            dataset,\n",
					"            `end-date` AS endDate,\n",
					"            entity,\n",
					"            `entry-date` AS entryDate,\n",
					"            geometry,\n",
					"            `listed-building-grade` AS listedBuildingGrade,\n",
					"            name,\n",
					"            `organisation-entity` AS organisationEntity,\n",
					"            point,\n",
					"            prefix,\n",
					"            reference,\n",
					"            `start-date` AS startDate,\n",
					"            typology,\n",
					"            `documentation-url` AS documentationUrl,\n",
					"        FROM {source_table}\n",
					"    \"\"\") \\\n",
					"    .withColumn(\"dateReceived\", F.current_date())\n",
					"    \n",
					"    # Count source rows for visibility\n",
					"    source_row_count = source_df.count()\n",
					"    logging_util.log_info(f\"Source rows retrieved: {source_row_count}\")\n",
					"    \n",
					"    # Add rowID calculation    \n",
					"    final_df = source_df.withColumn(\n",
					"        \"rowID\",\n",
					"        md5(concat(\n",
					"            coalesce(col(\"entity\"), lit('.')),\n",
					"            coalesce(col(\"dataset\"), lit('.')),\n",
					"            coalesce(col(\"endDate\"), lit('.')),\n",
					"            coalesce(col(\"entryDate\"), lit('.')),\n",
					"            coalesce(col(\"geometry\"), lit('.')),\n",
					"            coalesce(col(\"listedBuildingGrade\"), lit('.')),\n",
					"            coalesce(col(\"name\"), lit('.')),\n",
					"            coalesce(col(\"organisationEntity\"), lit('.')),\n",
					"            coalesce(col(\"point\"), lit('.')),\n",
					"            coalesce(col(\"prefix\"), lit('.')),\n",
					"            coalesce(col(\"reference\"), lit('.')),\n",
					"            coalesce(col(\"startDate\"), lit('.')),\n",
					"            coalesce(col(\"typology\"), lit('.')),\n",
					"            coalesce(col(\"documentationUrl\"), lit('.'))\n",
					"            ))\n",
					"    ) \\\n",
					"    .withColumn(\"validTo\", lit(None).cast(\"timestamp\")) \\\n",
					"    .withColumn(\"isActive\", lit(\"Y\"))\n",
					"\n",
					"    \n",
					"    logging_util.log_info(f\"Processing records for {target_table}\")\n",
					"    \n",
					"    # Write data to target table\n",
					"    if target_exists:\n",
					"        logging_util.log_info(\"Merging into existing harmonised table\")\n",
					"        \n",
					"        # Use Delta Lake MERGE INTO\n",
					"        deltaTable = DeltaTable.forName(spark, target_table)\n",
					"\n",
					"        logging_util.log_info(\"Starting Delta MERGE operation\")\n",
					"        \n",
					"        (\n",
					"            deltaTable.alias(\"t\")\n",
					"            .merge(\n",
					"                final_df.alias(\"s\"),\n",
					"                \"t.entity = s.entity\" \n",
					"            )\n",
					"            .whenMatchedUpdate(\n",
					"                condition=\"t.rowID <> s.rowID AND t.isActive = 'Y'\",\n",
					"                set={\n",
					"                    \"validTo\": \"s.validTo\",\n",
					"                    \"isActive\": \"s.isActive\"\n",
					"                }\n",
					"            )\n",
					"            .whenNotMatchedInsert(values={\n",
					"                \"dataset\": \"s.dataset\",\n",
					"                \"endDate\": \"s.endDate\",\n",
					"                \"entity\": \"s.entity\",\n",
					"                \"entryDate\": \"s.entryDate\",\n",
					"                \"geometry\": \"s.geometry\",\n",
					"                \"listedBuildingGrade\": \"s.listedBuildingGrade\",\n",
					"                \"name\": \"s.name\",\n",
					"                \"organisationEntity\": \"s.organisationEntity\",\n",
					"                \"point\": \"s.point\",\n",
					"                \"prefix\": \"s.prefix\",\n",
					"                \"reference\": \"s.reference\",\n",
					"                \"startDate\": \"s.startDate\",\n",
					"                \"typology\": \"s.typology\",\n",
					"                \"documentationUrl\": \"s.documentationUrl\",\n",
					"                \"dateReceived\": \"s.dateReceived\",\n",
					"                \"rowID\": \"s.rowID\",\n",
					"                \"validTo\": \"s.validTo\",\n",
					"                \"isActive\": \"s.isActive\"\n",
					"            })\n",
					"            .execute()\n",
					"        )\n",
					"        logging_util.log_info(\"Delta MERGE operation completed\")\n",
					"        \n",
					"    else:\n",
					"        logging_util.log_info(\"Creating new harmonised table\")\n",
					"        logging_util.log_info(f\"Initial load row count: {final_df.count()}\")\n",
					"        final_df.write.format(\"delta\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .saveAsTable(target_table)\n",
					"    \n",
					"    # Get record count\n",
					"    if target_exists:\n",
					"        # For incremental loads, calculate new and changed rows\n",
					"        latest_df = deltaTable.toDF().filter(\"isActive = 'Y'\")\n",
					"        new_rows = final_df.join(latest_df, \"reference\", \"left_anti\")\n",
					"        changed_rows = final_df.join(\n",
					"            latest_df, \"reference\", \"inner\"\n",
					"        ).filter(final_df.rowID != latest_df.rowID)\n",
					"        insert_count = new_rows.count()\n",
					"        update_count = changed_rows.count()\n",
					"    else:\n",
					"        # For initial loads, all records are inserts\n",
					"        insert_count = final_df.count()\n",
					"        update_count = 0\n",
					"\n",
					"    logging_util.log_info(f\"Record count summary - inserts: {insert_count}, updates: {update_count}\")\n",
					"    \n",
					"    end_exec_time = str(datetime.now())\n",
					"    logging_util.log_info(f\"Successfully processed {target_table} - {insert_count} inserts, {update_count} updates\")\n",
					"    \n",
					"    \n",
					"    # Add successful result to logger\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=insert_count,\n",
					"        update_count=update_count,\n",
					"        delete_count=delete_count,\n",
					"        table_result=\"success\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time\n",
					"    )\n",
					"    \n",
					"except Exception as e:\n",
					"    # Handle errors with proper logging\n",
					"    logging_util.log_error(f\"Error processing {target_table}: {e}\")\n",
					"    error_message = app_insight_logger.format_error_message(e, max_length=800)\n",
					"    \n",
					"    end_exec_time = str(datetime.now())\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=insert_count,\n",
					"        update_count=update_count,\n",
					"        delete_count=delete_count,\n",
					"        table_result=\"failed\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        error_message=error_message\n",
					"    )\n",
					"    \n",
					"    # Exit with the JSON result\n",
					"    mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": 250
			},
			{
				"cell_type": "code",
				"source": [
					"deltaTable"
				],
				"execution_count": 254
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Generate and exit with final logging results\n",
					"mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# from pyspark.sql import SparkSession\n",
					"# from pyspark.sql.types import StructType, StructField, StringType\n",
					"\n",
					"# spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"# # Define schema matching your table\n",
					"# schema = StructType([\n",
					"#     StructField(\"dataset\", StringType(), True),\n",
					"#     StructField(\"end-date\", StringType(), True),\n",
					"#     StructField(\"entity\", StringType(), True),\n",
					"#     StructField(\"entry-date\", StringType(), True),\n",
					"#     StructField(\"geometry\", StringType(), True),\n",
					"#     StructField(\"name\", StringType(), True),\n",
					"#     StructField(\"organisation-entity\", StringType(), True),\n",
					"#     StructField(\"point\", StringType(), True),\n",
					"#     StructField(\"prefix\", StringType(), True),\n",
					"#     StructField(\"reference\", StringType(), True),\n",
					"#     StructField(\"start-date\", StringType(), True),\n",
					"#     StructField(\"typology\", StringType(), True),\n",
					"#     StructField(\"documentation-url\", StringType(), True),\n",
					"#     StructField(\"listed-building-grade\", StringType(), True)\n",
					"# ])\n",
					"\n",
					"# # Mock row data\n",
					"# data = [(\n",
					"#     \"TestDataset\",\n",
					"#     \"2025-12-31\",\n",
					"#     \"31717216\",\n",
					"#     \"2025-09-09\",\n",
					"#     None,\n",
					"#     \"Test Building\",\n",
					"#     \"TestOrganisation\",\n",
					"#     None,\n",
					"#     \"TB\",\n",
					"#     \"REF123\",\n",
					"#     \"2025-01-01\",\n",
					"#     \"TestType\",\n",
					"#     \"http://example.com\",\n",
					"#     \"A\"\n",
					"# )]\n",
					"\n",
					"# # Create DataFrame\n",
					"# mock_df = spark.createDataFrame(data, schema)\n",
					"\n",
					"# # Show mock row\n",
					"# mock_df.show(truncate=False)\n",
					"# mock_df.write.format(\"parquet\").mode(\"append\").saveAsTable(\"odw_standardised_db.listed_building\")"
				],
				"execution_count": 233
			},
			{
				"cell_type": "code",
				"source": [
					"# from pyspark.sql import SparkSession\n",
					"\n",
					"# spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"# # Load the table\n",
					"# df = spark.table(\"odw_standardised_db.listed_building\")\n",
					"\n",
					"# # Filter out the row you want to drop\n",
					"# df_filtered = df.filter(\n",
					"#     ~((df[\"entity\"] == \"31717216\") & (df[\"dataset\"] == \"listed-building\"))\n",
					"# )\n",
					"\n",
					"# # Write to a temporary table\n",
					"# df_filtered.write.mode(\"overwrite\").saveAsTable(\"odw_standardised_db.listed_building_temp\")\n",
					"\n",
					"# # Replace the original table with the temp table\n",
					"# spark.sql(\"DROP TABLE odw_standardised_db.listed_building\")\n",
					"# spark.sql(\"ALTER TABLE odw_standardised_db.listed_building_temp RENAME TO odw_standardised_db.listed_building\")\n",
					""
				],
				"execution_count": 241
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"select * from odw_standardised_db.listed_building\n",
					"where entity = '31717216'"
				],
				"execution_count": 242
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"select count(*) from odw_harmonised_db.listed_building\n",
					"where entity = '31717216'"
				],
				"execution_count": 247
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}