{
	"name": "py_load_listed_buildings_to_harmonised",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4d41f6cf-1fb4-4980-b074-fdd75ede13fd"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from odw.core.util.logging_util import LoggingUtil"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import IntegerType, ArrayType, StructType, StructField\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql import functions as F\n",
					"import json\n",
					"from datetime import datetime\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import md5, concat, coalesce, lit, col\n",
					"from delta.tables import DeltaTable"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialize Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_utils_common_logging_output"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define variables\n",
					"source_table = \"odw_standardised_db.listed_building\"\n",
					"target_table = \"odw_harmonised_db.listed_building\"\n",
					"primary_key = 'entity'\n",
					"\n",
					"# Initialize logging utility\n",
					"logging_util = LoggingUtil()\n",
					"\n",
					"# Initialize tracking variables\n",
					"start_exec_time = str(datetime.now())\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize Application Insights logger\n",
					"app_insight_logger = ProcessingLogger()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"try:\n",
					"    # Read and transform data from source\n",
					"    logging_util.log_info(f\"Reading data from {source_table}\")\n",
					"    \n",
					"    # Check if target table exists to determine if this is initial load or incremental\n",
					"    target_exists = spark._jsparkSession.catalog().tableExists('odw_harmonised_db', 'listed_building')\n",
					"    \n",
					"    if target_exists:\n",
					"        logging_util.log_info(\"Target table exists - performing incremental processing\")\n",
					"        # Get max ID for incremental ID assignment\n",
					"        max_id_result = spark.sql(f\"SELECT COALESCE(MAX(rowID), 0) as max_id FROM {target_table}\").collect()\n",
					"        max_existing_id = max_id_result[0]['max_id']\n",
					"    else:\n",
					"        logging_util.log_info(\"Target table does not exist - performing initial load\")\n",
					"        max_existing_id = 0\n",
					"    \n",
					"    # Read source data and add row numbers for ID generation\n",
					"    source_df = spark.sql(f\"\"\"\n",
					"        SELECT \n",
					"            dataset,\n",
					"            `end-date` AS endDate,\n",
					"            entity,\n",
					"            `entry-date` AS entryDate,\n",
					"            geometry,\n",
					"            `listed-building-grade` AS listedBuildingGrade,\n",
					"            name,\n",
					"            `organisation-entity` AS organisationEntity,\n",
					"            point,\n",
					"            prefix,\n",
					"            reference,\n",
					"            `start-date` AS startDate,\n",
					"            typology,\n",
					"            `documentation-url` AS documentationUrl,\n",
					"            'Y' as IsActive\n",
					"        FROM {source_table}\n",
					"    \"\"\") \\\n",
					"    .withColumn(\"dateReceived\", F.current_date())\n",
					"    \n",
					"    \n",
					"    # Add rowID calculation    \n",
					"    final_df = source_df.withColumn(\n",
					"        \"rowID\",\n",
					"        md5(concat(\n",
					"            coalesce(col(\"entity\"), lit('.')),\n",
					"            coalesce(col(\"dataset\"), lit('.')),\n",
					"            coalesce(col(\"endDate\"), lit('.')),\n",
					"            coalesce(col(\"entryDate\"), lit('.')),\n",
					"            coalesce(col(\"geometry\"), lit('.')),\n",
					"            coalesce(col(\"listedBuildingGrade\"), lit('.')),\n",
					"            coalesce(col(\"name\"), lit('.')),\n",
					"            coalesce(col(\"organisationEntity\"), lit('.')),\n",
					"            coalesce(col(\"point\"), lit('.')),\n",
					"            coalesce(col(\"prefix\"), lit('.')),\n",
					"            coalesce(col(\"reference\"), lit('.')),\n",
					"            coalesce(col(\"startDate\"), lit('.')),\n",
					"            coalesce(col(\"typology\"), lit('.')),\n",
					"            coalesce(col(\"documentationUrl\"), lit('.')),\n",
					"            coalesce(col(\"dateReceived\"), lit('.'))\n",
					"            ))\n",
					"    ) \\\n",
					"    .withColumn(\"validTo\", F.lit(None).cast(\"timestamp\")) \\\n",
					"    .withColumn(\"isActive\", lit(\"Y\"))\n",
					"    \n",
					"    logging_util.log_info(f\"Processing records for {target_table}\")\n",
					"    \n",
					"    # Write data to target table\n",
					"    if target_exists:\n",
					"        logging_util.log_info(\"Merging into existing harmonised table\")\n",
					"        \n",
					"        # Use Delta Lake MERGE INTO\n",
					"        deltaTable = DeltaTable.forName(spark, target_table)\n",
					"        \n",
					"        (\n",
					"            deltaTable.alias(\"t\")\n",
					"            .merge(\n",
					"                final_df.alias(\"s\"),\n",
					"                \"t.entity = s.entity\" \n",
					"            )\n",
					"            .whenMatchedUpdate(\n",
					"                condition=\"t.rowID <> s.rowID AND t.isActive = 'Y'\",\n",
					"                set={\n",
					"                    \"validTo\": \"current_date()\",\n",
					"                    \"isActive\": \"'N'\"\n",
					"                }\n",
					"            )\n",
					"            .whenNotMatchedInsert(values={\n",
					"                \"dataset\": \"s.dataset\",\n",
					"                \"endDate\": \"s.endDate\",\n",
					"                \"entity\": \"s.entity\",\n",
					"                \"entryDate\": \"s.entryDate\",\n",
					"                \"geometry\": \"s.geometry\",\n",
					"                \"listedBuildingGrade\": \"s.listedBuildingGrade\",\n",
					"                \"name\": \"s.name\",\n",
					"                \"organisationEntity\": \"s.organisationEntity\",\n",
					"                \"point\": \"s.point\",\n",
					"                \"prefix\": \"s.prefix\",\n",
					"                \"reference\": \"s.reference\",\n",
					"                \"startDate\": \"s.startDate\",\n",
					"                \"typology\": \"s.typology\",\n",
					"                \"documentationUrl\": \"s.documentationUrl\",\n",
					"                \"dateReceived\": \"s.dateReceived\",\n",
					"                \"rowID\": \"s.rowID\",\n",
					"                \"validTo\": \"s.validTo\",\n",
					"                \"isActive\": \"s.isActive\"\n",
					"            })\n",
					"            .execute()\n",
					"        )\n",
					"        \n",
					"    else:\n",
					"        logging_util.log_info(\"Creating new harmonised table\")\n",
					"        final_df.write.format(\"delta\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .saveAsTable(target_table)\n",
					"    \n",
					"    # Get record count\n",
					"    latest_df = deltaTable.toDF().filter(\"isActive = 'Y'\")\n",
					"    new_rows = final_df.join(latest_df, \"reference\", \"left_anti\")\n",
					"    changed_rows = final_df.join(\n",
					"        latest_df, \"reference\", \"inner\"\n",
					"    ).filter(final_df.rowID != latest_df.rowID)\n",
					"    insert_count = new_rows.count()\n",
					"    update_count = changed_rows.count()\n",
					"\n",
					"    end_exec_time = str(datetime.now())\n",
					"    logging_util.log_info(f\"Successfully processed {target_table} with {insert_count} records\")\n",
					"    \n",
					"    \n",
					"    # Add successful result to logger\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=insert_count,\n",
					"        update_count=update_count,\n",
					"        delete_count=delete_count,\n",
					"        table_result=\"success\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time\n",
					"    )\n",
					"    \n",
					"except Exception as e:\n",
					"    # Handle errors with proper logging\n",
					"    logging_util.log_error(f\"Error processing {target_table}: {e}\")\n",
					"    error_message = app_insight_logger.format_error_message(e, max_length=800)\n",
					"    \n",
					"    end_exec_time = str(datetime.now())\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=insert_count,\n",
					"        update_count=update_count,\n",
					"        delete_count=delete_count,\n",
					"        table_result=\"failed\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        error_message=error_message\n",
					"    )\n",
					"    \n",
					"    # Exit with the JSON result\n",
					"    mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Generate and exit with final logging results\n",
					"mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			}
		]
	}
}