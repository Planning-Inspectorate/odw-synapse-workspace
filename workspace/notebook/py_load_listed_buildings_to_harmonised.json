{
	"name": "py_load_listed_buildings_to_harmonised",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "1d14cc23-0c22-4e65-982a-d2895656d41b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Remove data that's changed in some way. We won't keep an explicit history as we have no requirements, this is master data, and if we need to debug changes we can use the delta timetravel history."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import asyncio\r\n",
					"import nest_asyncio\r\n",
					"import tracemalloc\r\n",
					"tracemalloc.start()\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from notebookutils import mssparkutils\r\n",
					"import json\r\n",
					"import calendar\r\n",
					"from datetime import datetime, timedelta, date\r\n",
					"import pandas as pd\r\n",
					"import os\r\n",
					"import re\r\n",
					"from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat,max as spark_max\r\n",
					"from pyspark.sql.types import StringType, DateType, TimestampType, IntegerType, FloatType, StructType, StructField\r\n",
					"from delta import DeltaTable"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_utils_common_logging_output"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize Logging and Tracking Metrics\r\n",
					"start_exec_time = str(datetime.now())\r\n",
					"insert_count = 0\r\n",
					"update_count = 0\r\n",
					"delete_count = 0\r\n",
					"\r\n",
					"@logging_to_appins\r\n",
					"def process_listed_building():\r\n",
					"    \"\"\"\r\n",
					"    Process listed building data from standardised to harmonised layer\r\n",
					"    \"\"\"\r\n",
					"    global insert_count, update_count, delete_count\r\n",
					"    \r\n",
					"    try:\r\n",
					"        spark = SparkSession.builder.getOrCreate()\r\n",
					"        spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY;\"\"\")\r\n",
					"        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\r\n",
					"        \r\n",
					"        # Initialize result dictionary with only required fields\r\n",
					"        result = {\r\n",
					"            \"status\": \"success\",\r\n",
					"            \"record_count\": 0,\r\n",
					"            \"error_message\": None\r\n",
					"        }\r\n",
					"        \r\n",
					"        logInfo(\"Starting listed building data processing from standardised to harmonised\")\r\n",
					"        \r\n",
					"        # Get storage account\r\n",
					"        storage_account = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\r\n",
					"        standardised_container = f\"abfss://odw-standardised@{storage_account}\"\r\n",
					"        harmonised_container = f\"abfss://odw-harmonised@{storage_account}\"\r\n",
					"        \r\n",
					"        # Define table names and paths\r\n",
					"        source_table = \"odw_standardised_db.listed_building\"\r\n",
					"        target_table = \"odw_harmonised_db.listed_building\"\r\n",
					"        target_path = f\"{harmonised_container}/listed_building\"\r\n",
					"        \r\n",
					"        logInfo(f\"Source table: {source_table}\")\r\n",
					"        logInfo(f\"Target table: {target_table}\")\r\n",
					"        logInfo(f\"Target path: {target_path}\")\r\n",
					"        \r\n",
					"        # Check if source table exists\r\n",
					"        if not spark._jsparkSession.catalog().tableExists('odw_standardised_db', 'listed_building'):\r\n",
					"            error_msg = \"Source table 'odw_standardised_db.listed_building' does not exist\"\r\n",
					"            logError(error_msg)\r\n",
					"            result[\"status\"] = \"failed\"\r\n",
					"            result[\"error_message\"] = error_msg\r\n",
					"            result[\"record_count\"] = -1\r\n",
					"            \r\n",
					"            # Log failure to app insights\r\n",
					"            end_exec_time = str(datetime.now())\r\n",
					"            app_insight_logger.add_table_result(\r\n",
					"                delta_table_name=target_table,\r\n",
					"                insert_count=insert_count,\r\n",
					"                update_count=update_count,\r\n",
					"                delete_count=delete_count,\r\n",
					"                table_result=\"failed\",\r\n",
					"                start_exec_time=start_exec_time,\r\n",
					"                end_exec_time=end_exec_time,\r\n",
					"                total_exec_time=\"\",\r\n",
					"                error_message=error_msg\r\n",
					"            )\r\n",
					"            \r\n",
					"            mssparkutils.notebook.exit(json.dumps(result))\r\n",
					"            return\r\n",
					"        \r\n",
					"        # Get initial row count from source\r\n",
					"        source_count = spark.sql(f\"SELECT COUNT(*) as count FROM {source_table}\").collect()[0]['count']\r\n",
					"        logInfo(f\"Source table contains {source_count} records\")\r\n",
					"        \r\n",
					"        # Check if target table exists and perform appropriate data selection\r\n",
					"        if spark._jsparkSession.catalog().tableExists('odw_harmonised_db', 'listed_building'):\r\n",
					"            logInfo(\"Target table exists - performing incremental load with change detection\")\r\n",
					"            \r\n",
					"            # Get the current target table count\r\n",
					"            target_count_before = spark.sql(f\"SELECT COUNT(*) as count FROM {target_table}\").collect()[0]['count']\r\n",
					"            logInfo(f\"Target table currently contains {target_count_before} records\")\r\n",
					"            \r\n",
					"            # Use hash-based change detection for better performance\r\n",
					"            df = spark.sql(\"\"\"\r\n",
					"            SELECT\r\n",
					"                Listings.dataset\r\n",
					"                ,Listings.`end-date` AS endDate\r\n",
					"                ,Listings.entity AS entity\r\n",
					"                ,Listings.`entry-date` AS entryDate\r\n",
					"                ,Listings.geometry\r\n",
					"                ,Listings.`listed-building-grade` AS listedBuildingGrade\r\n",
					"                ,Listings.name\r\n",
					"                ,Listings.`organisation-entity` AS organisationEntity\r\n",
					"                ,Listings.point\r\n",
					"                ,Listings.prefix\r\n",
					"                ,Listings.reference\r\n",
					"                ,Listings.`start-date` AS startDate\r\n",
					"                ,Listings.typology\r\n",
					"                ,Listings.`documentation-url` AS documentationUrl\r\n",
					"                ,NOW() AS dateReceived\r\n",
					"            FROM (\r\n",
					"                SELECT \r\n",
					"                    *,\r\n",
					"                    MD5(CONCAT(\r\n",
					"                        COALESCE(dataset, ''),\r\n",
					"                        COALESCE(`end-date`, ''),\r\n",
					"                        COALESCE(`entry-date`, ''),\r\n",
					"                        COALESCE(geometry, ''),\r\n",
					"                        COALESCE(`listed-building-grade`, ''),\r\n",
					"                        COALESCE(name, ''),\r\n",
					"                        COALESCE(`organisation-entity`, ''),\r\n",
					"                        COALESCE(point, ''),\r\n",
					"                        COALESCE(prefix, ''),\r\n",
					"                        COALESCE(reference, ''),\r\n",
					"                        COALESCE(`start-date`, ''),\r\n",
					"                        COALESCE(typology, ''),\r\n",
					"                        COALESCE(`documentation-url`, '')\r\n",
					"                    )) AS source_hash\r\n",
					"                FROM odw_standardised_db.listed_building\r\n",
					"            ) Listings\r\n",
					"            LEFT JOIN (\r\n",
					"                SELECT \r\n",
					"                    entity,\r\n",
					"                    MD5(CONCAT(\r\n",
					"                        COALESCE(dataset, ''),\r\n",
					"                        COALESCE(endDate, ''),\r\n",
					"                        COALESCE(entryDate, ''),\r\n",
					"                        COALESCE(geometry, ''),\r\n",
					"                        COALESCE(listedBuildingGrade, ''),\r\n",
					"                        COALESCE(name, ''),\r\n",
					"                        COALESCE(organisationEntity, ''),\r\n",
					"                        COALESCE(point, ''),\r\n",
					"                        COALESCE(prefix, ''),\r\n",
					"                        COALESCE(reference, ''),\r\n",
					"                        COALESCE(startDate, ''),\r\n",
					"                        COALESCE(typology, ''),\r\n",
					"                        COALESCE(documentationUrl, '')\r\n",
					"                    )) AS target_hash\r\n",
					"                FROM (\r\n",
					"                    SELECT\r\n",
					"                        entity,\r\n",
					"                        dataset,\r\n",
					"                        endDate,\r\n",
					"                        entryDate,\r\n",
					"                        geometry,\r\n",
					"                        listedBuildingGrade,\r\n",
					"                        name,\r\n",
					"                        organisationEntity,\r\n",
					"                        point,\r\n",
					"                        prefix,\r\n",
					"                        reference,\r\n",
					"                        startDate,\r\n",
					"                        typology,\r\n",
					"                        documentationUrl,\r\n",
					"                        ROW_NUMBER() OVER (PARTITION BY entity ORDER BY dateReceived DESC) AS rn\r\n",
					"                    FROM odw_harmonised_db.listed_building\r\n",
					"                ) t WHERE t.rn = 1\r\n",
					"            ) Target ON Listings.entity = Target.entity\r\n",
					"            WHERE Target.entity IS NULL OR Listings.source_hash != Target.target_hash\r\n",
					"            \"\"\")\r\n",
					"        else:\r\n",
					"            logInfo(\"Target table does not exist - performing full load from source\")\r\n",
					"            df = spark.sql(\"\"\"\r\n",
					"            SELECT\r\n",
					"                    Listings.dataset\r\n",
					"                    ,Listings.`end-date` AS endDate\r\n",
					"                    ,Listings.entity AS entity\r\n",
					"                    ,Listings.`entry-date` AS entryDate\r\n",
					"                    ,Listings.geometry\r\n",
					"                    ,Listings.`listed-building-grade` AS listedBuildingGrade\r\n",
					"                    ,Listings.name\r\n",
					"                    ,Listings.`organisation-entity` AS organisationEntity\r\n",
					"                    ,Listings.point\r\n",
					"                    ,Listings.prefix\r\n",
					"                    ,Listings.reference\r\n",
					"                    ,Listings.`start-date` AS startDate\r\n",
					"                    ,Listings.typology\r\n",
					"                    ,Listings.`documentation-url` AS documentationUrl\r\n",
					"                    ,NOW() AS dateReceived\r\n",
					"                FROM\r\n",
					"                    odw_standardised_db.listed_building as Listings\"\"\")\r\n",
					"        \r\n",
					"        # Get record count before writing\r\n",
					"        record_count = df.count()\r\n",
					"        logInfo(f\"Processed {record_count} listed building records for write operation\")\r\n",
					"        \r\n",
					"        # Check if we have any data to process\r\n",
					"        if record_count == 0:\r\n",
					"            logInfo(\"No data to process - source table is empty or no changes detected\")\r\n",
					"            result[\"record_count\"] = 0\r\n",
					"            \r\n",
					"            # Log success for no-change scenario\r\n",
					"            end_exec_time = str(datetime.now())\r\n",
					"            app_insight_logger.add_table_result(\r\n",
					"                delta_table_name=target_table,\r\n",
					"                insert_count=0,\r\n",
					"                update_count=0,\r\n",
					"                delete_count=0,\r\n",
					"                table_result=\"success\",\r\n",
					"                start_exec_time=start_exec_time,\r\n",
					"                end_exec_time=end_exec_time,\r\n",
					"                total_exec_time=\"\",\r\n",
					"                error_message=\"\"\r\n",
					"            )\r\n",
					"        else:\r\n",
					"            # Write the DataFrame to Delta table using merge operation\r\n",
					"            logInfo(\"Writing data to listed_building table using merge operation\")\r\n",
					"            \r\n",
					"            # Create temporary view for the source data\r\n",
					"            df.createOrReplaceTempView(\"source_listed_building\")\r\n",
					"            \r\n",
					"            # Check if target table exists\r\n",
					"            if spark._jsparkSession.catalog().tableExists('odw_harmonised_db', 'listed_building'):\r\n",
					"                # Use Delta merge for incremental updates\r\n",
					"                spark.sql(f\"\"\"\r\n",
					"                MERGE INTO {target_table} AS target\r\n",
					"                USING source_listed_building AS source\r\n",
					"                ON target.entity = source.entity\r\n",
					"                WHEN MATCHED THEN\r\n",
					"                    UPDATE SET\r\n",
					"                        dataset = source.dataset,\r\n",
					"                        endDate = source.endDate,\r\n",
					"                        entryDate = source.entryDate,\r\n",
					"                        geometry = source.geometry,\r\n",
					"                        listedBuildingGrade = source.listedBuildingGrade,\r\n",
					"                        name = source.name,\r\n",
					"                        organisationEntity = source.organisationEntity,\r\n",
					"                        point = source.point,\r\n",
					"                        prefix = source.prefix,\r\n",
					"                        reference = source.reference,\r\n",
					"                        startDate = source.startDate,\r\n",
					"                        typology = source.typology,\r\n",
					"                        documentationUrl = source.documentationUrl,\r\n",
					"                        dateReceived = source.dateReceived\r\n",
					"                WHEN NOT MATCHED THEN\r\n",
					"                    INSERT (\r\n",
					"                        dataset, endDate, entity, entryDate, geometry, \r\n",
					"                        listedBuildingGrade, name, organisationEntity, point, \r\n",
					"                        prefix, reference, startDate, typology, \r\n",
					"                        documentationUrl, dateReceived\r\n",
					"                    ) VALUES (\r\n",
					"                        source.dataset, source.endDate, source.entity, \r\n",
					"                        source.entryDate, source.geometry, source.listedBuildingGrade, \r\n",
					"                        source.name, source.organisationEntity, source.point, \r\n",
					"                        source.prefix, source.reference, source.startDate, \r\n",
					"                        source.typology, source.documentationUrl, source.dateReceived\r\n",
					"                    )\r\n",
					"                \"\"\")\r\n",
					"            else:\r\n",
					"                # First time load - create table with overwrite\r\n",
					"                df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(target_table)\r\n",
					"            \r\n",
					"            logInfo(\"Successfully written data to listed_building table\")\r\n",
					"            \r\n",
					"            # Verify the write was successful\r\n",
					"            loaded_count = spark.sql(f\"SELECT COUNT(*) as count FROM {target_table}\").collect()[0]['count']\r\n",
					"            result[\"record_count\"] = loaded_count\r\n",
					"            insert_count = loaded_count  # For app insights logging\r\n",
					"            \r\n",
					"            logInfo(f\"Successfully loaded {loaded_count} listed building records to target table\")\r\n",
					"            \r\n",
					"            # Log success to app insights BEFORE optional optimization steps\r\n",
					"            end_exec_time = str(datetime.now())\r\n",
					"            app_insight_logger.add_table_result(\r\n",
					"                delta_table_name=target_table,\r\n",
					"                insert_count=insert_count,\r\n",
					"                update_count=update_count,\r\n",
					"                delete_count=delete_count,\r\n",
					"                table_result=\"success\",\r\n",
					"                start_exec_time=start_exec_time,\r\n",
					"                end_exec_time=end_exec_time,\r\n",
					"                total_exec_time=\"\",\r\n",
					"                error_message=\"\"\r\n",
					"            )\r\n",
					"            \r\n",
					"            # Optional: Optimize the Delta table for better performance\r\n",
					"            try:\r\n",
					"                logInfo(\"Optimizing Delta table for better query performance\")\r\n",
					"                spark.sql(f\"OPTIMIZE {target_table}\")\r\n",
					"                logInfo(\"Delta table optimization completed\")\r\n",
					"            except Exception as opt_e:\r\n",
					"                logInfo(f\"Delta table optimization skipped: {str(opt_e)}\")\r\n",
					"            \r\n",
					"            # Optional: Update table statistics (Delta v2 tables don't support ANALYZE TABLE)\r\n",
					"            try:\r\n",
					"                logInfo(\"Updating table statistics\")\r\n",
					"                # Use Delta-specific statistics command instead\r\n",
					"                spark.sql(f\"DESCRIBE DETAIL {target_table}\")\r\n",
					"                logInfo(\"Table statistics updated\")\r\n",
					"            except Exception as stats_e:\r\n",
					"                logInfo(f\"Table statistics update skipped: {str(stats_e)}\")\r\n",
					"        \r\n",
					"        logInfo(\"Listed building data processing completed successfully\")\r\n",
					"        \r\n",
					"        logInfo(\"Listed building data processing completed successfully\")\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        # Capture error information\r\n",
					"        error_msg = f\"Error during listed building data processing: {str(e)}\"\r\n",
					"        logError(error_msg)\r\n",
					"        logException(e)\r\n",
					"        \r\n",
					"        result[\"status\"] = \"failed\"\r\n",
					"        result[\"error_message\"] = error_msg\r\n",
					"        result[\"record_count\"] = -1  # Indicate failure with -1 count\r\n",
					"        \r\n",
					"        # Log failure to app insights\r\n",
					"        end_exec_time = str(datetime.now())\r\n",
					"        app_insight_logger.add_table_result(\r\n",
					"            delta_table_name=target_table,\r\n",
					"            insert_count=insert_count,\r\n",
					"            update_count=update_count,\r\n",
					"            delete_count=delete_count,\r\n",
					"            table_result=\"failed\",\r\n",
					"            start_exec_time=start_exec_time,\r\n",
					"            end_exec_time=end_exec_time,\r\n",
					"            total_exec_time=\"\",\r\n",
					"            error_message=error_msg\r\n",
					"        )\r\n",
					"        \r\n",
					"        # Re-raise the exception to ensure the notebook fails properly\r\n",
					"        raise e\r\n",
					"    finally:\r\n",
					"        # Always flush logs regardless of success or failure\r\n",
					"        logInfo(\"Flushing logs\")\r\n",
					"        flushLogging()\r\n",
					"        \r\n",
					"        # Output the simplified result as JSON for ADF to capture\r\n",
					"        mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())\r\n",
					"\r\n",
					"# Execute the main function\r\n",
					"if __name__ == \"__main__\":\r\n",
					"    process_listed_building()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}