{
	"name": "py_load_listed_buildings_to_harmonised",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodwpr",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "51741a29-68d3-413f-b0cc-4f10d062d985"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodwpr",
				"name": "pinssynspodwpr",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodwpr",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Listed Buildings ETL - Harmonised Layer\n",
					"\n",
					"## Overview\n",
					"This notebook processes listed building data from the standardised layer to the harmonised layer, implementing proper change data capture (CDC) and versioning using Delta Lake MERGE operations."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Using common logging utils: %run utils/py_utils_common_logging_output"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import IntegerType, ArrayType, StructType, StructField\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql import functions as F\n",
					"import json\n",
					"from datetime import datetime\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import md5, concat, coalesce, lit, col\n",
					"from delta.tables import DeltaTable\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialize Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define variables\n",
					"source_table = \"odw_standardised_db.listed_building\"\n",
					"target_table = \"odw_harmonised_db.listed_building\"\n",
					"spark_table_final = target_table\n",
					"\n",
					"# Initialize tracking variables\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message = \"\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"from datetime import datetime\n",
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(\n",
					"    level=logging.INFO,\n",
					"    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
					"    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
					")\n",
					"\n",
					"# Define logging functions\n",
					"def logInfo(message):\n",
					"    logging.info(message)\n",
					"\n",
					"def logWarning(message):\n",
					"    logging.warning(message)\n",
					"\n",
					"def logError(message):\n",
					"    logging.error(message)\n",
					"    \n",
					"  "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"try:\n",
					"    # Read and transform data from source\n",
					"    logInfo(f\"Reading data from {source_table}\")\n",
					"    \n",
					"    # Check if target table exists to determine if this is initial load or incremental\n",
					"    target_exists = spark._jsparkSession.catalog().tableExists('odw_harmonised_db', 'listed_building')\n",
					"    logInfo(f\"Target table exists? {target_exists}\")\n",
					"    \n",
					"    if target_exists:\n",
					"        logInfo(\"Target table exists - performing incremental processing\")\n",
					"        # Table exists, we'll use MERGE operation\n",
					"        pass\n",
					"    else:\n",
					"        logInfo(\"Target table does not exist - performing initial load\")\n",
					"    \n",
					"    # Read source data and add row numbers for ID generation\n",
					"    source_df = spark.sql(f\"\"\"\n",
					"        SELECT \n",
					"            dataset,\n",
					"            `end-date` AS endDate,\n",
					"            entity,\n",
					"            `entry-date` AS entryDate,\n",
					"            geometry,\n",
					"            `listed-building-grade` AS listedBuildingGrade,\n",
					"            name,\n",
					"            `organisation-entity` AS organisationEntity,\n",
					"            point,\n",
					"            prefix,\n",
					"            reference,\n",
					"            `start-date` AS startDate,\n",
					"            typology,\n",
					"            `documentation-url` AS documentationUrl\n",
					"        FROM {source_table}\n",
					"    \"\"\") \\\n",
					"    .withColumn(\"dateReceived\", F.current_date())\n",
					"    \n",
					"    # Count source rows for visibility\n",
					"    source_row_count = source_df.count()\n",
					"    logInfo(f\"Source rows retrieved: {source_row_count}\")\n",
					"    \n",
					"    # Add rowID calculation    \n",
					"    final_df = source_df.withColumn(\n",
					"        \"rowID\",\n",
					"        md5(concat(\n",
					"            coalesce(col(\"entity\"), lit('.')),\n",
					"            coalesce(col(\"dataset\"), lit('.')),\n",
					"            coalesce(col(\"endDate\"), lit('.')),\n",
					"            coalesce(col(\"entryDate\"), lit('.')),\n",
					"            coalesce(col(\"geometry\"), lit('.')),\n",
					"            coalesce(col(\"listedBuildingGrade\"), lit('.')),\n",
					"            coalesce(col(\"name\"), lit('.')),\n",
					"            coalesce(col(\"organisationEntity\"), lit('.')),\n",
					"            coalesce(col(\"point\"), lit('.')),\n",
					"            coalesce(col(\"prefix\"), lit('.')),\n",
					"            coalesce(col(\"reference\"), lit('.')),\n",
					"            coalesce(col(\"startDate\"), lit('.')),\n",
					"            coalesce(col(\"typology\"), lit('.')),\n",
					"            coalesce(col(\"documentationUrl\"), lit('.'))\n",
					"            ))\n",
					"    ) \\\n",
					"    .withColumn(\"validTo\", lit(None).cast(\"timestamp\")) \\\n",
					"    .withColumn(\"isActive\", lit(\"Y\"))\n",
					"\n",
					"    \n",
					"    logInfo(f\"Processing records for {target_table}\")\n",
					"    \n",
					"    # Write data to target table\n",
					"    if target_exists:\n",
					"        logInfo(\"Merging into existing harmonised table\")\n",
					"        \n",
					"        # Use Delta Lake MERGE INTO\n",
					"        deltaTable = DeltaTable.forName(spark, target_table)\n",
					"\n",
					"        logInfo(\"Starting Delta MERGE operation\")\n",
					"        \n",
					"        # Step 1: Mark old versions as inactive when rowID differs but entity matches\n",
					"        logInfo(\"Step 1: Marking old versions as inactive for changed records\")\n",
					"        (\n",
					"            deltaTable.alias(\"t\")\n",
					"            .merge(\n",
					"                final_df.alias(\"s\"),\n",
					"                \"t.entity = s.entity AND t.isActive = 'Y'\" \n",
					"            )\n",
					"            .whenMatchedUpdate(\n",
					"                condition=\"t.rowID <> s.rowID\",\n",
					"                set={\n",
					"                    \"validTo\": \"current_date()\",\n",
					"                    \"isActive\": \"'N'\"\n",
					"                }\n",
					"            )\n",
					"            .execute()\n",
					"        )\n",
					"        \n",
					"        # Step 2: Insert all records from source (new entities and new versions of existing entities)\n",
					"        # Only records that don't exactly match (same entity AND same rowID) will be inserted\n",
					"        logInfo(\"Step 2: Inserting new and updated records\")\n",
					"        (\n",
					"            deltaTable.alias(\"t\")\n",
					"            .merge(\n",
					"                final_df.alias(\"s\"),\n",
					"                \"t.entity = s.entity AND t.rowID = s.rowID AND t.isActive = 'Y'\" \n",
					"            )\n",
					"            .whenNotMatchedInsert(values={\n",
					"                \"dataset\": \"s.dataset\",\n",
					"                \"endDate\": \"s.endDate\",\n",
					"                \"entity\": \"s.entity\",\n",
					"                \"entryDate\": \"s.entryDate\",\n",
					"                \"geometry\": \"s.geometry\",\n",
					"                \"listedBuildingGrade\": \"s.listedBuildingGrade\",\n",
					"                \"name\": \"s.name\",\n",
					"                \"organisationEntity\": \"s.organisationEntity\",\n",
					"                \"point\": \"s.point\",\n",
					"                \"prefix\": \"s.prefix\",\n",
					"                \"reference\": \"s.reference\",\n",
					"                \"startDate\": \"s.startDate\",\n",
					"                \"typology\": \"s.typology\",\n",
					"                \"documentationUrl\": \"s.documentationUrl\",\n",
					"                \"dateReceived\": \"s.dateReceived\",\n",
					"                \"rowID\": \"s.rowID\",\n",
					"                \"validTo\": \"s.validTo\",\n",
					"                \"isActive\": \"s.isActive\"\n",
					"            })\n",
					"            .execute()\n",
					"        )\n",
					"        logInfo(\"Delta MERGE operation completed\")\n",
					"        \n",
					"    else:\n",
					"        logInfo(\"Creating new harmonised table\")\n",
					"        logInfo(f\"Initial load row count: {final_df.count()}\")\n",
					"        final_df.write.format(\"delta\") \\\n",
					"            .mode(\"overwrite\") \\\n",
					"            .saveAsTable(target_table)\n",
					"    \n",
					"    # Get record count\n",
					"    if target_exists:\n",
					"        # For incremental loads, calculate new and changed rows\n",
					"        latest_df = deltaTable.toDF().filter(\"isActive = 'Y'\")\n",
					"        new_rows = final_df.join(latest_df, \"reference\", \"left_anti\")\n",
					"        changed_rows = final_df.join(\n",
					"            latest_df, \"reference\", \"inner\"\n",
					"        ).filter(final_df.rowID != latest_df.rowID)\n",
					"        insert_count = new_rows.count()\n",
					"        update_count = changed_rows.count()\n",
					"    else:\n",
					"        # For initial loads, all records are inserts\n",
					"        insert_count = final_df.count()\n",
					"        update_count = 0\n",
					"\n",
					"    logInfo(f\"Record count summary - inserts: {insert_count}, updates: {update_count}\")\n",
					"    \n",
					"    end_exec_time = datetime.now()\n",
					"    logInfo(f\"Successfully processed {target_table} - {insert_count} inserts, {update_count} updates\")\n",
					"    \n",
					"    \n",
					"    # Add successful result to logger\n",
					"    duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"    \n",
					"    activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"    stage = \"Success\"\n",
					"    status_message = (\n",
					"        f\"Successfully loaded data into {spark_table_final} table\"        \n",
					"    )\n",
					"    status_code = \"200\"\n",
					"    log_telemetry_and_exit(\n",
					"        stage,\n",
					"        start_exec_time,\n",
					"        end_exec_time,\n",
					"        error_message,\n",
					"        spark_table_final,\n",
					"        insert_count,\n",
					"        update_count,\n",
					"        delete_count,\n",
					"        PipelineName,\n",
					"        PipelineRunID,\n",
					"        PipelineTriggerID,\n",
					"        PipelineTriggerName,\n",
					"        PipelineTriggerType,\n",
					"        PipelineTriggeredbyPipelineName,\n",
					"        PipelineTriggeredbyPipelineRunID,\n",
					"        activity_type,\n",
					"        duration_seconds,\n",
					"        status_message,\n",
					"        status_code\n",
					"    )\n",
					"    \n",
					"except Exception as e:\n",
					"    # Handle errors with proper logging\n",
					"    logError(f\"Error processing {spark_table_final}: {e}\")\n",
					"    error_message = f\"Error processing {spark_table_final}: {str(e)[:800]}\"\n",
					"    \n",
					"    end_exec_time = datetime.now()\n",
					"    duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"    \n",
					"    activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"    stage = \"Failed\"\n",
					"    status_message = (\n",
					"        f\"Failed to load data from {spark_table_final} table\"\n",
					"    )\n",
					"\n",
					"    status_code = \"500\"    \n",
					"    log_telemetry_and_exit(\n",
					"        stage,\n",
					"        start_exec_time,\n",
					"        end_exec_time,\n",
					"        error_message,\n",
					"        f\"odw_standardised_db.{spark_table_final}\",\n",
					"        insert_count,\n",
					"        update_count,\n",
					"        delete_count,\n",
					"        PipelineName,\n",
					"        PipelineRunID,\n",
					"        PipelineTriggerID,\n",
					"        PipelineTriggerName,\n",
					"        PipelineTriggerType,\n",
					"        PipelineTriggeredbyPipelineName,\n",
					"        PipelineTriggeredbyPipelineRunID,\n",
					"        activity_type,\n",
					"        duration_seconds,\n",
					"        status_message,\n",
					"        status_code\n",
					"    )\n",
					"    \n",
					"    "
				],
				"execution_count": null
			}
		]
	}
}