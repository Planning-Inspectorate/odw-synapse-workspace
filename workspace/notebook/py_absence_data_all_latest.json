{
	"name": "py_absence_data_all_latest",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4b1de527-0e29-4538-a356-1b231eb5b80e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of absence data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that absence data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intializations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from datetime import datetime, timedelta\n",
					"import json\n",
					"import re\n",
					"from pyspark.sql.functions import col\n",
					"from pyspark.sql.types import StringType\n",
					"\n",
					""
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": 27
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Absence Incremental load"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"inserted_count\": 0,\n",
					"    \"updated_count\": 0,\n",
					"    \"deleted_count\": 0,\n",
					"    \"cancelled_deleted_count\": 0,\n",
					"    \"deactivated_by_cancellation_count\": 0,  # NEW: Track records deactivated due to status change\n",
					"    \"skipped_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"# Enhanced UUID extraction function (handles NULL AbsType)\n",
					"def extract_uuid_from_abstype(abs_type):\n",
					"    \"\"\"\n",
					"    Extract UUID from AbsType field using simplified logic\n",
					"    Enhanced to handle NULL AbsType\n",
					"    The UUID is the unique identifier that remains constant even when status changes\n",
					"    Both have the same UUID: 57ca0df273f342e6a2569f61a00f685b\n",
					"    \"\"\"\n",
					"    if abs_type is None or (isinstance(abs_type, str) and abs_type.strip() == ''):\n",
					"        # Generate synthetic UUID for NULL AbsType\n",
					"        import hashlib\n",
					"        return f\"NULL_ABSTYPE_{hashlib.md5('NULL_RECORD'.encode()).hexdigest()}\"\n",
					"    \n",
					"    # Pattern 1: Complex timestamp pattern - extract UUID after the timestamp\n",
					"    pattern1 = r'^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}[.][0-9]+-(.+)$'\n",
					"    match1 = re.match(pattern1, abs_type)\n",
					"    if match1:\n",
					"        return match1.group(1)\n",
					"    \n",
					"    # Pattern 2: 32-character hex string\n",
					"    pattern2 = r'[a-f0-9]{32}'\n",
					"    match2 = re.search(pattern2, abs_type)\n",
					"    if match2:\n",
					"        return match2.group(0)\n",
					"    \n",
					"    # Pattern 3: Suffix after last dash (4+ characters)\n",
					"    pattern3 = r'.*-([A-Za-z0-9_]{4,})$'\n",
					"    match3 = re.match(pattern3, abs_type)\n",
					"    if match3:\n",
					"        return match3.group(1)\n",
					"    \n",
					"    # Default: return MD5 hash\n",
					"    import hashlib\n",
					"    return hashlib.md5(abs_type.encode()).hexdigest()\n",
					"\n",
					"# Register UDF for use in SQL\n",
					"spark.udf.register(\"extract_uuid\", extract_uuid_from_abstype, StringType())\n",
					"\n",
					"# Set legacy time parser for compatibility\n",
					"logInfo(\"Setting legacy time parser policy\")\n",
					"spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"logInfo(\"Legacy time parser policy set successfully\")"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"logInfo(\"Determining incremental processing watermark\")\n",
					"\n",
					"try:\n",
					"    last_processed_result = spark.sql(\"\"\"\n",
					"        SELECT COALESCE(MAX(CAST(IngestionDate AS DATE)), DATE('1900-01-01')) as last_processed_date\n",
					"        FROM odw_harmonised_db.sap_hr_absence_all\n",
					"        WHERE IsActive = 'Y'\n",
					"    \"\"\").collect()\n",
					"    \n",
					"    last_processed_date = last_processed_result[0]['last_processed_date']\n",
					"    logInfo(f\"Last processed date: {last_processed_date}\")\n",
					"    \n",
					"    from datetime import datetime, timedelta\n",
					"    if last_processed_date.year > 1900:\n",
					"        safe_start_date = last_processed_date - timedelta(days=7)\n",
					"    else:\n",
					"        safe_start_date = datetime(1900, 1, 1).date()\n",
					"    \n",
					"    logInfo(f\"Processing records from: {safe_start_date} (safe start date)\")\n",
					"    \n",
					"except Exception as e:\n",
					"    logInfo(f\"Target table empty or error getting watermark: {e}\")\n",
					"    safe_start_date = datetime(1900, 1, 1).date()"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    logInfo(\"Creating incremental source view with new/changed records only\")\n",
					"    \n",
					"    source_total_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_standardised_db.hr_absence_monthly\").collect()[0]['count']\n",
					"    logInfo(f\"Total source records available: {source_total_count}\")\n",
					"    \n",
					"    # FIX: Changed from StartDate/EndDate filtering to ingested_datetime\n",
					"    # This ensures we capture all newly ingested records regardless of when the absence occurred\n",
					"    spark.sql(f\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW incremental_source AS\n",
					"    SELECT * FROM odw_standardised_db.hr_absence_monthly\n",
					"    WHERE \n",
					"        StaffNumber IS NOT NULL \n",
					"        AND StartDate IS NOT NULL \n",
					"        AND EndDate IS NOT NULL\n",
					"        AND (\n",
					"            \n",
					"            \n",
					"            CAST(ingested_datetime AS DATE) >= DATE('{safe_start_date}')\n",
					"            OR\n",
					"            \n",
					"            AbsType IS NULL\n",
					"        )\n",
					"    \"\"\")\n",
					"    \n",
					"    incremental_source_count = spark.sql(\"SELECT COUNT(*) as count FROM incremental_source\").collect()[0]['count']\n",
					"    logInfo(f\"Incremental source records to process: {incremental_source_count}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error creating incremental source view: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    raise e\n",
					""
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count == 0:\n",
					"        logInfo(\"No new records to process. Exiting early.\")\n",
					"        result[\"skipped_count\"] = source_total_count\n",
					"    else:\n",
					"        logInfo(\"Creating staging view for incremental records\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW staging_absence AS\n",
					"        SELECT  \n",
					"            StaffNumber,\n",
					"            COALESCE(AbsType, '') AS AbsType,\n",
					"            COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"            CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"            CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"            AttendanceorAbsenceType,\n",
					"            ROUND(CAST(REPLACE(Days, ',', '') AS DOUBLE), 2) AS Days,\n",
					"            ROUND(CAST(REPLACE(Hrs, ',', '') AS DOUBLE), 2) AS Hrs,\n",
					"            TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"            TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"            Caldays,\n",
					"            WorkScheduleRule,\n",
					"            ROUND(TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE), 2) AS Wkhrs,  \n",
					"            ROUND(TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE), 2) AS HrsDay,  \n",
					"            TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"            TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"            'saphr' AS SourceSystemID,\n",
					"            CURRENT_DATE() AS IngestionDate,\n",
					"            CURRENT_TIMESTAMP() AS ValidTo,\n",
					"            \n",
					"            -- Approval status extraction\n",
					"            CASE \n",
					"                WHEN AbsType IS NULL OR TRIM(AbsType) = '' THEN 'NULL_ABSTYPE'\n",
					"                WHEN AbsType LIKE '%-%' THEN UPPER(TRIM(SPLIT(AbsType, '-')[0]))\n",
					"                ELSE 'UNKNOWN'\n",
					"            END AS ApprovalStatus,\n",
					"            \n",
					"            -- RecordUUID: The consistent identifier across status changes\n",
					"            -- This is KEY for matching CANCELLED records to their original APPROVED records\n",
					"            CASE\n",
					"                WHEN AbsType IS NULL OR TRIM(AbsType) = '' THEN\n",
					"                    CONCAT('NULL_', MD5(CONCAT_WS('_', StaffNumber, TO_DATE(StartDate,'dd/MM/yyyy'), TO_DATE(EndDate,'dd/MM/yyyy'))))\n",
					"                ELSE\n",
					"                    extract_uuid(AbsType)\n",
					"            END AS RecordUUID,\n",
					"            \n",
					"            -- Last modified date from AbsType timestamp\n",
					"            CASE \n",
					"                WHEN AbsType IS NULL OR TRIM(AbsType) = '' THEN CURRENT_TIMESTAMP()\n",
					"                WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN\n",
					"                    TRY_CAST(\n",
					"                        REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                        AS TIMESTAMP\n",
					"                    )\n",
					"                ELSE CURRENT_TIMESTAMP()\n",
					"            END AS LastModifiedDate,\n",
					"            \n",
					"            -- RowID: Hash of all fields (changes when any field changes including status)\n",
					"            MD5(CONCAT_WS('|',\n",
					"                StaffNumber,            \n",
					"                COALESCE(AbsType, 'NULL_ABSTYPE'),                \n",
					"                COALESCE(SicknessGroup, ''),          \n",
					"                TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"                TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"                AttendanceorAbsenceType,\n",
					"                REPLACE(Days, ',', ''),                   \n",
					"                REPLACE(Hrs, ',', ''),                    \n",
					"                Caldays,                \n",
					"                WorkScheduleRule,       \n",
					"                REPLACE(Wkhrs, ',', ''),                  \n",
					"                REPLACE(HrsDay, ',', ''),                 \n",
					"                REPLACE(WkDys, ',', '')\n",
					"            )) AS RowID,\n",
					"            'Y' AS IsActive\n",
					"            \n",
					"        FROM incremental_source\n",
					"        \"\"\")\n",
					"        \n",
					"        logInfo(\"Staging view created successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error creating staging view: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    spark.sql(\"DROP VIEW IF EXISTS incremental_source\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"    raise e"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0:\n",
					"        logInfo(\"Creating deduplicated incremental records\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW latest_staging_records AS\n",
					"        SELECT *\n",
					"        FROM (\n",
					"            SELECT *,\n",
					"                ROW_NUMBER() OVER (\n",
					"                    PARTITION BY RecordUUID \n",
					"                    ORDER BY LastModifiedDate DESC, IngestionDate DESC\n",
					"                ) AS rn\n",
					"            FROM staging_absence\n",
					"            WHERE RecordUUID IS NOT NULL\n",
					"        ) ranked\n",
					"        WHERE rn = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        staging_count = spark.sql(\"SELECT COUNT(*) as count FROM latest_staging_records\").collect()[0]['count']\n",
					"        logInfo(f\"Deduplicated staging records: {staging_count}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error deduplicating staging records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    spark.sql(\"DROP VIEW IF EXISTS incremental_source\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"    raise e"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0:\n",
					"        logInfo(\"=== NEW STEP: Deactivating existing records that have been CANCELLED/REJECTED in source ===\")\n",
					"        \n",
					"        # Find CANCELLED/REJECTED records in staging\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW cancelled_records_with_uuid AS\n",
					"        SELECT \n",
					"            RecordUUID,\n",
					"            AbsType,\n",
					"            ApprovalStatus,\n",
					"            StaffNumber,\n",
					"            StartDate,\n",
					"            EndDate\n",
					"        FROM latest_staging_records\n",
					"        WHERE UPPER(ApprovalStatus) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"        \"\"\")\n",
					"        \n",
					"        cancelled_uuids_count = spark.sql(\"SELECT COUNT(*) as count FROM cancelled_records_with_uuid\").collect()[0]['count']\n",
					"        logInfo(f\"Found {cancelled_uuids_count} CANCELLED/REJECTED records in source\")\n",
					"        \n",
					"        if cancelled_uuids_count > 0:\n",
					"            # FIX: Match by RecordUUID to find existing APPROVED records that need deactivation\n",
					"            # This handles the case where status changes from APPROVED to CANCELLED\n",
					"            spark.sql(\"\"\"\n",
					"            CREATE OR REPLACE TEMPORARY VIEW existing_records_to_deactivate AS\n",
					"            SELECT DISTINCT t.RowID, t.AbsType as existing_abstype, c.AbsType as new_abstype, c.RecordUUID\n",
					"            FROM odw_harmonised_db.sap_hr_absence_all t\n",
					"            INNER JOIN cancelled_records_with_uuid c \n",
					"                ON t.AbsType LIKE CONCAT('%', c.RecordUUID, '%')\n",
					"            WHERE t.IsActive = 'Y'\n",
					"            \"\"\")\n",
					"            \n",
					"            records_to_deactivate_count = spark.sql(\"SELECT COUNT(*) as count FROM existing_records_to_deactivate\").collect()[0]['count']\n",
					"            logInfo(f\"Found {records_to_deactivate_count} existing active records to deactivate (status changed to CANCELLED/REJECTED)\")\n",
					"            \n",
					"            if records_to_deactivate_count > 0:\n",
					"                # Deactivate existing records that have been cancelled\n",
					"                spark.sql(\"\"\"\n",
					"                MERGE INTO odw_harmonised_db.sap_hr_absence_all AS target\n",
					"                USING existing_records_to_deactivate AS source\n",
					"                ON target.RowID = source.RowID\n",
					"                WHEN MATCHED AND target.IsActive = 'Y' THEN \n",
					"                    UPDATE SET \n",
					"                        IsActive = 'N', \n",
					"                        ValidTo = CURRENT_TIMESTAMP()\n",
					"                \"\"\")\n",
					"                \n",
					"                logInfo(f\"Successfully deactivated {records_to_deactivate_count} records due to CANCELLED/REJECTED status change\")\n",
					"                result[\"deactivated_by_cancellation_count\"] = records_to_deactivate_count\n",
					"            \n",
					"            spark.sql(\"DROP VIEW IF EXISTS existing_records_to_deactivate\")\n",
					"        \n",
					"        spark.sql(\"DROP VIEW IF EXISTS cancelled_records_with_uuid\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in cancellation deactivation step: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    # Clean up but don't fail - continue with rest of processing\n",
					"    spark.sql(\"DROP VIEW IF EXISTS cancelled_records_with_uuid\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS existing_records_to_deactivate\")\n",
					"    logInfo(\"Continuing despite error in cancellation step...\")"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0:\n",
					"        logInfo(\"Filtering out cancelled/rejected records from staging (these should not be inserted)\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW final_staging_records AS\n",
					"        SELECT * FROM latest_staging_records\n",
					"        WHERE UPPER(ApprovalStatus) NOT IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"        \"\"\")\n",
					"        \n",
					"        final_staging_count = spark.sql(\"SELECT COUNT(*) as count FROM final_staging_records\").collect()[0]['count']\n",
					"        cancelled_in_staging = staging_count - final_staging_count\n",
					"        \n",
					"        logInfo(f\"Final staging records (after filtering): {final_staging_count}\")\n",
					"        logInfo(f\"Cancelled/rejected records filtered out: {cancelled_in_staging}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error filtering cancelled records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    raise e"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0:\n",
					"        logInfo(\"Identifying records for insert vs update operations\")\n",
					"        \n",
					"        # Records to insert (new RowIDs)\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_insert AS\n",
					"        SELECT s.* FROM final_staging_records s\n",
					"        LEFT JOIN odw_harmonised_db.sap_hr_absence_all t ON s.RowID = t.RowID\n",
					"        WHERE t.RowID IS NULL\n",
					"        \"\"\")\n",
					"        \n",
					"        # Records to update (existing RowIDs with newer data)\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_update AS\n",
					"        SELECT s.* FROM final_staging_records s\n",
					"        INNER JOIN odw_harmonised_db.sap_hr_absence_all t ON s.RowID = t.RowID\n",
					"        WHERE s.LastModifiedDate > t.ValidTo OR t.IsActive = 'N'\n",
					"        \"\"\")\n",
					"        \n",
					"        insert_count = spark.sql(\"SELECT COUNT(*) as count FROM records_to_insert\").collect()[0]['count']\n",
					"        update_count = spark.sql(\"SELECT COUNT(*) as count FROM records_to_update\").collect()[0]['count']\n",
					"        \n",
					"        logInfo(f\"Records to insert: {insert_count}\")\n",
					"        logInfo(f\"Records to update: {update_count}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error identifying insert/update records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    raise e"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0 and update_count > 0:\n",
					"        logInfo(f\"Updating {update_count} existing records\")\n",
					"        spark.sql(\"\"\"\n",
					"        MERGE INTO odw_harmonised_db.sap_hr_absence_all AS target\n",
					"        USING records_to_update AS source\n",
					"        ON target.RowID = source.RowID\n",
					"        WHEN MATCHED THEN UPDATE SET\n",
					"            StaffNumber = source.StaffNumber,\n",
					"            AbsType = source.AbsType,\n",
					"            SicknessGroup = source.SicknessGroup,\n",
					"            StartDate = source.StartDate,\n",
					"            EndDate = source.EndDate,\n",
					"            AttendanceorAbsenceType = source.AttendanceorAbsenceType,\n",
					"            Days = source.Days,\n",
					"            Hrs = source.Hrs,\n",
					"            Start = source.Start,\n",
					"            Endtime = source.Endtime,\n",
					"            Caldays = source.Caldays,\n",
					"            WorkScheduleRule = source.WorkScheduleRule,\n",
					"            Wkhrs = source.Wkhrs,\n",
					"            HrsDay = source.HrsDay,\n",
					"            WkDys = source.WkDys,\n",
					"            AnnualLeaveStart = source.AnnualLeaveStart,\n",
					"            SourceSystemID = source.SourceSystemID,\n",
					"            IngestionDate = source.IngestionDate,\n",
					"            ValidTo = source.ValidTo,\n",
					"            IsActive = source.IsActive\n",
					"        \"\"\")\n",
					"        logInfo(f\"Successfully updated {update_count} records\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error updating records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    raise e"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0 and insert_count > 0:\n",
					"        logInfo(f\"Inserting {insert_count} new records\")\n",
					"        spark.sql(\"\"\"\n",
					"        INSERT INTO odw_harmonised_db.sap_hr_absence_all (\n",
					"            StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"            AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"            Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"            AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"            RowID, IsActive\n",
					"        )\n",
					"        SELECT \n",
					"            StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"            AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"            Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"            AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"            RowID, IsActive\n",
					"        FROM records_to_insert\n",
					"        \"\"\")\n",
					"        logInfo(f\"Successfully inserted {insert_count} records\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error inserting records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    raise e"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count == 0:\n",
					"        logInfo(\"No incremental processing needed - skipping merge operations\")\n",
					"        final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all WHERE IsActive = 'Y'\").collect()[0]['count']\n",
					"        result[\"record_count\"] = final_record_count\n",
					"        result[\"skipped_count\"] = source_total_count\n",
					"    else:\n",
					"        # Final counts and reporting\n",
					"        final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all WHERE IsActive = 'Y'\").collect()[0]['count']\n",
					"        total_processed = insert_count + update_count\n",
					"        \n",
					"        result[\"record_count\"] = final_record_count\n",
					"        result[\"inserted_count\"] = insert_count\n",
					"        result[\"updated_count\"] = update_count\n",
					"        result[\"deleted_count\"] = 0  # We mark as inactive, not delete\n",
					"        result[\"cancelled_deleted_count\"] = cancelled_in_staging\n",
					"        \n",
					"        logInfo(\"=\" * 60)\n",
					"        logInfo(\"Incremental load completed successfully:\")\n",
					"        logInfo(f\"- Total source records available: {source_total_count}\")\n",
					"        logInfo(f\"- Incremental records processed: {incremental_source_count}\")\n",
					"        logInfo(f\"- New records inserted: {insert_count}\")\n",
					"        logInfo(f\"- Existing records updated: {update_count}\")\n",
					"        logInfo(f\"- Records deactivated (status changed to CANCELLED): {result.get('deactivated_by_cancellation_count', 0)}\")\n",
					"        logInfo(f\"- CANCELLED/REJECTED records filtered (not inserted): {cancelled_in_staging}\")\n",
					"        logInfo(f\"- Total active records in target: {final_record_count}\")\n",
					"        logInfo(f\"- Records skipped (no changes): {source_total_count - incremental_source_count}\")\n",
					"        logInfo(\"=\" * 60)\n",
					"    \n",
					"    # Clean up temporary views\n",
					"    spark.sql(\"DROP VIEW IF EXISTS incremental_source\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_staging_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS records_to_insert\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS records_to_update\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS cancelled_records_with_uuid\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS existing_records_to_deactivate\")\n",
					"    \n",
					"    logInfo(\"Successfully completed incremental load process\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in final processing phase: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1\n",
					"    result[\"inserted_count\"] = -1\n",
					"    result[\"updated_count\"] = -1\n",
					"    result[\"deleted_count\"] = -1\n",
					"    result[\"cancelled_deleted_count\"] = -1\n",
					"    \n",
					"    # Clean up on error\n",
					"    try:\n",
					"        spark.sql(\"DROP VIEW IF EXISTS incremental_source\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_insert\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_update\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS cancelled_records_with_uuid\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS existing_records_to_deactivate\")\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"\n",
					"WITH CancelledInSource AS (\n",
					"\n",
					"-- Get UUIDs of all CANCELLED records from SOURCE\n",
					"\n",
					"SELECT\n",
					"\n",
					"StaffNumber,\n",
					"\n",
					"AbsType as Source_AbsType,\n",
					"\n",
					"UPPER(TRIM(SPLIT(AbsType, '-')[0])) as Source_Status,\n",
					"\n",
					"REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS RecordUUID\n",
					"\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"\n",
					"WHERE AbsType IS NOT NULL\n",
					"\n",
					"AND AbsType LIKE '%-%'\n",
					"\n",
					"AND UPPER(TRIM(SPLIT(AbsType, '-')[0])) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"\n",
					")\n",
					"\n",
					"SELECT\n",
					"\n",
					"h.StaffNumber,\n",
					"\n",
					"h.StartDate,\n",
					"\n",
					"h.EndDate,\n",
					"\n",
					"h.AbsType as Harmonised_AbsType_APPROVED,\n",
					"\n",
					"c.Source_AbsType as Source_AbsType_CANCELLED,\n",
					"\n",
					"c.RecordUUID,\n",
					"\n",
					"h.IsActive,\n",
					"\n",
					"'DELETE THIS - Approved in Harmonised but Cancelled in Source' as Action_Required\n",
					"\n",
					"FROM odw_harmonised_db.sap_hr_absence_all h\n",
					"\n",
					"INNER JOIN CancelledInSource c\n",
					"\n",
					"ON h.AbsType LIKE CONCAT('%', c.RecordUUID, '%')\n",
					"\n",
					"WHERE h.IsActive = 'Y'\n",
					"\n",
					"ORDER BY h.StartDate;"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"\n",
					"\n",
					"-- Count: How many APPROVED records in Harmonised have been CANCELLED in Source?\n",
					"\n",
					"WITH CancelledInSource AS (\n",
					"\n",
					"SELECT DISTINCT\n",
					"\n",
					"REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS RecordUUID\n",
					"\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"\n",
					"WHERE AbsType IS NOT NULL\n",
					"\n",
					"AND AbsType LIKE '%-%'\n",
					"\n",
					"AND UPPER(TRIM(SPLIT(AbsType, '-')[0])) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"\n",
					")\n",
					"\n",
					"SELECT\n",
					"\n",
					"COUNT(*) as records_to_delete,\n",
					"\n",
					"'APPROVED records in Harmonised where Source shows CANCELLED' as description\n",
					"\n",
					"FROM odw_harmonised_db.sap_hr_absence_all h\n",
					"\n",
					"INNER JOIN CancelledInSource c\n",
					"\n",
					"ON h.AbsType LIKE CONCAT('%', c.RecordUUID, '%')\n",
					"\n",
					"WHERE h.IsActive = 'Y';"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"\n",
					"\n",
					"-- DELETE APPROVED records from Harmonised where Source shows CANCELLED\n",
					"\n",
					"MERGE INTO odw_harmonised_db.sap_hr_absence_all AS target\n",
					"\n",
					"USING (\n",
					"\n",
					"SELECT DISTINCT\n",
					"\n",
					"REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS RecordUUID\n",
					"\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"\n",
					"WHERE AbsType IS NOT NULL\n",
					"\n",
					"AND AbsType LIKE '%-%'\n",
					"\n",
					"AND UPPER(TRIM(SPLIT(AbsType, '-')[0])) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"\n",
					") AS cancelled_in_source\n",
					"\n",
					"ON target.AbsType LIKE CONCAT('%', cancelled_in_source.RecordUUID, '%')\n",
					"\n",
					"AND target.IsActive = 'Y'\n",
					"\n",
					"WHEN MATCHED THEN DELETE;"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"\n",
					"\n",
					"-- Verify: The specific record should no longer exist (or IsActive='N')\n",
					"\n",
					"SELECT StaffNumber, AbsType, StartDate, EndDate, IsActive\n",
					"\n",
					"FROM odw_harmonised_db.sap_hr_absence_all\n",
					"\n",
					"WHERE AbsType LIKE '%57ca0df273f342e6a2569f61a00f685b%';"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Check if the specific problematic record still exists in harmonised\n",
					"SELECT\n",
					"StaffNumber,\n",
					"AbsType,\n",
					"StartDate,\n",
					"EndDate,\n",
					"IsActive\n",
					"FROM odw_harmonised_db.sap_hr_absence_all\n",
					"WHERE StaffNumber = '50410555'\n",
					"AND StartDate = '2025-04-25';"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Check the CANCELLED record in source\n",
					"SELECT\n",
					"StaffNumber,\n",
					"AbsType,\n",
					"StartDate,\n",
					"EndDate\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE StaffNumber = '50410555'\n",
					"AND AbsType LIKE '%CANCELLED%';"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Test UUID extraction from the CANCELLED record\n",
					"SELECT\n",
					"AbsType,\n",
					"REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS ExtractedUUID\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE StaffNumber = '50410555'\n",
					"AND AbsType LIKE '%CANCELLED%';"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Check the harmonised record and extract its UUID\n",
					"SELECT\n",
					"AbsType,\n",
					"REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS ExtractedUUID\n",
					"FROM odw_harmonised_db.sap_hr_absence_all\n",
					"WHERE StaffNumber = '50410555'\n",
					"AND StartDate = '2025-04-25';"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Direct test: Does the harmonised record contain the UUID from source?\n",
					"SELECT\n",
					"h.StaffNumber,\n",
					"h.AbsType as Harmonised_AbsType,\n",
					"s.AbsType as Source_AbsType,\n",
					"REVERSE(SPLIT(REVERSE(s.AbsType), '-')[0]) AS Source_UUID,\n",
					"CASE\n",
					"WHEN h.AbsType LIKE CONCAT('%', REVERSE(SPLIT(REVERSE(s.AbsType), '-')[0]), '%')\n",
					"THEN 'MATCH'\n",
					"ELSE 'NO MATCH'\n",
					"END as UUID_Match\n",
					"FROM odw_harmonised_db.sap_hr_absence_all h\n",
					"CROSS JOIN odw_standardised_db.hr_absence_monthly s\n",
					"WHERE h.StaffNumber = '50410555'\n",
					"AND h.StartDate = '2025-04-25'\n",
					"AND s.StaffNumber = '50410555'\n",
					"AND s.AbsType LIKE '%CANCELLED%';"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"delete from odw_harmonised_db.sap_hr_absence_all h\n",
					"WHERE  h.StaffNumber = '50410555' and h.AbsType LIKE '%57ca0df273f342e6a2569f61a00f685b%'\n",
					"AND h.IsActive = 'N' "
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Direct check with hardcoded UUID\n",
					"SELECT\n",
					"h.StaffNumber,\n",
					"h.AbsType,\n",
					"h.StartDate,\n",
					"h.EndDate,\n",
					"h.IsActive,\n",
					"'SHOULD BE DELETED' as Action\n",
					"FROM odw_harmonised_db.sap_hr_absence_all h\n",
					"WHERE h.AbsType LIKE '%57ca0df273f342e6a2569f61a00f685b%'\n",
					"AND h.IsActive = 'N';"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Find ALL active APPROVED records in Harmonised that have matching CANCELLED in Source\n",
					"WITH SourceCancelled AS (\n",
					"SELECT DISTINCT\n",
					"REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS RecordUUID\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE AbsType LIKE 'CANCELLED%'\n",
					"),\n",
					"HarmonisedActive AS (\n",
					"SELECT\n",
					"StaffNumber,\n",
					"AbsType,\n",
					"StartDate,\n",
					"EndDate,\n",
					"REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS RecordUUID,\n",
					"IsActive\n",
					"FROM odw_harmonised_db.sap_hr_absence_all\n",
					"WHERE IsActive = 'Y'\n",
					")\n",
					"SELECT\n",
					"h.StaffNumber,\n",
					"h.AbsType as Harmonised_AbsType,\n",
					"h.StartDate,\n",
					"h.EndDate,\n",
					"h.RecordUUID,\n",
					"'SHOULD BE DELETED/DEACTIVATED' as Action\n",
					"FROM HarmonisedActive h\n",
					"INNER JOIN SourceCancelled s ON h.RecordUUID = s.RecordUUID;"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT TOP (100) * FROM [odw_harmonised_db].[dbo].[sap_hr_absence_all] where StaffNumber = '00500728' and StartDate = '2025-09-29'\n",
					" \n",
					"Both records are approved, but that's because the same record had its end date extended.\n",
					" \n",
					"Both records have the same ID, but one has an updated date of 2025-10-06 which is later that the previous record with the same ID. It was also ingested later on 2025-12-11. Therefore, this record should replace the older record. "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Find UUIDs that have multiple ACTIVE records (should only have 1)\n",
					"WITH RecordCounts AS (\n",
					"SELECT\n",
					"REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS RecordUUID,\n",
					"COUNT(*) as record_count\n",
					"FROM odw_harmonised_db.sap_hr_absence_all\n",
					"WHERE IsActive = 'Y'\n",
					"AND AbsType LIKE '%-%'\n",
					"GROUP BY REVERSE(SPLIT(REVERSE(AbsType), '-')[0])\n",
					"HAVING COUNT(*) > 1\n",
					")\n",
					"SELECT\n",
					"h.StaffNumber,\n",
					"h.AbsType,\n",
					"h.StartDate,\n",
					"h.EndDate,\n",
					"h.Days,\n",
					"h.IsActive,\n",
					"REVERSE(SPLIT(REVERSE(h.AbsType), '-')[0]) AS RecordUUID,\n",
					"rc.record_count as total_versions\n",
					"FROM odw_harmonised_db.sap_hr_absence_all h\n",
					"INNER JOIN RecordCounts rc\n",
					"ON REVERSE(SPLIT(REVERSE(h.AbsType), '-')[0]) = rc.RecordUUID\n",
					"WHERE h.IsActive = 'Y'\n",
					"ORDER BY RecordUUID, h.EndDate DESC;"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Count of duplicate active records (same UUID, multiple active versions)\n",
					"WITH RecordCounts AS (\n",
					"SELECT\n",
					"REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS RecordUUID,\n",
					"COUNT(*) as record_count\n",
					"FROM odw_harmonised_db.sap_hr_absence_all\n",
					"WHERE IsActive = 'Y'\n",
					"AND AbsType LIKE '%-%'\n",
					"GROUP BY REVERSE(SPLIT(REVERSE(AbsType), '-')[0])\n",
					"HAVING COUNT(*) > 1\n",
					")\n",
					"SELECT\n",
					"SUM(record_count) as total_duplicate_records,\n",
					"COUNT(*) as unique_uuids_with_duplicates,\n",
					"SUM(record_count) - COUNT(*) as records_to_deactivate\n",
					"FROM RecordCounts;"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"MERGE INTO odw_harmonised_db.sap_hr_absence_all AS target\n",
					"USING (\n",
					"  SELECT \n",
					"    RowID,\n",
					"    REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) AS RecordUUID,\n",
					"    REGEXP_EXTRACT(AbsType, '(\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2})', 1) AS ModifiedTimestamp,\n",
					"    ROW_NUMBER() OVER (\n",
					"      PARTITION BY REVERSE(SPLIT(REVERSE(AbsType), '-')[0])\n",
					"      ORDER BY REGEXP_EXTRACT(AbsType, '(\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2})', 1) DESC\n",
					"    ) as rn\n",
					"  FROM odw_harmonised_db.sap_hr_absence_all\n",
					"  WHERE IsActive = 'Y'\n",
					"    AND AbsType LIKE '%-%'\n",
					") AS source\n",
					"ON target.RowID = source.RowID\n",
					"WHEN MATCHED AND source.rn > 1 THEN DELETE\n",
					""
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Verify: Staff 00500397 should now have only 1 active record\n",
					"SELECT\n",
					"StaffNumber,\n",
					"AbsType,\n",
					"StartDate,\n",
					"EndDate,\n",
					"Days,\n",
					"IsActive\n",
					"FROM odw_harmonised_db.sap_hr_absence_all\n",
					"WHERE StaffNumber = '00500397'\n",
					"AND StartDate = '2025-09-29'\n",
					"ORDER BY EndDate DESC;"
				],
				"execution_count": 45
			}
		]
	}
}