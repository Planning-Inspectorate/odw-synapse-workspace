{
	"name": "synapse-managed-to-external_v2",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "dade2af1-056d-42a0-92ae-bf5135a2a528"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# üîÑ Convert Managed Table ‚Üí External Table\n",
					"---\n",
					"**Fully automatic: auto-detects path, format, partitions.**\n",
					"**Handles both partitioned and non-partitioned tables.**\n",
					"\n",
					"### What's detected automatically:\n",
					"- **Path** ‚Üí from `DESCRIBE EXTENDED`\n",
					"- **Format** ‚Üí Delta or Parquet\n",
					"- **Partition columns** ‚Üí from table metadata\n",
					"- **Table type** ‚Üí skips if already external\n",
					"\n",
					"### Flow per table:\n",
					"1. Auto-detect path, format, partitions\n",
					"2. Read & capture metadata\n",
					"3. Backup (preserving partitions)\n",
					"4. Validate backup\n",
					"5. Drop managed table\n",
					"6. Restore to same path (with partition structure)\n",
					"7. Register as external table\n",
					"8. Final validation"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"harmonised_container = f\"abfss://odw-harmonised@{storage_account}\"\n",
					"print(harmonised_container)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## üìå Cell 1: Configuration"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					},
					"collapsed": false
				},
				"source": [
					"from datetime import datetime\n",
					"\n",
					"# =====================================================\n",
					"# CONFIGURATION\n",
					"# =====================================================\n",
					"\n",
					"# Temp backup base path (timestamped)\n",
					"backup_base = harmonised_container+\"tmp_backup/migration_\" + datetime.now().strftime('%Y%m%d_%H%M%S')\n",
					"\n",
					"# Just pass the fully qualified table names - path & format auto-detected\n",
					"tables_to_migrate = [\n",
					"    # \"odw_harmonised_db.sb_appeal_s78\",\n",
					"    \"odw_harmonised_db.appeal_s78\"\n",
					"    # \"my_db.orders\",\n",
					"    # \"my_db.products\",\n",
					"    # Add more tables...\n",
					"]\n",
					"\n",
					"print(f\"Backup base : {backup_base}\")\n",
					"print(f\"Tables      : {len(tables_to_migrate)}\")\n",
					"for t in tables_to_migrate:\n",
					"    print(f\"  - {t}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## üìå Cell 2: Auto-detect Utilities (Path, Format, Partitions)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					},
					"collapsed": false
				},
				"source": [
					"from notebookutils import mssparkutils\n",
					"\n",
					"def get_table_location(source_table):\n",
					"    \"\"\"Gets the data path from managed table metadata.\"\"\"\n",
					"    desc_df = spark.sql(f\"DESCRIBE EXTENDED {source_table}\")\n",
					"    location_row = desc_df.filter(\"col_name = 'Location'\").collect()\n",
					"    if not location_row:\n",
					"        raise ValueError(f\"Could not find Location for: {source_table}\")\n",
					"    return location_row[0][\"data_type\"].strip()\n",
					"\n",
					"\n",
					"def get_table_type(source_table):\n",
					"    \"\"\"Returns MANAGED or EXTERNAL.\"\"\"\n",
					"    desc_df = spark.sql(f\"DESCRIBE EXTENDED {source_table}\")\n",
					"    type_row = desc_df.filter(\"col_name = 'Type'\").collect()\n",
					"    if type_row:\n",
					"        return type_row[0][\"data_type\"].strip().upper()\n",
					"    return \"UNKNOWN\"\n",
					"\n",
					"\n",
					"def detect_table_format(source_table, data_path):\n",
					"    \"\"\"Detects Delta or Parquet using multiple methods.\"\"\"\n",
					"    try:\n",
					"        desc_df = spark.sql(f\"DESCRIBE EXTENDED {source_table}\")\n",
					"        provider_row = desc_df.filter(\n",
					"            \"col_name = 'Provider' OR col_name = 'Serde Library' OR col_name = 'InputFormat'\"\n",
					"        ).collect()\n",
					"        for row in provider_row:\n",
					"            val = str(row[\"data_type\"]).lower()\n",
					"            if \"delta\" in val:\n",
					"                return \"DELTA\"\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"    try:\n",
					"        files = mssparkutils.fs.ls(data_path)\n",
					"        for f in files:\n",
					"            if f.name == \"_delta_log\" or f.name == \"_delta_log/\":\n",
					"                return \"DELTA\"\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"    try:\n",
					"        spark.read.format(\"delta\").load(data_path).limit(1).collect()\n",
					"        return \"DELTA\"\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"    return \"PARQUET\"\n",
					"\n",
					"\n",
					"def get_partition_columns(source_table):\n",
					"    \"\"\"\n",
					"    Detects partition columns from table metadata.\n",
					"    Returns list of partition column names, empty list if not partitioned.\n",
					"    \"\"\"\n",
					"    partition_cols = []\n",
					"\n",
					"    try:\n",
					"        # Method 1: SHOW PARTITIONS (works for Hive-style partitioned tables)\n",
					"        desc_df = spark.sql(f\"DESCRIBE EXTENDED {source_table}\")\n",
					"        rows = desc_df.collect()\n",
					"\n",
					"        # Find the '# Partition Information' section in DESCRIBE EXTENDED\n",
					"        in_partition_section = False\n",
					"        for row in rows:\n",
					"            col_name = str(row[\"col_name\"]).strip()\n",
					"            data_type = str(row[\"data_type\"]).strip() if row[\"data_type\"] else \"\"\n",
					"\n",
					"            if col_name == \"# Partition Information\":\n",
					"                in_partition_section = True\n",
					"                continue\n",
					"\n",
					"            if in_partition_section:\n",
					"                # End of partition section when we hit another # section or empty\n",
					"                if col_name.startswith(\"#\") or col_name == \"\" or col_name.startswith(\"# Detailed\"):\n",
					"                    break\n",
					"                # Skip the header row \"# col_name\"\n",
					"                if col_name == \"# col_name\":\n",
					"                    continue\n",
					"                if col_name and data_type:\n",
					"                    partition_cols.append(col_name)\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"    # Method 2: For Delta tables, check Delta table detail\n",
					"    if not partition_cols:\n",
					"        try:\n",
					"            detail_df = spark.sql(f\"DESCRIBE DETAIL {source_table}\")\n",
					"            detail_row = detail_df.collect()[0]\n",
					"            part_cols = detail_row[\"partitionColumns\"]\n",
					"            if part_cols:\n",
					"                partition_cols = list(part_cols)\n",
					"        except Exception:\n",
					"            pass\n",
					"\n",
					"    return partition_cols\n",
					"\n",
					"\n",
					"def get_table_metadata(source_table):\n",
					"    \"\"\"\n",
					"    Returns all auto-detected metadata for a table as a dict.\n",
					"    \"\"\"\n",
					"    data_path = get_table_location(source_table)\n",
					"    return {\n",
					"        \"table\": source_table,\n",
					"        \"path\": data_path,\n",
					"        \"type\": get_table_type(source_table),\n",
					"        \"format\": detect_table_format(source_table, data_path),\n",
					"        \"partitions\": get_partition_columns(source_table)\n",
					"    }\n",
					"\n",
					"\n",
					"# =====================================================\n",
					"# Preview: Show detected metadata for all tables\n",
					"# =====================================================\n",
					"print(f\"\\n{'Table':<28} {'Type':<10} {'Format':<10} {'Partitions':<30} {'Location'}\")\n",
					"print(f\"{'-'*120}\")\n",
					"for t in tables_to_migrate:\n",
					"    try:\n",
					"        meta = get_table_metadata(t)\n",
					"        pcols = \", \".join(meta[\"partitions\"]) if meta[\"partitions\"] else \"None\"\n",
					"        print(f\"  {t:<26} {meta['type']:<10} {meta['format']:<10} {pcols:<30} {meta['path']}\")\n",
					"    except Exception as e:\n",
					"        print(f\"  {t:<26} ERROR: {str(e)[:60]}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## üìå Cell 3: Migration Function (Partition-aware)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					},
					"collapsed": false
				},
				"source": [
					"def convert_managed_to_external(source_table, backup_path):\n",
					"    \"\"\"\n",
					"    Converts managed table to external at same location.\n",
					"    Handles partitioned and non-partitioned tables.\n",
					"    Path, format, partitions all auto-detected.\n",
					"    \"\"\"\n",
					"    print(f\"{'='*60}\")\n",
					"    print(f\"Converting: {source_table}\")\n",
					"    print(f\"{'='*60}\")\n",
					"\n",
					"    # ---- Step 1: Auto-detect all metadata ----\n",
					"    print(f\"\\n[Step 1/9] Detecting metadata...\")\n",
					"    meta = get_table_metadata(source_table)\n",
					"    data_path = meta[\"path\"]\n",
					"    tbl_type = meta[\"type\"]\n",
					"    table_format = meta[\"format\"]\n",
					"    partition_cols = meta[\"partitions\"]\n",
					"    fmt = table_format.lower()\n",
					"\n",
					"    print(f\"  Path       : {data_path}\")\n",
					"    print(f\"  Type       : {tbl_type}\")\n",
					"    print(f\"  Format     : {table_format}\")\n",
					"    print(f\"  Partitions : {partition_cols if partition_cols else 'None'}\")\n",
					"\n",
					"    if tbl_type == \"EXTERNAL\":\n",
					"        print(f\"  ‚ö†Ô∏è Already EXTERNAL. Skipping.\")\n",
					"        return \"SKIPPED_EXTERNAL\", table_format, data_path, partition_cols\n",
					"\n",
					"    # ---- Step 2: Read & capture metadata ----\n",
					"    print(f\"\\n[Step 2/9] Reading data...\")\n",
					"    df = spark.table(source_table)\n",
					"    original_count = df.count()\n",
					"    original_columns = sorted(df.columns)\n",
					"    print(f\"  Rows    : {original_count}\")\n",
					"    print(f\"  Columns : {len(original_columns)} -> {original_columns}\")\n",
					"\n",
					"    if original_count == 0:\n",
					"        print(f\"  ‚ö†Ô∏è Empty table. Skipping.\")\n",
					"        return \"SKIPPED_EMPTY\", table_format, data_path, partition_cols\n",
					"\n",
					"    # Capture distinct partition values for validation\n",
					"    original_partition_counts = {}\n",
					"    if partition_cols:\n",
					"        for pcol in partition_cols:\n",
					"            cnt = df.select(pcol).distinct().count()\n",
					"            original_partition_counts[pcol] = cnt\n",
					"        print(f\"  Partition values: {original_partition_counts}\")\n",
					"\n",
					"    # ---- Step 3: Backup (preserving partitions) ----\n",
					"    print(f\"\\n[Step 3/9] Backing up as {table_format}...\")\n",
					"    writer = df.write \\\n",
					"      .mode(\"overwrite\") \\\n",
					"      .format(fmt) \\\n",
					"      .option(\"overwriteSchema\", \"true\")\n",
					"\n",
					"    if partition_cols:\n",
					"        writer = writer.partitionBy(*partition_cols)\n",
					"        print(f\"  Backing up with partitionBy: {partition_cols}\")\n",
					"\n",
					"    writer.save(backup_path)\n",
					"    print(f\"  Backup at: {backup_path}\")\n",
					"\n",
					"    # ---- Step 4: Validate backup ----\n",
					"    print(f\"\\n[Step 4/9] Validating backup...\")\n",
					"    df_backup = spark.read.format(fmt).load(backup_path)\n",
					"    backup_count = df_backup.count()\n",
					"    backup_columns = sorted(df_backup.columns)\n",
					"\n",
					"    assert backup_count == original_count, \\\n",
					"        f\"BACKUP FAILED: Original={original_count}, Backup={backup_count}\"\n",
					"    assert backup_columns == original_columns, \\\n",
					"        f\"BACKUP FAILED: Schema mismatch!\"\n",
					"\n",
					"    # Validate partition values\n",
					"    if partition_cols:\n",
					"        for pcol in partition_cols:\n",
					"            bkp_cnt = df_backup.select(pcol).distinct().count()\n",
					"            assert bkp_cnt == original_partition_counts[pcol], \\\n",
					"                f\"BACKUP FAILED: Partition '{pcol}' mismatch! Original={original_partition_counts[pcol]}, Backup={bkp_cnt}\"\n",
					"        print(f\"  Partition values validated ‚úì\")\n",
					"\n",
					"    print(f\"  Rows: {backup_count} ‚úì | Schema ‚úì\")\n",
					"\n",
					"    # ---- Step 5: Drop managed table ----\n",
					"    print(f\"\\n[Step 5/9] Dropping managed table...\")\n",
					"    spark.sql(f\"DROP TABLE IF EXISTS {source_table}\")\n",
					"    print(f\"  Dropped.\")\n",
					"\n",
					"    # ---- Step 6: Restore data to original path (with partitions) ----\n",
					"    print(f\"\\n[Step 6/9] Restoring to original path...\")\n",
					"    df_restore = spark.read.format(fmt).load(backup_path)\n",
					"    restore_writer = df_restore.write \\\n",
					"      .mode(\"overwrite\") \\\n",
					"      .format(fmt) \\\n",
					"      .option(\"overwriteSchema\", \"true\")\n",
					"\n",
					"    if partition_cols:\n",
					"        restore_writer = restore_writer.partitionBy(*partition_cols)\n",
					"        print(f\"  Restoring with partitionBy: {partition_cols}\")\n",
					"\n",
					"    restore_writer.save(data_path)\n",
					"\n",
					"    restored_count = spark.read.format(fmt).load(data_path).count()\n",
					"    assert restored_count == original_count, \\\n",
					"        f\"RESTORE FAILED: Original={original_count}, Restored={restored_count}\"\n",
					"    print(f\"  Restored. Rows: {restored_count} ‚úì\")\n",
					"\n",
					"    # ---- Step 7: Register as external table ----\n",
					"    print(f\"\\n[Step 7/9] Registering external {table_format} table...\")\n",
					"\n",
					"    if partition_cols and table_format == \"PARQUET\":\n",
					"        # For partitioned Parquet: create with schema + partition columns + location\n",
					"        # Read schema from restored data\n",
					"        df_schema = spark.read.format(fmt).load(data_path)\n",
					"        # Create table using the dataframe schema\n",
					"        spark.sql(f\"DROP TABLE IF EXISTS {source_table}\")\n",
					"\n",
					"        # Build CREATE TABLE with partition columns\n",
					"        schema_fields = df_schema.schema\n",
					"        non_partition_cols = [f for f in schema_fields if f.name not in partition_cols]\n",
					"        partition_col_fields = [f for f in schema_fields if f.name in partition_cols]\n",
					"\n",
					"        def spark_type_to_sql(dtype):\n",
					"            type_str = str(dtype).lower()\n",
					"            mapping = {\n",
					"                \"stringtype()\": \"STRING\",\n",
					"                \"integertype()\": \"INT\",\n",
					"                \"longtype()\": \"BIGINT\",\n",
					"                \"doubletype()\": \"DOUBLE\",\n",
					"                \"floattype()\": \"FLOAT\",\n",
					"                \"booleantype()\": \"BOOLEAN\",\n",
					"                \"datetype()\": \"DATE\",\n",
					"                \"timestamptype()\": \"TIMESTAMP\",\n",
					"                \"shorttype()\": \"SMALLINT\",\n",
					"                \"bytetype()\": \"TINYINT\",\n",
					"                \"binarytype()\": \"BINARY\"\n",
					"            }\n",
					"            if type_str.startswith(\"decimaltype\"):\n",
					"                return str(dtype).replace(\"Type\", \"\").upper()\n",
					"            return mapping.get(type_str, \"STRING\")\n",
					"\n",
					"        col_defs = \", \".join([f\"`{f.name}` {spark_type_to_sql(f.dataType)}\" for f in non_partition_cols])\n",
					"        part_defs = \", \".join([f\"`{f.name}` {spark_type_to_sql(f.dataType)}\" for f in partition_col_fields])\n",
					"\n",
					"        create_sql = f\"\"\"\n",
					"            CREATE TABLE {source_table} ({col_defs})\n",
					"            USING PARQUET\n",
					"            PARTITIONED BY ({part_defs})\n",
					"            LOCATION '{data_path}'\n",
					"        \"\"\"\n",
					"        spark.sql(create_sql)\n",
					"        # Recover partitions so Spark sees existing partition folders\n",
					"        spark.sql(f\"MSCK REPAIR TABLE {source_table}\")\n",
					"        print(f\"  Partitioned Parquet table registered + partitions recovered.\")\n",
					"\n",
					"    elif partition_cols and table_format == \"DELTA\":\n",
					"        # Delta handles partitions automatically via _delta_log\n",
					"        spark.sql(f\"\"\"\n",
					"            CREATE TABLE {source_table}\n",
					"            USING DELTA\n",
					"            LOCATION '{data_path}'\n",
					"        \"\"\")\n",
					"        print(f\"  Partitioned Delta table registered (partitions in _delta_log).\")\n",
					"\n",
					"    else:\n",
					"        # Non-partitioned: simple registration\n",
					"        spark.sql(f\"\"\"\n",
					"            CREATE TABLE {source_table}\n",
					"            USING {table_format}\n",
					"            LOCATION '{data_path}'\n",
					"        \"\"\")\n",
					"        print(f\"  Non-partitioned table registered.\")\n",
					"\n",
					"    print(f\"  Registered: {source_table} -> {data_path}\")\n",
					"\n",
					"    # ---- Step 8: Validate partitions on external table ----\n",
					"    if partition_cols:\n",
					"        print(f\"\\n[Step 8/9] Validating partitions...\")\n",
					"        df_ext = spark.table(source_table)\n",
					"        for pcol in partition_cols:\n",
					"            ext_cnt = df_ext.select(pcol).distinct().count()\n",
					"            assert ext_cnt == original_partition_counts[pcol], \\\n",
					"                f\"PARTITION FAILED: '{pcol}' Original={original_partition_counts[pcol]}, External={ext_cnt}\"\n",
					"        print(f\"  Partition values match ‚úì\")\n",
					"\n",
					"        # Verify partition columns are registered\n",
					"        ext_partitions = get_partition_columns(source_table)\n",
					"        print(f\"  Registered partitions: {ext_partitions}\")\n",
					"    else:\n",
					"        print(f\"\\n[Step 8/9] No partitions to validate.\")\n",
					"\n",
					"    # ---- Step 9: Final validation ----\n",
					"    print(f\"\\n[Step 9/9] Final validation...\")\n",
					"    df_final = spark.table(source_table)\n",
					"    final_count = df_final.count()\n",
					"    final_columns = sorted(df_final.columns)\n",
					"\n",
					"    assert final_count == original_count, \\\n",
					"        f\"FINAL FAILED: Original={original_count}, Final={final_count}\"\n",
					"    assert final_columns == original_columns, \\\n",
					"        f\"FINAL FAILED: Schema mismatch!\"\n",
					"\n",
					"    new_type = get_table_type(source_table)\n",
					"    print(f\"  Type       : {new_type}\")\n",
					"    print(f\"  Format     : {table_format}\")\n",
					"    print(f\"  Partitions : {partition_cols if partition_cols else 'None'}\")\n",
					"    print(f\"  Path       : {data_path}\")\n",
					"    print(f\"  Rows       : {final_count} ‚úì | Schema ‚úì\")\n",
					"    print(f\"\\n‚úÖ Done: {source_table} [{table_format}] [Partitions: {partition_cols if partition_cols else 'None'}]\\n\")\n",
					"    return \"SUCCESS\", table_format, data_path, partition_cols\n",
					"\n",
					"print(\"Migration function loaded.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## üìå Cell 4: Rollback Function (Partition-aware)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					},
					"collapsed": false
				},
				"source": [
					"def rollback_from_backup(table_name, data_path, backup_path, table_format, partition_cols=None):\n",
					"    \"\"\"\n",
					"    Restores a managed table from backup, preserving partitions.\n",
					"    \"\"\"\n",
					"    fmt = table_format.lower()\n",
					"    print(f\"üîÅ Rolling back: {table_name} [{table_format}] [Partitions: {partition_cols}]\")\n",
					"\n",
					"    df_backup = spark.read.format(fmt).load(backup_path)\n",
					"    backup_count = df_backup.count()\n",
					"    print(f\"  Backup rows: {backup_count}\")\n",
					"\n",
					"    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
					"\n",
					"    # Restore to original path\n",
					"    writer = df_backup.write \\\n",
					"      .mode(\"overwrite\") \\\n",
					"      .format(fmt) \\\n",
					"      .option(\"overwriteSchema\", \"true\")\n",
					"\n",
					"    if partition_cols:\n",
					"        writer = writer.partitionBy(*partition_cols)\n",
					"\n",
					"    writer.save(data_path)\n",
					"\n",
					"    # Re-create as managed table\n",
					"    managed_writer = df_backup.write \\\n",
					"      .mode(\"overwrite\") \\\n",
					"      .format(fmt)\n",
					"\n",
					"    if partition_cols:\n",
					"        managed_writer = managed_writer.partitionBy(*partition_cols)\n",
					"\n",
					"    managed_writer.saveAsTable(table_name)\n",
					"\n",
					"    restored_count = spark.table(table_name).count()\n",
					"    assert restored_count == backup_count, \\\n",
					"        f\"ROLLBACK FAILED: Backup={backup_count}, Restored={restored_count}\"\n",
					"    print(f\"  ‚úÖ Rollback complete. Rows: {restored_count} ‚úì\\n\")\n",
					"\n",
					"print(\"Rollback function loaded.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## üìå Cell 5: Execute Migration"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					},
					"collapsed": false
				},
				"source": [
					"migration_log = []\n",
					"\n",
					"for table_name in tables_to_migrate:\n",
					"    bkp_path = f\"{backup_base}/{table_name.replace('.', '_')}\"\n",
					"    try:\n",
					"        status, fmt, dpath, pcols = convert_managed_to_external(table_name, bkp_path)\n",
					"        migration_log.append((table_name, status, fmt, dpath, bkp_path, pcols, \"\"))\n",
					"    except Exception as e:\n",
					"        fmt, dpath, pcols = \"UNKNOWN\", \"UNKNOWN\", []\n",
					"        try:\n",
					"            meta = get_table_metadata(table_name)\n",
					"            fmt, dpath, pcols = meta[\"format\"], meta[\"path\"], meta[\"partitions\"]\n",
					"        except Exception:\n",
					"            pass\n",
					"        print(f\"\\n‚ùå ERROR: {table_name}: {str(e)}\")\n",
					"        print(f\"   Backup : {bkp_path}\")\n",
					"        print(f\"   Rollback: rollback_from_backup('{table_name}', '{dpath}', '{bkp_path}', '{fmt}', {pcols})\\n\")\n",
					"        migration_log.append((table_name, \"FAILED\", fmt, dpath, bkp_path, pcols, str(e)))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## üìå Cell 6: Migration Summary"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					},
					"collapsed": false
				},
				"source": [
					"print(f\"\\n{'='*110}\")\n",
					"print(f\"{'MIGRATION SUMMARY':^110}\")\n",
					"print(f\"{'='*110}\")\n",
					"print(f\"{'Table':<28} {'Format':<10} {'Partitions':<25} {'Status':<18} {'Location'}\")\n",
					"print(f\"{'-'*110}\")\n",
					"\n",
					"for table, status, fmt, dpath, bpath, pcols, error in migration_log:\n",
					"    if status == \"SUCCESS\":       icon = \"‚úÖ\"\n",
					"    elif \"SKIPPED\" in status:      icon = \"‚ö†Ô∏è\"\n",
					"    else:                          icon = \"‚ùå\"\n",
					"    pcol_str = \", \".join(pcols) if pcols else \"None\"\n",
					"    print(f\"  {icon} {table:<26} {fmt:<10} {pcol_str:<25} {status:<18} {dpath}\")\n",
					"    if error:\n",
					"        print(f\"     Error  : {error[:100]}\")\n",
					"        print(f\"     Backup : {bpath}\")\n",
					"\n",
					"print(f\"{'-'*110}\")\n",
					"total   = len(migration_log)\n",
					"success = sum(1 for _,s,_,_,_,_,_ in migration_log if s == 'SUCCESS')\n",
					"skipped = sum(1 for _,s,_,_,_,_,_ in migration_log if 'SKIPPED' in s)\n",
					"failed  = sum(1 for _,s,_,_,_,_,_ in migration_log if s == 'FAILED')\n",
					"delta_c = sum(1 for _,s,f,_,_,_,_ in migration_log if f == 'DELTA' and s == 'SUCCESS')\n",
					"parq_c  = sum(1 for _,s,f,_,_,_,_ in migration_log if f == 'PARQUET' and s == 'SUCCESS')\n",
					"part_c  = sum(1 for _,s,_,_,_,p,_ in migration_log if p and s == 'SUCCESS')\n",
					"print(f\"Total: {total} | ‚úÖ Success: {success} (Delta: {delta_c}, Parquet: {parq_c}, Partitioned: {part_c}) | ‚ö†Ô∏è Skipped: {skipped} | ‚ùå Failed: {failed}\")\n",
					"print(f\"{'='*110}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## üìå Cell 7: ‚≠ê Wrapper Function for Existing Notebooks (Partition-aware)\n",
					"**Replace `saveAsTable` calls in your existing notebooks with this function.**\n",
					"\n",
					"```python\n",
					"# BEFORE (fails on external tables):\n",
					"df.write.mode('overwrite').format('parquet').saveAsTable('db.table')\n",
					"\n",
					"# AFTER (works with external + partitioned tables):\n",
					"write_to_external_table(df, 'db.table')\n",
					"```"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					},
					"collapsed": false
				},
				"source": [
					"def write_to_external_table(df, table_name, mode=\"overwrite\", partition_by=None):\n",
					"    \"\"\"\n",
					"    Drop-in replacement for saveAsTable that works with external tables.\n",
					"    Auto-detects path, format, and partition columns from the external table.\n",
					"    \n",
					"    BEFORE (fails on external table):\n",
					"        df.write.mode('overwrite').format('parquet').saveAsTable('db.table')\n",
					"        df.write.mode('overwrite').partitionBy('year','month').format('parquet').saveAsTable('db.table')\n",
					"    \n",
					"    AFTER (works with external + partitioned tables):\n",
					"        write_to_external_table(df, 'db.table')\n",
					"        write_to_external_table(df, 'db.table', partition_by=['year','month'])  # or auto-detected\n",
					"    \n",
					"    Args:\n",
					"        df            : DataFrame to write\n",
					"        table_name    : Fully qualified table name (e.g., 'my_db.my_table')\n",
					"        mode          : Write mode ('overwrite' or 'append')\n",
					"        partition_by  : List of partition columns. If None, auto-detected from table metadata.\n",
					"    \"\"\"\n",
					"    # Auto-detect metadata\n",
					"    data_path = get_table_location(table_name)\n",
					"    table_format = detect_table_format(table_name, data_path)\n",
					"    fmt = table_format.lower()\n",
					"\n",
					"    # Auto-detect partitions if not provided\n",
					"    if partition_by is None:\n",
					"        partition_by = get_partition_columns(table_name)\n",
					"\n",
					"    print(f\"Writing to external table: {table_name}\")\n",
					"    print(f\"  Path       : {data_path}\")\n",
					"    print(f\"  Format     : {table_format}\")\n",
					"    print(f\"  Mode       : {mode}\")\n",
					"    print(f\"  Partitions : {partition_by if partition_by else 'None'}\")\n",
					"\n",
					"    # Build writer\n",
					"    writer = df.write \\\n",
					"      .mode(mode) \\\n",
					"      .format(fmt) \\\n",
					"      .option(\"overwriteSchema\", \"true\")\n",
					"\n",
					"    if partition_by:\n",
					"        writer = writer.partitionBy(*partition_by)\n",
					"\n",
					"    # Write directly to external path\n",
					"    writer.save(data_path)\n",
					"\n",
					"    # For Parquet, recover any new partitions\n",
					"    if table_format == \"PARQUET\" and partition_by:\n",
					"        spark.sql(f\"MSCK REPAIR TABLE {table_name}\")\n",
					"        print(f\"  Partitions recovered (MSCK REPAIR).\")\n",
					"\n",
					"    row_count = spark.table(table_name).count()\n",
					"    print(f\"  Written rows: {row_count} ‚úì\")\n",
					"\n",
					"\n",
					"# =====================================================\n",
					"# USAGE EXAMPLES\n",
					"# =====================================================\n",
					"print(\"write_to_external_table() function loaded.\")\n",
					"print(\"\")\n",
					"print(\"Usage examples:\")\n",
					"print(\"\")\n",
					"print(\"  # Non-partitioned table:\")\n",
					"print(\"  write_to_external_table(df, 'my_db.customers')\")\n",
					"print(\"\")\n",
					"print(\"  # Partitioned table (auto-detects partition cols):\")\n",
					"print(\"  write_to_external_table(df, 'my_db.orders')\")\n",
					"print(\"\")\n",
					"print(\"  # Explicit partition columns:\")\n",
					"print(\"  write_to_external_table(df, 'my_db.orders', partition_by=['year', 'month'])\")\n",
					"print(\"\")\n",
					"print(\"  # Append mode:\")\n",
					"print(\"  write_to_external_table(df, 'my_db.orders', mode='append')\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## üìå Cell 8: Rollback (if needed)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					},
					"collapsed": false
				},
				"source": [
					"# Rollback ALL failed:\n",
					"# for table, status, fmt, dpath, bpath, pcols, error in migration_log:\n",
					"#     if status == \"FAILED\":\n",
					"#         rollback_from_backup(table, dpath, bpath, fmt, pcols)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## üìå Cell 9: Cleanup Backups (ONLY after verification)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					},
					"collapsed": false
				},
				"source": [
					"# mssparkutils.fs.rm(backup_base, recurse=True)\n",
					"# print(f\"üóëÔ∏è Backups cleaned up: {backup_base}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df_table_path = spark.sql(f\"DESCRIBE EXTENDED {tables_to_migrate[0]}\")\n",
					"\n",
					"table_path = df_table_path.where(\"col_name == 'Location'\").select(\"data_type\").collect()[0][0]\n",
					"table_type = df_table_path.where(\"col_name == 'Type'\").select(\"data_type\").collect()[0][0]\n",
					"\n",
					"print(table_path)\n",
					"print(table_type)"
				],
				"execution_count": null
			}
		]
	}
}