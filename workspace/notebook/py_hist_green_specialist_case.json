{
	"name": "py_hist_green_specialist_case",
	"properties": {
		"folder": {
			"name": "odw-harmonised/green_specialist_case"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4799164b-9e57-40d6-872c-2e5c58bbd4ee"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"PipelineName = \"pln_curated\"\n",
					"PipelineRunID = \"5b66ce96-e547-4303-954a-2bfe138924bb\"\n",
					"PipelineTriggerID = \"8eb8c2f00fa446e88812104b6c8fe493\"\n",
					"PipelineTriggerName = \"Sandbox\"\n",
					"PipelineTriggerType = \"Manual\"\n",
					"PipelineTriggeredbyPipelineName = None\n",
					"PipelineTriggeredbyPipelineRunID = None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import sys\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.window import Window\n",
					"from delta.tables import DeltaTable\n",
					"from datetime import datetime"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_harmonised_db.hist_green_specialist_case\"\n",
					"source_table = \"odw_harmonised_db.load_green_specialist_case\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message = ''\n",
					"\n",
					"# SCD metrics\n",
					"new_records = 0\n",
					"changed_records = 0\n",
					"unchanged_records = 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Setting legacy time parser policy\")\n",
					"        spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"        logInfo(\"Legacy time parser policy set successfully\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in setting time parser policy: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(f\"Reading source data from {source_table}\")\n",
					"        \n",
					"        # Create staging view with deduplication and hash calculation\n",
					"        # Filter only active records (active = 'Y')\n",
					"        spark.sql(f\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW stg_casework_specialist AS\n",
					"            SELECT\n",
					"                casework_specialist_id,\n",
					"                greenCaseType,\n",
					"                greenCaseId,\n",
					"                caseReference,\n",
					"                horizonId,\n",
					"                linkedGreenCaseId,\n",
					"                caseOfficerName,\n",
					"                caseOfficerEmail,\n",
					"                appealType,\n",
					"                procedure,\n",
					"                processingState,\n",
					"                pinsLpaCode,\n",
					"                pinsLpaName,\n",
					"                appellantName,\n",
					"                agentName,\n",
					"                siteAddressDescription,\n",
					"                sitePostcode,\n",
					"                otherPartyName,\n",
					"                receiptDate,\n",
					"                validDate,\n",
					"                startDate,\n",
					"                lpaQuestionnaireDue,\n",
					"                lpaQuestionnaireReceived,\n",
					"                week6Date,\n",
					"                week8Date,\n",
					"                week9Date,\n",
					"                eventType,\n",
					"                eventDate,\n",
					"                eventTime,\n",
					"                inspectorName,\n",
					"                inspectorStaffNumber,\n",
					"                decision,\n",
					"                decisionDate,\n",
					"                withdrawnOrTurnedAwayDate,\n",
					"                comments,\n",
					"                active,\n",
					"                'N' as Migrated,\n",
					"                -- Data hash for change detection\n",
					"                md5(concat_ws('|',\n",
					"                    COALESCE(greenCaseType, '.'),\n",
					"                    COALESCE(CAST(greenCaseId AS STRING), '.'),\n",
					"                    COALESCE(caseReference, '.'),\n",
					"                    COALESCE(horizonId, '.'),\n",
					"                    COALESCE(CAST(linkedGreenCaseId AS STRING), '.'),\n",
					"                    COALESCE(caseOfficerName, '.'),\n",
					"                    COALESCE(caseOfficerEmail, '.'),\n",
					"                    COALESCE(appealType, '.'),\n",
					"                    COALESCE(procedure, '.'),\n",
					"                    COALESCE(processingState, '.'),\n",
					"                    COALESCE(pinsLpaCode, '.'),\n",
					"                    COALESCE(pinsLpaName, '.'),\n",
					"                    COALESCE(appellantName, '.'),\n",
					"                    COALESCE(agentName, '.'),\n",
					"                    COALESCE(siteAddressDescription, '.'),\n",
					"                    COALESCE(sitePostcode, '.'),\n",
					"                    COALESCE(otherPartyName, '.'),\n",
					"                    COALESCE(CAST(receiptDate AS STRING), '.'),\n",
					"                    COALESCE(CAST(validDate AS STRING), '.'),\n",
					"                    COALESCE(CAST(startDate AS STRING), '.'),\n",
					"                    COALESCE(CAST(lpaQuestionnaireDue AS STRING), '.'),\n",
					"                    COALESCE(CAST(lpaQuestionnaireReceived AS STRING), '.'),\n",
					"                    COALESCE(CAST(week6Date AS STRING), '.'),\n",
					"                    COALESCE(CAST(week8Date AS STRING), '.'),\n",
					"                    COALESCE(CAST(week9Date AS STRING), '.'),\n",
					"                    COALESCE(eventType, '.'),\n",
					"                    COALESCE(CAST(eventDate AS STRING), '.'),\n",
					"                    COALESCE(eventTime, '.'),\n",
					"                    COALESCE(inspectorName, '.'),\n",
					"                    COALESCE(inspectorStaffNumber, '.'),\n",
					"                    COALESCE(decision, '.'),\n",
					"                    COALESCE(CAST(decisionDate AS STRING), '.'),\n",
					"                    COALESCE(CAST(withdrawnOrTurnedAwayDate AS STRING), '.'),\n",
					"                    COALESCE(comments, '.'),\n",
					"                    COALESCE(active, '.')\n",
					"                )) as data_hash\n",
					"            FROM (\n",
					"                SELECT *, ROW_NUMBER() OVER (\n",
					"                    PARTITION BY greenCaseId \n",
					"                    ORDER BY receiptDate DESC NULLS LAST\n",
					"                ) as rn\n",
					"                FROM {source_table}\n",
					"                --- WHERE active = 'Y'\n",
					"            ) dedup \n",
					"            WHERE rn = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        src_count = spark.sql(\"SELECT COUNT(*) FROM stg_casework_specialist\").collect()[0][0]\n",
					"        logInfo(f\"Source loaded: {src_count} active records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error preparing source: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Performing change analysis\")\n",
					"        \n",
					"        # Get current active records from target\n",
					"        spark.sql(f\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW current_active AS\n",
					"            SELECT casework_specialist_id, greenCaseId,greenCaseType, RowID as data_hash\n",
					"            FROM {spark_table_final} \n",
					"            WHERE is_current = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        # Analyze changes: NEW, CHANGED, UNCHANGED\n",
					"        spark.sql(\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW change_analysis AS\n",
					"            SELECT \n",
					"                src.greenCaseId, \n",
					"                tgt.greenCaseType,\n",
					"                tgt.casework_specialist_id,\n",
					"                CASE \n",
					"                    WHEN tgt.greenCaseId IS NULL THEN 'NEW'\n",
					"                    WHEN src.data_hash != tgt.data_hash THEN 'CHANGED'\n",
					"                    ELSE 'UNCHANGED'\n",
					"                END as change_type\n",
					"            FROM stg_casework_specialist src\n",
					"            LEFT JOIN current_active tgt ON src.greenCaseId = tgt.greenCaseId\n",
					"        \"\"\")\n",
					"        \n",
					"        # Count changes by type\n",
					"        for row in spark.sql(\"SELECT change_type, COUNT(*) as cnt FROM change_analysis GROUP BY change_type\").collect():\n",
					"            if row['change_type'] == 'NEW': \n",
					"                new_records = row['cnt']\n",
					"            elif row['change_type'] == 'CHANGED': \n",
					"                changed_records = row['cnt']\n",
					"            elif row['change_type'] == 'UNCHANGED': \n",
					"                unchanged_records = row['cnt']\n",
					"        \n",
					"        # Count deletions (records in target but not in source)\n",
					"        delete_count = spark.sql(\"\"\"\n",
					"            SELECT COUNT(*) FROM current_active tgt\n",
					"            LEFT JOIN stg_casework_specialist src ON tgt.greenCaseId = src.greenCaseId\n",
					"            WHERE src.greenCaseId IS NULL\n",
					"        \"\"\").collect()[0][0]\n",
					"        \n",
					"        logInfo(f\"Change analysis - New: {new_records}, Changed: {changed_records}, Unchanged: {unchanged_records}, Deleted: {delete_count}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in change analysis: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"if not error_message and (changed_records > 0 or delete_count > 0):\n",
					"    try:\n",
					"        update_count = changed_records + delete_count\n",
					"        logInfo(f\"Closing {update_count} records (changed: {changed_records}, deleted: {delete_count})\")\n",
					"        \n",
					"        # Create temp view with records to close\n",
					"        spark.sql(\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW records_to_close AS\n",
					"            SELECT DISTINCT greenCaseId\n",
					"            FROM (\n",
					"                -- Changed records\n",
					"                SELECT greenCaseId,greenCaseType FROM change_analysis WHERE change_type = 'CHANGED'\n",
					"                UNION\n",
					"                -- Deleted records (in target but not in source)\n",
					"                SELECT tgt.greenCaseId, tgt.greenCaseType\n",
					"                FROM current_active tgt\n",
					"                LEFT JOIN stg_casework_specialist src ON tgt.greenCaseId = src.greenCaseId\n",
					"                WHERE src.greenCaseId IS NULL\n",
					"            )\n",
					"        \"\"\")\n",
					"        print(\"sql1\")\n",
					"        # Use MERGE to close records (Delta Lake compatible)\n",
					"        spark.sql(f\"\"\"\n",
					"            MERGE INTO {spark_table_final} AS target\n",
					"            USING records_to_close AS source\n",
					"            ON target.greenCaseId = source.greenCaseId\n",
					"               AND target.greenCaseType = source.greenCaseType\n",
					"               AND target.is_current = 1\n",
					"            WHEN MATCHED THEN\n",
					"                UPDATE SET \n",
					"                    record_end_date = DATE_TRUNC('day', CURRENT_TIMESTAMP()) - 1,                     \n",
					"                    is_current = 0\n",
					"        \"\"\")\n",
					"        \n",
					"        print(\"sql2\")\n",
					"        logInfo(f\"Successfully closed {update_count} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error closing records: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"if not error_message and (new_records > 0 or changed_records > 0):\n",
					"    try:\n",
					"        logInfo(f\"Inserting {new_records + changed_records} records (new: {new_records}, changed: {changed_records})\")\n",
					"        \n",
					"        # Get max ID for new record assignment\n",
					"        max_id = spark.sql(f\"SELECT COALESCE(MAX(casework_specialist_id), 0) FROM {spark_table_final}\").collect()[0][0]\n",
					"        \n",
					"        # Insert new versions of changed records + completely new records\n",
					"        spark.sql(f\"\"\"\n",
					"            INSERT INTO {spark_table_final} (\n",
					"                casework_specialist_id,\n",
					"                greenCaseType,\n",
					"                greenCaseId,\n",
					"                caseReference,\n",
					"                horizonId,\n",
					"                linkedGreenCaseId,\n",
					"                caseOfficerName,\n",
					"                caseOfficerEmail,\n",
					"                appealType,\n",
					"                procedure,\n",
					"                processingState,\n",
					"                pinsLpaCode,\n",
					"                pinsLpaName,\n",
					"                appellantName,\n",
					"                agentName,\n",
					"                siteAddressDescription,\n",
					"                sitePostcode,\n",
					"                otherPartyName,\n",
					"                receiptDate,\n",
					"                validDate,\n",
					"                startDate,\n",
					"                lpaQuestionnaireDue,\n",
					"                lpaQuestionnaireReceived,\n",
					"                week6Date,\n",
					"                week8Date,\n",
					"                week9Date,\n",
					"                eventType,\n",
					"                eventDate,\n",
					"                eventTime,\n",
					"                inspectorName,\n",
					"                inspectorStaffNumber,\n",
					"                decision,\n",
					"                decisionDate,\n",
					"                withdrawnOrTurnedAwayDate,\n",
					"                comments,\n",
					"                Migrated,\n",
					"                IngestionDate,\n",
					"                is_current,\n",
					"                record_start_date,\n",
					"                record_end_date,\n",
					"                RowID,\n",
					"                active\n",
					"            )\n",
					"            SELECT\n",
					"                CASE \n",
					"                    WHEN ca.change_type = 'CHANGED' THEN ca.casework_specialist_id\n",
					"                    ELSE ROW_NUMBER() OVER (ORDER BY stg.greenCaseId) + {max_id}\n",
					"                END AS casework_specialist_id,\n",
					"                stg.greenCaseType,\n",
					"                stg.greenCaseId,\n",
					"                stg.caseReference,\n",
					"                stg.horizonId,\n",
					"                stg.linkedGreenCaseId,\n",
					"                stg.caseOfficerName,\n",
					"                stg.caseOfficerEmail,\n",
					"                stg.appealType,\n",
					"                stg.procedure,\n",
					"                stg.processingState,\n",
					"                stg.pinsLpaCode,\n",
					"                stg.pinsLpaName,\n",
					"                stg.appellantName,\n",
					"                stg.agentName,\n",
					"                stg.siteAddressDescription,\n",
					"                stg.sitePostcode,\n",
					"                stg.otherPartyName,\n",
					"                stg.receiptDate,\n",
					"                stg.validDate,\n",
					"                stg.startDate,\n",
					"                stg.lpaQuestionnaireDue,\n",
					"                stg.lpaQuestionnaireReceived,\n",
					"                stg.week6Date,\n",
					"                stg.week8Date,\n",
					"                stg.week9Date,\n",
					"                stg.eventType,\n",
					"                stg.eventDate,\n",
					"                stg.eventTime,\n",
					"                stg.inspectorName,\n",
					"                stg.inspectorStaffNumber,\n",
					"                stg.decision,\n",
					"                stg.decisionDate,\n",
					"                stg.withdrawnOrTurnedAwayDate,\n",
					"                stg.comments,\n",
					"                stg.Migrated,\n",
					"                CURRENT_TIMESTAMP() AS IngestionDate,\n",
					"                1 AS is_current,\n",
					"                DATE_TRUNC('day', CURRENT_TIMESTAMP()) AS record_start_date,\n",
					"                --CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS record_end_date,\n",
					"                CAST('9999-12-31' AS TIMESTAMP) AS record_end_date,\n",
					"                stg.data_hash AS RowID,\n",
					"                stg.active\n",
					"            FROM stg_casework_specialist stg\n",
					"            INNER JOIN change_analysis ca \n",
					"                ON stg.greenCaseId = ca.greenCaseId \n",
					"                AND ca.change_type IN ('NEW', 'CHANGED')\n",
					"        \"\"\")\n",
					"        \n",
					"        insert_count = new_records + changed_records\n",
					"        logInfo(f\"Successfully inserted {insert_count} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error inserting records: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Final statistics\n",
					"        total = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final}\").collect()[0][0]\n",
					"        active = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final} WHERE is_current = 1\").collect()[0][0]\n",
					"        hist = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final} WHERE is_current = 0\").collect()[0][0]\n",
					"        \n",
					"        logInfo(f\"SCD Type 2 completed - Total: {total}, Active: {active}, Historical: {hist}\")\n",
					"        logInfo(f\"Changes - New: {new_records}, Changed: {changed_records}, Deleted: {delete_count}, Unchanged: {unchanged_records}\")\n",
					"        \n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in final stats: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					"\n",
					"# Telemetry\n",
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"SCD Type 2 complete (New: {new_records}, Changed: {changed_records}, Deleted: {delete_count})\"\n",
					"    if not error_message \n",
					"    else f\"SCD Type 2 failed for {spark_table_final}\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					"\n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    spark_table_final,\n",
					"    insert_count,\n",
					"    update_count,\n",
					"    delete_count,\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")"
				],
				"execution_count": null
			}
		]
	}
}