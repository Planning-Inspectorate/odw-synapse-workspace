{
	"name": "py_hist_green_specialist_case",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c65dfdc9-ecee-46ff-82db-7bcd418d1b32"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"PipelineName = \"pln_curated\"\n",
					"PipelineRunID = \"5b66ce96-e547-4303-954a-2bfe138924bb\"\n",
					"PipelineTriggerID = \"8eb8c2f00fa446e88812104b6c8fe493\"\n",
					"PipelineTriggerName = \"Sandbox\"\n",
					"PipelineTriggerType = \"Manual\"\n",
					"PipelineTriggeredbyPipelineName = None\n",
					"PipelineTriggeredbyPipelineRunID = None"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.window import Window\n",
					"from delta.tables import DeltaTable\n",
					"from datetime import datetime"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_harmonised_db.hist_green_specialist_case\"\n",
					"source_table = \"odw_harmonised_db.load_green_specialist_case\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message = ''\n",
					"\n",
					"# SCD metrics\n",
					"new_records = 0\n",
					"changed_records = 0\n",
					"unchanged_records = 0\n",
					"duplicates_removed = 0\n",
					"table_exists = False"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Setting legacy time parser policy\")\n",
					"        spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"        logInfo(\"Legacy time parser policy set successfully\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in setting time parser policy: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(f\"Reading source data from {source_table}\")\n",
					"        \n",
					"        # Check duplicates\n",
					"        dup_count = spark.sql(f\"\"\"\n",
					"            SELECT COUNT(*) as cnt FROM (\n",
					"                SELECT greenCaseId FROM {source_table}\n",
					"                GROUP BY greenCaseId HAVING COUNT(*) > 1\n",
					"            )\n",
					"        \"\"\").collect()[0]['cnt']\n",
					"        \n",
					"        if dup_count > 0:\n",
					"            logInfo(f\"Found {dup_count} duplicates - applying deduplication\")\n",
					"        \n",
					"        # Create staging with hash (CALCULATED ONCE)\n",
					"        spark.sql(f\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW stg_casework_specialist AS\n",
					"    SELECT\n",
					"                casework_specialist_id,greenCaseType, greenCaseId, caseReference,\n",
					"                horizonId, linkedGreenCaseId, caseOfficerName, caseOfficerEmail,\n",
					"                appealType, procedure, processingState,\n",
					"                pinsLpaCode, pinsLpaName,\n",
					"                appellantName, agentName,\n",
					"                siteAddressDescription,sitePostcode,\n",
					"                otherPartyName,\n",
					"                to_date(receiptDate, 'dd/MM/yyyy') AS receiptDate,\n",
					"                to_date(validDate, 'dd/MM/yyyy') AS validDate,\n",
					"                to_date(startDate, 'dd/MM/yyyy') AS startDate,\n",
					"                lpaQuestionnaireDue,\n",
					"                lpaQuestionnaireReceived,\n",
					"                week6Date,\n",
					"                week8Date,\n",
					"                week9Date,\n",
					"                eventType,\n",
					"                to_date(eventDate, 'dd/MM/yyyy') AS eventDate,\n",
					"                eventTime, inspectorName, inspectorStaffNumber,\n",
					"                decision, to_date(decisionDate, 'dd/MM/yyyy') AS decisionDate,\n",
					"                withdrawnOrTurnedAwayDate,\n",
					"                comments, 'N' as Migrated,\n",
					"                md5(concat_ws('|',\n",
					"                    COALESCE(greenCaseType, '.'), COALESCE(CAST(greenCaseId AS STRING), '.'),\n",
					"                    COALESCE(caseReference, '.'), COALESCE(horizonId, '.'),\n",
					"                    COALESCE(procedure, '.'), COALESCE(processingState, '.'),\n",
					"                    COALESCE(pinsLpaCode, '.'), COALESCE(decision, '.')\n",
					"                )) as data_hash\n",
					"            FROM (\n",
					"                SELECT *, ROW_NUMBER() OVER (\n",
					"                    PARTITION BY greenCaseId ORDER BY \n",
					"                    to_date(receiptDate, 'dd/MM/yyyy') DESC NULLS LAST\n",
					"                ) as rn\n",
					"                FROM {source_table}\n",
					"            ) dedup WHERE rn = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        src_count = spark.sql(\"SELECT COUNT(*) FROM stg_casework_specialist\").collect()[0][0]\n",
					"        logInfo(f\"Source loaded: {src_count} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error preparing source: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message and table_exists:\n",
					"    try:\n",
					"        # Get active records\n",
					"        spark.sql(f\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW current_active AS\n",
					"            SELECT casework_specialist_id, greenCaseId, RowID as data_hash\n",
					"            FROM {spark_table_final} WHERE is_current = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        # Analyze changes\n",
					"        spark.sql(\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW change_analysis AS\n",
					"            SELECT src.greenCaseId, tgt.casework_specialist_id,\n",
					"                CASE \n",
					"                    WHEN tgt.greenCaseId IS NULL THEN 'NEW'\n",
					"                    WHEN src.data_hash != tgt.data_hash THEN 'CHANGED'\n",
					"                    ELSE 'UNCHANGED'\n",
					"                END as change_type\n",
					"            FROM stg_casework_specialist src\n",
					"            LEFT JOIN current_active tgt ON src.greenCaseId = tgt.greenCaseId\n",
					"        \"\"\")\n",
					"        \n",
					"        # Count changes\n",
					"        for row in spark.sql(\"SELECT change_type, COUNT(*) as cnt FROM change_analysis GROUP BY change_type\").collect():\n",
					"            if row['change_type'] == 'NEW': new_records = row['cnt']\n",
					"            elif row['change_type'] == 'CHANGED': changed_records = row['cnt']\n",
					"            elif row['change_type'] == 'UNCHANGED': unchanged_records = row['cnt']\n",
					"        \n",
					"        # Count deletions\n",
					"        delete_count = spark.sql(\"\"\"\n",
					"            SELECT COUNT(*) FROM current_active tgt\n",
					"            LEFT JOIN stg_casework_specialist src ON tgt.greenCaseId = src.greenCaseId\n",
					"            WHERE src.greenCaseId IS NULL\n",
					"        \"\"\").collect()[0][0]\n",
					"        \n",
					"        logInfo(f\"Changes - New: {new_records}, Changed: {changed_records}, Deleted: {delete_count}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in change analysis: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message and table_exists and (changed_records > 0 or delete_count > 0):\n",
					"    try:\n",
					"        update_count = changed_records + delete_count\n",
					"        logInfo(f\"Closing {update_count} records\")\n",
					"        \n",
					"        spark.sql(f\"\"\"\n",
					"            UPDATE {spark_table_final}\n",
					"            SET record_end_date = CURRENT_TIMESTAMP(), is_current = 0\n",
					"            WHERE is_current = 1 AND greenCaseId IN (\n",
					"                SELECT greenCaseId FROM change_analysis WHERE change_type = 'CHANGED'\n",
					"                UNION\n",
					"                SELECT tgt.greenCaseId FROM current_active tgt\n",
					"                LEFT JOIN stg_casework_specialist src ON tgt.greenCaseId = src.greenCaseId\n",
					"                WHERE src.greenCaseId IS NULL\n",
					"            )\n",
					"        \"\"\")\n",
					"        logInfo(f\"Closed {update_count} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error closing records: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message and table_exists and (new_records > 0 or changed_records > 0):\n",
					"    try:\n",
					"        logInfo(f\"Inserting {new_records + changed_records} records\")\n",
					"        \n",
					"        max_id = spark.sql(f\"SELECT COALESCE(MAX(casework_specialist_id), 0) FROM {spark_table_final}\").collect()[0][0]\n",
					"        \n",
					"        spark.sql(f\"\"\"\n",
					"            INSERT INTO {spark_table_final}\n",
					"            SELECT\n",
					"                CASE WHEN ca.change_type = 'CHANGED' THEN ca.casework_specialist_id\n",
					"                     ELSE ROW_NUMBER() OVER (ORDER BY stg.greenCaseId) + {max_id}\n",
					"                END AS casework_specialist_id,\n",
					"                stg.*, CURRENT_TIMESTAMP() AS IngestionDate,\n",
					"                1 AS is_current, CURRENT_TIMESTAMP() AS record_start_date,\n",
					"                CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS record_end_date,\n",
					"                stg.data_hash AS RowID\n",
					"            FROM stg_casework_specialist stg\n",
					"            INNER JOIN change_analysis ca ON stg.greenCaseId = ca.greenCaseId \n",
					"                AND ca.change_type IN ('NEW', 'CHANGED')\n",
					"        \"\"\")\n",
					"        \n",
					"        insert_count = new_records + changed_records\n",
					"        logInfo(f\"Inserted {insert_count} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error inserting records: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Data quality checks\")\n",
					"        \n",
					"        # Check NULLs\n",
					"        nulls = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final} WHERE RowID IS NULL\").collect()[0][0]\n",
					"        if nulls > 0: logError(f\"{nulls} NULL RowIDs\")\n",
					"        \n",
					"        # Check duplicates\n",
					"        dups = spark.sql(f\"\"\"\n",
					"            SELECT COUNT(*) FROM (\n",
					"                SELECT greenCaseId FROM {spark_table_final} WHERE is_current = 1\n",
					"                GROUP BY greenCaseId HAVING COUNT(*) > 1\n",
					"            )\n",
					"        \"\"\").collect()[0][0]\n",
					"        if dups > 0: logError(f\"{dups} duplicate active records\")\n",
					"        else: logInfo(\"No duplicates\")\n",
					"        \n",
					"        # Stats\n",
					"        total = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final}\").collect()[0][0]\n",
					"        active = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final} WHERE is_current=1\").collect()[0][0]\n",
					"        hist = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final} WHERE is_current=0\").collect()[0][0]\n",
					"        \n",
					"        logInfo(f\"Complete - Total: {total}, Active: {active}, Historical: {hist}\")\n",
					"        logInfo(f\"New: {new_records}, Changed: {changed_records}, Deleted: {delete_count}\")\n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in quality checks: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					"\n",
					"# Telemetry\n",
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"SCD Type 2 complete (New: {new_records}, Changed: {changed_records}, Deleted: {delete_count})\"\n",
					"    if not error_message else f\"SCD Type 2 failed\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					"\n",
					"log_telemetry_and_exit(\n",
					"    stage, start_exec_time, end_exec_time, error_message, spark_table_final,\n",
					"    insert_count, update_count, delete_count,\n",
					"    PipelineName, PipelineRunID, PipelineTriggerID, PipelineTriggerName, PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName, PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type, duration_seconds, status_message, status_code\n",
					")"
				],
				"execution_count": 46
			}
		]
	}
}