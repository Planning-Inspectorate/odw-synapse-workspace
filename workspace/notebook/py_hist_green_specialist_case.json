{
	"name": "py_hist_green_specialist_case",
	"properties": {
		"folder": {
			"name": "odw-harmonised/green_specialist_case"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodwpr",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a16dac1c-4e5f-4ca6-afa5-1f69d6af4dae"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodwpr",
				"name": "pinssynspodwpr",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodwpr",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"#### The purpose of this notebook is to read data from Standardised Layer and create Hostoric data for green cases in Harmonised layer \n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Casework Specialist curated view and table for Power BI\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"import sys\n",
					"from pyspark.sql.window import Window\n",
					"from delta.tables import DeltaTable\n",
					"from datetime import datetime\n",
					"from pyspark.sql import functions as F"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_harmonised_db.hist_green_specialist_case\"\n",
					"source_table = \"odw_harmonised_db.load_green_specialist_case\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message = ''\n",
					"\n",
					"# SCD metrics\n",
					"new_records = 0\n",
					"changed_records = 0\n",
					"unchanged_records = 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Setting legacy time parser policy\")\n",
					"        spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"        logInfo(\"Legacy time parser policy set successfully\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in setting time parser policy: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        \n",
					"           \n",
					"        logInfo(f\"Reading source data from {source_table}\")\n",
					"        \n",
					"        # Define columns for hash calculation\n",
					"        hash_cols = [\n",
					"            'greenCaseType', 'greenCaseId', 'caseReference', 'horizonId',\n",
					"            'linkedGreenCaseId', 'caseOfficerName', 'caseOfficerEmail',\n",
					"            'appealType', 'procedure', 'processingState', 'pinsLpaCode',\n",
					"            'pinsLpaName', 'appellantName', 'agentName', 'siteAddressDescription',\n",
					"            'sitePostcode', 'otherPartyName', 'receiptDate', 'validDate',\n",
					"            'startDate', 'lpaQuestionnaireDue', 'lpaQuestionnaireReceived',\n",
					"            'week6Date', 'week8Date', 'week9Date', 'eventType', 'eventDate',\n",
					"            'eventTime', 'inspectorName', 'inspectorStaffNumber', 'decision',\n",
					"            'decisionDate', 'withdrawnOrTurnedAwayDate', 'comments', 'active'\n",
					"        ]\n",
					"        \n",
					"        # Create window spec for deduplication\n",
					"        window_spec = Window.partitionBy('greenCaseId').orderBy(\n",
					"            F.col('receiptDate').desc_nulls_last()\n",
					"        )\n",
					"        \n",
					"        # Read, deduplicate, and calculate hash\n",
					"        stg_df = (\n",
					"            spark.table(source_table)\n",
					"            .withColumn('rn', F.row_number().over(window_spec))\n",
					"            .filter(F.col('rn') == 1)\n",
					"            .withColumn('Migrated', F.lit('N'))\n",
					"            .withColumn('data_hash', \n",
					"                F.md5(F.concat_ws('|', *[F.coalesce(F.col(c).cast('string'), F.lit('.')) for c in hash_cols]))\n",
					"            )\n",
					"            .drop('rn')\n",
					"        )\n",
					"        \n",
					"        stg_df.createOrReplaceTempView('stg_casework_specialist')\n",
					"        src_count = stg_df.count()\n",
					"        logInfo(f\"Source loaded: {src_count} records after deduplication\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error preparing source: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"if not error_message:\n",
					"    try:\n",
					"        target_exists = spark.sql(f\"\"\"\n",
					"            SELECT COUNT(*) as cnt FROM {spark_table_final} WHERE is_current = 1\n",
					"        \"\"\").collect()[0][0]\n",
					"        \n",
					"        logInfo(f\"Target table has {target_exists} current records\")\n",
					"        \n",
					"    except Exception:\n",
					"        \n",
					"        target_exists = 0\n",
					"        error_message = f\"records count sql query failed: {str(e)[:800]}\"\n",
					"        logInfo(\"Target table is empty or does not exist - performing initial load\")\n",
					"        "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Performing change analysis\")\n",
					"        \n",
					"        if target_exists > 0:\n",
					"            # Get current active records from target\n",
					"            spark.sql(f\"\"\"\n",
					"                CREATE OR REPLACE TEMP VIEW current_active AS\n",
					"                SELECT casework_specialist_id, greenCaseId, RowID as data_hash\n",
					"                FROM {spark_table_final} \n",
					"                WHERE is_current = 1\n",
					"            \"\"\")\n",
					"            \n",
					"            # Analyze changes: NEW, CHANGED, UNCHANGED\n",
					"            spark.sql(\"\"\"\n",
					"                CREATE OR REPLACE TEMP VIEW change_analysis AS\n",
					"                SELECT \n",
					"                    src.greenCaseId, \n",
					"                    tgt.casework_specialist_id,\n",
					"                    CASE \n",
					"                        WHEN tgt.greenCaseId IS NULL THEN 'NEW'\n",
					"                        WHEN src.data_hash != tgt.data_hash THEN 'CHANGED'\n",
					"                        ELSE 'UNCHANGED'\n",
					"                    END as change_type\n",
					"                FROM stg_casework_specialist src\n",
					"                LEFT JOIN current_active tgt ON src.greenCaseId = tgt.greenCaseId\n",
					"            \"\"\")\n",
					"            \n",
					"            # Count changes by type\n",
					"            for row in spark.sql(\"SELECT change_type, COUNT(*) as cnt FROM change_analysis GROUP BY change_type\").collect():\n",
					"                if row['change_type'] == 'NEW': \n",
					"                    new_records = row['cnt']\n",
					"                elif row['change_type'] == 'CHANGED': \n",
					"                    changed_records = row['cnt']\n",
					"                elif row['change_type'] == 'UNCHANGED': \n",
					"                    unchanged_records = row['cnt']\n",
					"            \n",
					"            # Count deletions (records in target but not in source)\n",
					"            delete_count = spark.sql(\"\"\"\n",
					"                SELECT COUNT(*) FROM current_active tgt\n",
					"                LEFT JOIN stg_casework_specialist src ON tgt.greenCaseId = src.greenCaseId\n",
					"                WHERE src.greenCaseId IS NULL\n",
					"            \"\"\").collect()[0][0]\n",
					"            \n",
					"            logInfo(f\"Change analysis - New: {new_records}, Changed: {changed_records}, Unchanged: {unchanged_records}, Deleted: {delete_count}\")\n",
					"            \n",
					"        else:\n",
					"            # Initial load - all records are NEW\n",
					"            new_records = src_count\n",
					"            changed_records = 0\n",
					"            unchanged_records = 0\n",
					"            delete_count = 0\n",
					"            logInfo(f\"Initial load - All {new_records} records are NEW\")\n",
					"            \n",
					"            # Create change_analysis view for initial load\n",
					"            spark.sql(\"\"\"\n",
					"                CREATE OR REPLACE TEMP VIEW change_analysis AS\n",
					"                SELECT \n",
					"                    greenCaseId,\n",
					"                    CAST(NULL AS BIGINT) as casework_specialist_id,\n",
					"                    'NEW' as change_type\n",
					"                FROM stg_casework_specialist\n",
					"            \"\"\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in change analysis: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# STEP 4: Close Changed/Deleted Records\n",
					"\n",
					"\n",
					"if not error_message and (changed_records > 0 or delete_count > 0):\n",
					"    try:\n",
					"        update_count = changed_records + delete_count\n",
					"        logInfo(f\"Closing {update_count} records (changed: {changed_records}, deleted: {delete_count})\")\n",
					"        \n",
					"        # Create temp view with records to close\n",
					"        spark.sql(\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW records_to_close AS\n",
					"            SELECT DISTINCT greenCaseId\n",
					"            FROM (\n",
					"                SELECT greenCaseId FROM change_analysis WHERE change_type = 'CHANGED'\n",
					"                UNION\n",
					"                SELECT tgt.greenCaseId \n",
					"                FROM current_active tgt\n",
					"                LEFT JOIN stg_casework_specialist src ON tgt.greenCaseId = src.greenCaseId\n",
					"                WHERE src.greenCaseId IS NULL\n",
					"            )\n",
					"        \"\"\")\n",
					"        \n",
					"        # Use MERGE to close records\n",
					"        spark.sql(f\"\"\"\n",
					"            MERGE INTO {spark_table_final} AS target\n",
					"            USING records_to_close AS source\n",
					"            ON target.greenCaseId = source.greenCaseId \n",
					"               AND target.is_current = 1\n",
					"            WHEN MATCHED THEN\n",
					"                UPDATE SET \n",
					"                    record_end_date = DATE_SUB(CURRENT_DATE(), 1),\n",
					"                    is_current = 0\n",
					"        \"\"\")\n",
					"        \n",
					"        logInfo(f\"Successfully closed {update_count} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error closing records: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# STEP 5: Insert New and Changed Records\n",
					"\n",
					"\n",
					"if not error_message and (new_records > 0 or changed_records > 0):\n",
					"    try:\n",
					"        logInfo(f\"Inserting {new_records + changed_records} records (new: {new_records}, changed: {changed_records})\")\n",
					"        \n",
					"        # Get max ID for new record assignment\n",
					"        max_id = spark.sql(f\"SELECT COALESCE(MAX(casework_specialist_id), 0) FROM {spark_table_final}\").collect()[0][0]\n",
					"        \n",
					"        # Insert new versions of changed records + completely new records\n",
					"        spark.sql(f\"\"\"\n",
					"            INSERT INTO {spark_table_final} (\n",
					"                casework_specialist_id,\n",
					"                greenCaseType,\n",
					"                greenCaseId,\n",
					"                caseReference,\n",
					"                horizonId,\n",
					"                linkedGreenCaseId,\n",
					"                caseOfficerName,\n",
					"                caseOfficerEmail,\n",
					"                appealType,\n",
					"                procedure,\n",
					"                processingState,\n",
					"                pinsLpaCode,\n",
					"                pinsLpaName,\n",
					"                appellantName,\n",
					"                agentName,\n",
					"                siteAddressDescription,\n",
					"                sitePostcode,\n",
					"                otherPartyName,\n",
					"                receiptDate,\n",
					"                validDate,\n",
					"                startDate,\n",
					"                lpaQuestionnaireDue,\n",
					"                lpaQuestionnaireReceived,\n",
					"                week6Date,\n",
					"                week8Date,\n",
					"                week9Date,\n",
					"                eventType,\n",
					"                eventDate,\n",
					"                eventTime,\n",
					"                inspectorName,\n",
					"                inspectorStaffNumber,\n",
					"                decision,\n",
					"                decisionDate,\n",
					"                withdrawnOrTurnedAwayDate,\n",
					"                comments,\n",
					"                Migrated,\n",
					"                IngestionDate,\n",
					"                is_current,\n",
					"                record_start_date,\n",
					"                record_end_date,\n",
					"                RowID,\n",
					"                active\n",
					"            )\n",
					"            SELECT\n",
					"                CASE \n",
					"                    WHEN ca.change_type = 'CHANGED' THEN ca.casework_specialist_id\n",
					"                    ELSE ROW_NUMBER() OVER (ORDER BY stg.greenCaseId) + {max_id}\n",
					"                END AS casework_specialist_id,\n",
					"                stg.greenCaseType,\n",
					"                stg.greenCaseId,\n",
					"                stg.caseReference,\n",
					"                stg.horizonId,\n",
					"                stg.linkedGreenCaseId,\n",
					"                stg.caseOfficerName,\n",
					"                stg.caseOfficerEmail,\n",
					"                stg.appealType,\n",
					"                stg.procedure,\n",
					"                stg.processingState,\n",
					"                stg.pinsLpaCode,\n",
					"                stg.pinsLpaName,\n",
					"                stg.appellantName,\n",
					"                stg.agentName,\n",
					"                stg.siteAddressDescription,\n",
					"                stg.sitePostcode,\n",
					"                stg.otherPartyName,\n",
					"                stg.receiptDate,\n",
					"                stg.validDate,\n",
					"                stg.startDate,\n",
					"                stg.lpaQuestionnaireDue,\n",
					"                stg.lpaQuestionnaireReceived,\n",
					"                stg.week6Date,\n",
					"                stg.week8Date,\n",
					"                stg.week9Date,\n",
					"                stg.eventType,\n",
					"                stg.eventDate,\n",
					"                stg.eventTime,\n",
					"                stg.inspectorName,\n",
					"                stg.inspectorStaffNumber,\n",
					"                stg.decision,\n",
					"                stg.decisionDate,\n",
					"                stg.withdrawnOrTurnedAwayDate,\n",
					"                stg.comments,\n",
					"                stg.Migrated,\n",
					"                CURRENT_TIMESTAMP() AS IngestionDate,\n",
					"                1 AS is_current,\n",
					"                CURRENT_TIMESTAMP() AS record_start_date,\n",
					"                CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS record_end_date,\n",
					"                stg.data_hash AS RowID,\n",
					"                stg.active\n",
					"            FROM stg_casework_specialist stg\n",
					"            INNER JOIN change_analysis ca \n",
					"                ON stg.greenCaseId = ca.greenCaseId \n",
					"                AND ca.change_type IN ('NEW', 'CHANGED')\n",
					"        \"\"\")\n",
					"        \n",
					"        insert_count = new_records + changed_records\n",
					"        logInfo(f\"Successfully inserted {insert_count} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error inserting records: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# STEP 6: Final Statistics and Telemetry\n",
					"\n",
					"\n",
					"if not error_message:\n",
					"    try:\n",
					"        # Final statistics\n",
					"        total = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final}\").collect()[0][0]\n",
					"        active = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final} WHERE is_current = 1\").collect()[0][0]\n",
					"        hist = spark.sql(f\"SELECT COUNT(*) FROM {spark_table_final} WHERE is_current = 0\").collect()[0][0]\n",
					"        \n",
					"        logInfo(f\"SCD Type 2 completed - Total: {total}, Active: {active}, Historical: {hist}\")\n",
					"        logInfo(f\"Changes - New: {new_records}, Changed: {changed_records}, Deleted: {delete_count}, Unchanged: {unchanged_records}\")\n",
					"        \n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in final stats: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					"\n",
					"# Telemetry\n",
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"SCD Type 2 complete (New: {new_records}, Changed: {changed_records}, Deleted: {delete_count})\"\n",
					"    if not error_message \n",
					"    else f\"SCD Type 2 failed for {spark_table_final}\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					"\n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    spark_table_final,\n",
					"    insert_count,\n",
					"    update_count,\n",
					"    delete_count,\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")"
				],
				"execution_count": null
			}
		]
	}
}