{
	"name": "s62a_view_cases_dim_Temp",
	"properties": {
		"folder": {
			"name": "archive/odw-harmonised/s62a_casework"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "97c5b81e-309f-43b5-971d-d4bd31e513e5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import asyncio\n",
					"import nest_asyncio\n",
					"import tracemalloc\n",
					"tracemalloc.start()\n",
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os\n",
					"import re\n",
					"from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat,max as spark_max\n",
					"from pyspark.sql.types import StringType, DateType, TimestampType, IntegerType, FloatType, StructType, StructField\n",
					"from delta import DeltaTable"
				],
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 51
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialize Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_utils_common_logging_output"
				],
				"execution_count": 52
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from datetime import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"# Define variables\n",
					"target_table = \"odw_harmonised_db.s62a_view_cases_dim\"\n",
					"\n",
					"# Initialize tracking variables\n",
					"start_exec_time = str(datetime.now())\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0"
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize Application Insights logger\n",
					"app_insight_logger = ProcessingLogger()"
				],
				"execution_count": 54
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"The below cell is creating the table if it does not exist"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Process s62a_view_cases_dim data with App Insights logging\n",
					"try:\n",
					"    logInfo('Starting s62a_view_cases_dim processing')\n",
					"    \n",
					"    db_name: str = 'odw_harmonised_db'\n",
					"    table_name: str = 's62a_view_cases_dim'\n",
					"    \n",
					"    def test_table_exists(db_name: str, table_name: str) -> bool:\n",
					"        spark.sql(f\"USE {db_name}\")\n",
					"        tables_df: DataFrame = spark.sql(\"SHOW TABLES\")\n",
					"        table_names: list = [row['tableName'] for row in tables_df.collect()]\n",
					"        return table_name in table_names\n",
					"    \n",
					"    if test_table_exists(db_name, table_name):\n",
					"        logInfo(f'Table {db_name}.{table_name} exists in harmonised, updating the harmonised layer')\n",
					"    else:\n",
					"        logInfo(f'Table {db_name}.{table_name} does not exist, creating table first.')\n",
					"        mssparkutils.notebook.run('/py_odw_harmonised_table_creation',300,{'specific_table': table_name } )\n",
					"        logInfo(f'Table {db_name}.{table_name} created')\n",
					"        \n",
					"except Exception as e:\n",
					"    logError(f'Error in table existence check: {str(e)}')\n",
					"    raise"
				],
				"execution_count": 55
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"def create_table_from_schema(jsonschema: str, db_name: str, table_name: str, target_container: str, target_folder: str, change_data_feed=False):\n",
					"   \n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    target_loc = f\"{target_container}/{target_folder}\".rstrip(\"/\")\n",
					"\n",
					"    # Convert array fields to string in schema definition\n",
					"    schema_dict = json.loads(jsonschema)\n",
					"    for field in schema_dict.get(\"fields\", []):\n",
					"        if field.get(\"type\") == \"array\":\n",
					"            field[\"type\"] = \"string\"\n",
					"\n",
					"    # Create empty DataFrame with adjusted schema\n",
					"    schema = StructType.fromJson(schema_dict)\n",
					"    df = spark.createDataFrame([], schema)\n",
					"\n",
					"    # Verify database existence\n",
					"    if db_name not in [db.name for db in spark.catalog.listDatabases()]:\n",
					"        raise NameError(f\"Database '{db_name}' does not exist!\")\n",
					"\n",
					"    # Verify table existence\n",
					"    if table_name in [table.name for table in spark.catalog.listTables(db_name)]:\n",
					"        table_details = spark.sql(f\"DESCRIBE DETAIL {db_name}.{table_name}\").toPandas()\n",
					"        \n",
					"        if len(table_details) > 1:\n",
					"            raise logError(\"Multiple parquet files detected. Please investigate!\")\n",
					"        \n",
					"        existing_location = table_details['location'][0].rstrip(\"/\")\n",
					"        if existing_location == target_loc:\n",
					"            logInfo(f\"Table '{db_name}.{table_name}' already exists at correct location.\")\n",
					"        else:\n",
					"            raise logError(f\"Table exists but at a different location: {existing_location}. Expected: {target_loc}\")\n",
					"        return\n",
					"\n",
					"    # Create table if not exists\n",
					"    if not DeltaTable.isDeltaTable(spark, target_loc):\n",
					"        df.write.option(\"mergeSchema\", \"false\").format(\"delta\").save(target_loc)\n",
					"    \n",
					"    logInfo(f\"Creating table {db_name}.{table_name}\")\n",
					"    spark.sql(f\"CREATE TABLE {db_name}.{table_name} USING DELTA LOCATION '{target_loc}'\")\n",
					"    \n",
					"    if change_data_feed:\n",
					"        spark.sql(f\"ALTER TABLE {db_name}.{table_name} SET TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\")\n",
					"    \n",
					"    logInfo(f\"Table '{db_name}.{table_name}' created successfully!\")"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"async def ingest_horizon(date_folder: str, file: str):\n",
					"    try:\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"        if date_folder == '':\n",
					"            date_folder = datetime.now().date()\n",
					"        else:\n",
					"            date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\")\n",
					"\n",
					"        date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"\n",
					"        # READ ORCHESTRATION DATA\n",
					"        path_to_orchestration_file = f\"abfss://odw-config@{storage_account}/orchestration/orchestration.json\"\n",
					"        df = spark.read.option(\"multiline\", \"true\").json(path_to_orchestration_file)\n",
					"        definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"        definition = next((d for d in definitions \n",
					"                          if (specific_file == '' or d['Source_Filename_Start'] == specific_file) \n",
					"                          and file.startswith(d['Source_Filename_Start']) \n",
					"                          and d['Load_Enable_status'] == 'True'), None)\n",
					"\n",
					"        if definition:\n",
					"            expected_from = date_folder - timedelta(days=1)\n",
					"            expected_from = datetime.combine(expected_from, datetime.min.time())\n",
					"            expected_to = expected_from + timedelta(days=definition['Expected_Within_Weekdays'])\n",
					"\n",
					"            if delete_existing_table:\n",
					"                print(f\"Deleting existing table if exists odw_standardised_db.{definition['Standardised_Table_Name']}\")\n",
					"                mssparkutils.notebook.run('/utils/py_delete_table', 300, arguments={'db_name': 'odw_standardised_db', 'table_name': definition['Standardised_Table_Name']})\n",
					"\n",
					"            ingestion_failure, row_count = ingest_adhoc(storage_account, definition, source_path, file, expected_from, expected_to)\n",
					"\n",
					"            if ingestion_failure:\n",
					"                print(\"Errors reported during Ingestion!!\")\n",
					"\n",
					"                #logging and Tracking\n",
					"                table_name = f\"odw_standardised_db.{definition['Standardised_Table_Name']}\"\n",
					"                error_message = \"Errors reported during Ingestion!!\"                \n",
					"                end_exec_time = str(datetime.now())\n",
					"                app_insight_logger.add_table_result(\n",
					"                    delta_table_name = table_name,\n",
					"                    insert_count = insert_count, \n",
					"                    update_count = update_count, \n",
					"                    delete_count = delete_count, \n",
					"                    table_result = \"failed\",\n",
					"                    start_exec_time = start_exec_time, \n",
					"                    end_exec_time = end_exec_time,\n",
					"                    total_exec_time = \"\",\n",
					"                    error_message = error_message\n",
					"                )\n",
					"                # Exit with the JSON result\n",
					"                mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())    \n",
					"                #raise RuntimeError(\"Ingestion Failure\")\n",
					"\n",
					"            else:\n",
					"                print(\"No Errors reported during Ingestion\")\n",
					"        else:\n",
					"            if specific_file != '':\n",
					"                raise logError(f\"No definition found for {file}\")\n",
					"            else:\n",
					"                print(f\"Condition Not Satisfied for Load {file} File\")\n",
					"    except Exception as e:\n",
					"        print(f\"Failed to process {file}: {e}\")"
				],
				"execution_count": 57
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check for new, updated or deleted data\n",
					"- This script checks for new, updated or deleted data by checking the source data (horizon tables) against the target (odw_harmonised_db.casework tables)\n",
					"- **New Data:** where an main Reference in the source does not exist in the target, then NewData flag is set to 'Y'\n",
					"- **Updated data:** Comparison occurs on Reference Fields in source and in target where the row hash is different i.e. there is a change in one of the columns. NewData flag is set to 'Y'\n",
					"- **Deleted data:** where an Reference info in the target exists but the same identifyers don't exist in the source. DeletedData flag is set to 'Y'\n",
					"\n",
					"## View s62a_view_cases_dim_new is created"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Build s62a_view_cases_dim_new table\n",
					"-- Gets modified or deleted from source rows\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW s62a_view_cases_dim_new \n",
					"\n",
					"     AS\n",
					"\n",
					"-- gets data that matches of SourceID and flags that it is modified based on a row (md5) hash. Flags as \"NewData\"\n",
					"-- gets data that is in the target but not in source. Flags as \"DeletedData\"\n",
					"\n",
					"SELECT DISTINCT\n",
					"    CASE\n",
					"        WHEN T1.CaseDataID IS NULL\n",
					"        THEN T2.S62AViewCasesId\n",
					"        ELSE NULL\n",
					"    END                             AS S62AViewCasesId,\n",
					"    T1.Name                         AS Name,\n",
					"    T1.CaseRef                      AS CaseReference,\n",
					"    T1.CommentsDataID               AS CommentsDataID,\n",
					"    T1.CaseDataID                   AS CaseDataID,\n",
					"    T1.Description                  AS Description,\n",
					"    \"0\"                             AS Migrated,\n",
					"    \"Casework\"                      AS ODTSourceSystem,\n",
					"    T3.SourceSystemID               AS SourceSystemID,\n",
					"    to_timestamp(T1.expected_from)  AS IngestionDate,\n",
					"    NULL                            AS ValidTo,\n",
					"    md5(\n",
					"        concat(\n",
					"            IFNULL(T1.Name,'.'),\n",
					"            IFNULL(T1.CaseReference,'.'),\n",
					"            IFNULL(T1.CommentsDataID,'.'),\n",
					"            IFNULL(T1.CaseDataID,'.'),\n",
					"            IFNULL(T1.Description,'.')\n",
					"        ))                          AS RowID, -- this hash should contain all the defining fields\n",
					"    'Y'                             AS IsActive,\n",
					"    T2.IsActive                     AS HistoricIsActive\n",
					"\n",
					"FROM odw_standardised_db.horizon_s62a_view_cases T1\n",
					"LEFT JOIN odw_harmonised_db.main_sourcesystem_fact T3 ON \"Casework\" = T3.Description AND T3.IsActive = 'Y'\n",
					"FULL JOIN odw_harmonised_db.s62a_view_cases_dim T2 ON T1.CaseDataID = T2.CaseDataID AND T2.IsActive = 'Y'\n",
					"WHERE\n",
					"    -- flags new data        \n",
					"    (CASE\n",
					"        WHEN T1.CaseDataID = T2.CaseDataID AND md5(\n",
					"            concat(\n",
					"                IFNULL(T1.Name,'.'),\n",
					"                IFNULL(T1.CaseReference,'.'),\n",
					"                IFNULL(T1.CommentsDataID,'.'),\n",
					"                IFNULL(T1.CaseDataID,'.'),\n",
					"                IFNULL(T1.Description,'.')\n",
					"            )) <> T2.RowID  -- same row, changed data\n",
					"        THEN 'Y'\n",
					"        WHEN T2.CaseDataID IS NULL -- new data\n",
					"        THEN 'Y'\n",
					"    ELSE 'N'\n",
					"    END  = 'Y')\n",
					"    AND T1.CaseDataID IS NOT NULL\n",
					"    AND T1.expected_from = (SELECT MAX(expected_from) FROM odw_standardised_db.horizon_s62a_view_cases)\n",
					";"
				],
				"execution_count": 58
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Dataset is created that contains changed data and corresponding target data\n",
					"- This script combines data that has been updated, Deleted or is new, with corresponding target data\n",
					"- View **s62a_view_cases_dim_new** is unioned to the target data filter to only those rows where changes have been detected\n",
					"## View s62a_view_cases_dim_changed_rows is created"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Create new and updated dataset\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW s62a_view_cases_dim_changed_rows\n",
					"\n",
					"    AS\n",
					"\n",
					"-- gets updated, deleted and new rows \n",
					"Select \n",
					"    S62AViewCasesId,\n",
					"    Name,\n",
					"    CaseReference,\n",
					"    CommentsDataID,\n",
					"    CaseDataID,\n",
					"    Description,\n",
					"    Migrated,\n",
					"    ODTSourceSystem,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"\n",
					"From s62a_view_cases_dim_new WHERE HistoricIsActive = 'Y' or HistoricIsActive IS NULL\n",
					"\n",
					"    UNION ALL\n",
					"\n",
					"-- gets original versions of updated rows so we can update EndDate and set IsActive flag to 'N'\n",
					"SELECT\n",
					"    S62AViewCasesId,\n",
					"    Name,\n",
					"    CaseReference,\n",
					"    CommentsDataID,\n",
					"    CaseDataID,\n",
					"    Description,\n",
					"    Migrated,\n",
					"    ODTSourceSystem,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"    \n",
					"FROM odw_harmonised_db.s62a_view_cases_dim\n",
					"WHERE CaseDataID IN (SELECT CaseDataID FROM s62a_view_cases_dim_new WHERE CaseDataID IS NULL) AND IsActive = 'Y'; "
				],
				"execution_count": 59
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW Loading_month\n",
					"\n",
					"    AS\n",
					"\n",
					"SELECT DISTINCT\n",
					"    IngestionDate AS IngestionDate,\n",
					"    to_timestamp(date_sub(IngestionDate,1)) AS ClosingDate,\n",
					"    'Y' AS IsActive\n",
					"\n",
					"FROM s62a_view_cases_dim_new;\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW s62a_view_cases_dim_changed_rows_final\n",
					"\n",
					"    AS\n",
					"\n",
					"SELECT \n",
					"    S62AViewCasesId,\n",
					"    Name,\n",
					"    CaseReference,\n",
					"    CommentsDataID,\n",
					"    CaseDataID,\n",
					"    Description,\n",
					"    T1.Migrated,\n",
					"    T1.ODTSourceSystem,\n",
					"    T1.SourceSystemID,\n",
					"    T1.IngestionDate,\n",
					"    T1.ValidTo,\n",
					"    T1.RowID,\n",
					"    T1.IsActive,\n",
					"    T2.ClosingDate\n",
					"FROM s62a_view_cases_dim_changed_rows T1\n",
					"FULL JOIN Loading_month T2 ON T1.IsActive = T2.IsActive"
				],
				"execution_count": 60
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# View s62a_view_cases_dim_changed_rows is used in a merge (Upsert) statement into the target table\n",
					"- **WHEN MATCHED** ON the surrogate Key (i.e. Employees62a_view_cases_dim), EndDate is set to today -1 day and the IsActive flag is set to 'N'\n",
					"- **WHEN NOT MATCHED** ON the surrogate Key, insert rows\n",
					"## Table odw_harmonised_db.s62a_view_cases_dim is updated"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- merge into dim table\n",
					"MERGE INTO odw_harmonised_db.s62a_view_cases_dim AS Target\n",
					"USING s62a_view_cases_dim_changed_rows_final AS Source\n",
					"\n",
					"ON Source.CaseDataID = Target.CaseDataID AND Target.IsActive = 'Y'\n",
					"\n",
					"-- For Updates existing rows\n",
					"\n",
					"WHEN MATCHED\n",
					"    THEN \n",
					"    UPDATE SET\n",
					"    Target.ValidTo = to_timestamp(ClosingDate),\n",
					"    Target.IsActive = 'N'\n",
					"\n",
					"-- Insert completely new rows\n",
					"\n",
					"WHEN NOT MATCHED \n",
					"    THEN INSERT (\n",
					"        S62AViewCasesId,\n",
					"        Name,\n",
					"        CaseReference,\n",
					"        CommentsDataID,\n",
					"        CaseDataID,\n",
					"        Description,\n",
					"        Migrated,\n",
					"        ODTSourceSystem,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive)\n",
					"    VALUES (\n",
					"        Source.S62AViewCasesId,\n",
					"        Source.Name,\n",
					"        Source.CaseReference,\n",
					"        Source.CommentsDataID,\n",
					"        Source.CaseDataID,\n",
					"        Source.Description,\n",
					"        Source.Migrated,\n",
					"        Source.ODTSourceSystem,\n",
					"        Source.SourceSystemID,\n",
					"        Source.IngestionDate,\n",
					"        Source.ValidTo,\n",
					"        Source.RowID,\n",
					"        Source.IsActive)\n",
					"     ;   "
				],
				"execution_count": 61
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Fix the IDs\n",
					"- No auto-increment feature is available in delta tables, therefore we need to create new IDs for the inserted rows\n",
					"- This is done by select the target data and using INSERT OVERWRITE to re-insert the data is a new Row Number\n",
					"## Table odw_harmonised_db.s62a_view_cases_dim is updated"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Insert new s62a_view_cases_dim\n",
					"\n",
					"INSERT OVERWRITE odw_harmonised_db.s62a_view_cases_dim\n",
					"\n",
					"SELECT \n",
					"    ROW_NUMBER() OVER (ORDER BY S62AViewCasesId NULLS LAST) AS S62AViewCasesId,\n",
					"    Name,\n",
					"    CaseReference,\n",
					"    CommentsDataID,\n",
					"    CaseDataID,\n",
					"    Description,\n",
					"    Migrated,\n",
					"    ODTSourceSystem,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"    \n",
					"FROM odw_harmonised_db.s62a_view_cases_dim;\n",
					""
				],
				"execution_count": 62
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Processing summary cell"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Count records processed and log results\n",
					"try:\n",
					"    # Get row counts for logging\n",
					"    changed_rows_df = spark.sql(\"SELECT COUNT(*) as count FROM s62a_view_cases_dim_changed_rows_final\")\n",
					"    changed_count = changed_rows_df.collect()[0]['count']\n",
					"    \n",
					"    final_table_df = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.s62a_view_cases_dim WHERE IsActive = 'Y'\")\n",
					"    final_count = final_table_df.collect()[0]['count']\n",
					"    \n",
					"    logInfo(f's62a_view_cases_dim processing completed successfully. Processed {changed_count} changed rows. Final active record count: {final_count}')\n",
					"    \n",
					"    # Calculate execution duration\n",
					"    end_exec_time = str(datetime.now())\n",
					"    execution_duration = (datetime.strptime(end_exec_time[:19], '%Y-%m-%d %H:%M:%S') - \n",
					"                         datetime.strptime(start_exec_time[:19], '%Y-%m-%d %H:%M:%S')).total_seconds()\n",
					"    \n",
					"    # Log successful processing with App Insights structure\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=changed_count,\n",
					"        update_count=0,  # Combined with insert in this process\n",
					"        delete_count=0,  # Handled via IsActive flag\n",
					"        table_result=\"success\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        total_exec_time=execution_duration,\n",
					"        error_message=\"\"\n",
					"    )\n",
					"        \n",
					"except Exception as processing_error:\n",
					"    error_message = f'Error processing s62a_view_cases_dim data: {str(processing_error)}'\n",
					"    logError(error_message)\n",
					"    \n",
					"    # Calculate execution duration for error case\n",
					"    end_exec_time = str(datetime.now())\n",
					"    execution_duration = (datetime.strptime(end_exec_time[:19], '%Y-%m-%d %H:%M:%S') - \n",
					"                         datetime.strptime(start_exec_time[:19], '%Y-%m-%d %H:%M:%S')).total_seconds()\n",
					"    \n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=0,\n",
					"        update_count=0,\n",
					"        delete_count=0,\n",
					"        table_result=\"failed\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        total_exec_time=execution_duration,\n",
					"        error_message=error_message\n",
					"    )\n",
					"    raise\n",
					"\n",
					"# Generate and return processing results for notebook exit\n",
					"processing_results = app_insight_logger.generate_processing_results()\n",
					"mssparkutils.notebook.exit(processing_results)"
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"async def finalize_s62a_processing(target_table: str, start_exec_time: str, app_insight_logger, spark):\n",
					"    \"\"\"\n",
					"    Finalize s62a_view_cases_dim processing with comprehensive logging and error handling\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Count records processed and log results\n",
					"        try:\n",
					"            # Get row counts for logging\n",
					"            changed_rows_df = spark.sql(\"SELECT COUNT(*) as count FROM s62a_view_cases_dim_changed_rows_final\")\n",
					"            changed_count = changed_rows_df.collect()[0]['count']\n",
					"            \n",
					"            final_table_df = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.s62a_view_cases_dim WHERE IsActive = 'Y'\")\n",
					"            final_count = final_table_df.collect()[0]['count']\n",
					"            \n",
					"            logInfo(f's62a_view_cases_dim processing completed successfully. Processed {changed_count} changed rows. Final active record count: {final_count}')\n",
					"            \n",
					"            # Calculate execution duration\n",
					"            end_exec_time = str(datetime.now())\n",
					"            execution_duration = (datetime.strptime(end_exec_time[:19], '%Y-%m-%d %H:%M:%S') - \n",
					"                                 datetime.strptime(start_exec_time[:19], '%Y-%m-%d %H:%M:%S')).total_seconds()\n",
					"            \n",
					"            # Log successful processing with App Insights structure\n",
					"            app_insight_logger.add_table_result(\n",
					"                delta_table_name=target_table,\n",
					"                insert_count=changed_count,\n",
					"                update_count=0,  # Combined with insert in this process\n",
					"                delete_count=0,  # Handled via IsActive flag\n",
					"                table_result=\"success\",\n",
					"                start_exec_time=start_exec_time,\n",
					"                end_exec_time=end_exec_time,\n",
					"                total_exec_time=execution_duration,\n",
					"                error_message=\"\"\n",
					"            )\n",
					"            \n",
					"            # Return success indicator\n",
					"            return True, changed_count, final_count\n",
					"                \n",
					"        except Exception as processing_error:\n",
					"            error_message = f'Error processing s62a_view_cases_dim data: {str(processing_error)}'\n",
					"            logError(error_message)\n",
					"            \n",
					"            # Calculate execution duration for error case\n",
					"            end_exec_time = str(datetime.now())\n",
					"            execution_duration = (datetime.strptime(end_exec_time[:19], '%Y-%m-%d %H:%M:%S') - \n",
					"                                 datetime.strptime(start_exec_time[:19], '%Y-%m-%d %H:%M:%S')).total_seconds()\n",
					"            \n",
					"            app_insight_logger.add_table_result(\n",
					"                delta_table_name=target_table,\n",
					"                insert_count=0,\n",
					"                update_count=0,\n",
					"                delete_count=0,\n",
					"                table_result=\"failed\",\n",
					"                start_exec_time=start_exec_time,\n",
					"                end_exec_time=end_exec_time,\n",
					"                total_exec_time=execution_duration,\n",
					"                error_message=error_message\n",
					"            )\n",
					"            \n",
					"            # Generate and return processing results for notebook exit\n",
					"            processing_results = app_insight_logger.generate_processing_results()\n",
					"            mssparkutils.notebook.exit(processing_results)\n",
					"            \n",
					"            # Return failure indicator\n",
					"            return False, 0, 0\n",
					"            \n",
					"    except Exception as e:\n",
					"        # Handle any unexpected errors at the function level\n",
					"        error_message = f\"Failed to finalize s62a processing: {str(e)}\"\n",
					"        logError(error_message)\n",
					"        \n",
					"        # Calculate execution duration for unexpected error case\n",
					"        end_exec_time = str(datetime.now())\n",
					"        execution_duration = (datetime.strptime(end_exec_time[:19], '%Y-%m-%d %H:%M:%S') - \n",
					"                             datetime.strptime(start_exec_time[:19], '%Y-%m-%d %H:%M:%S')).total_seconds()\n",
					"        \n",
					"        app_insight_logger.add_table_result(\n",
					"            delta_table_name=target_table,\n",
					"            insert_count=0,\n",
					"            update_count=0,\n",
					"            delete_count=0,\n",
					"            table_result=\"failed\",\n",
					"            start_exec_time=start_exec_time,\n",
					"            end_exec_time=end_exec_time,\n",
					"            total_exec_time=execution_duration,\n",
					"            error_message=error_message\n",
					"        )\n",
					"        \n",
					"        # Generate and return processing results for notebook exit\n",
					"        processing_results = app_insight_logger.generate_processing_results()\n",
					"        \n",
					"        \n",
					"        # Re-raise the exception if needed\n",
					"        raise\n",
					""
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"source": [
					"mssparkutils.notebook.exit(processing_results)"
				],
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}