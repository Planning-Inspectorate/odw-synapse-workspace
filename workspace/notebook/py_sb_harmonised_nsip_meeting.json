{
	"name": "py_sb_harmonised_nsip_meeting",
	"properties": {
		"description": "SCD2 Full Load processing for nsip_meeting table with proper ValidTo handling",
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "524f3108-3df0-4cda-bf00-628e86bf0a1d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### SCD Type 2 Full Load Processing for NSIP Meetings\n",
					"\n",
					"**Description**\n",
					"This notebook handles full load from odw_harmonised_db.sb_nsip_project meetings array and applies SCD Type 2 logic by comparing against existing odw_harmonised_db.nsip_meeting target table. Changes are detected using hash comparison, and historical versions are maintained with proper ValidTo dates.\n",
					"\n",
					"**Key Features:**\n",
					"- Full load from source (not incremental)\n",
					"- meetingId as business key for SCD2\n",
					"- NSIPMeetingID as surrogate key\n",
					"- Hash-based change detection\n",
					"- Automatic IsActive and ValidTo management\n",
					"- ValidTo set for historical records (IsActive='N')\n",
					"- ValidTo = NULL for active records (IsActive='Y')\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version - 3.10, Delta Lake Version - 2.4\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Update all exceptions with appropriate comments"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Import required libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import IntegerType, ArrayType, StructType, StructField, StringType\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.window import Window\n",
					"from datetime import datetime, date"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define required delta tables and variables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"service_bus_table = \"odw_harmonised_db.sb_nsip_project\"\n",
					"spark_table_final = \"odw_harmonised_db.nsip_meeting\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message = ''"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Logging decorator"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Read full load from Service Bus and explode meetings array"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Full load from source\n",
					"        service_bus_data = spark.sql(f\"\"\"\n",
					"            SELECT \n",
					"                 NSIPProjectInfoInternalID\n",
					"                ,caseId\n",
					"                ,caseReference\n",
					"                ,meetings\n",
					"                ,Migrated\n",
					"                ,ODTSourceSystem\n",
					"                ,SourceSystemID\n",
					"                ,IngestionDate\n",
					"            FROM \n",
					"                {service_bus_table}\n",
					"            WHERE \n",
					"                meetings IS NOT NULL\n",
					"        \"\"\")\n",
					"\n",
					"        \n",
					"        # Explode meetings array\n",
					"        service_bus_exploded = service_bus_data.select(\n",
					"            col(\"NSIPProjectInfoInternalID\"),\n",
					"            col(\"caseId\"),\n",
					"            col(\"caseReference\"),\n",
					"            explode(col(\"meetings\")).alias(\"meeting\"),\n",
					"            col(\"Migrated\"),\n",
					"            col(\"ODTSourceSystem\"),\n",
					"            col(\"SourceSystemID\"),\n",
					"            col(\"IngestionDate\")\n",
					"        )\n",
					"        \n",
					"        # Extract meeting fields\n",
					"        source_df = service_bus_exploded.select(\n",
					"            col(\"NSIPProjectInfoInternalID\"),\n",
					"            col(\"caseId\"),\n",
					"            col(\"caseReference\"),\n",
					"            col(\"meeting.meetingId\").alias(\"meetingId\"),\n",
					"            col(\"meeting.meetingAgenda\").alias(\"meetingAgenda\"),\n",
					"            col(\"meeting.planningInspectorateRole\").alias(\"planningInspectorateRole\"),\n",
					"            col(\"meeting.meetingDate\").alias(\"meetingDate\"),\n",
					"            col(\"meeting.meetingType\").alias(\"meetingType\"),\n",
					"            col(\"Migrated\"),\n",
					"            col(\"ODTSourceSystem\"),\n",
					"            col(\"SourceSystemID\"),\n",
					"            col(\"IngestionDate\")\n",
					"        )\n",
					"        \n",
					"        # Add hash for change detection\n",
					"        source_df = source_df.withColumn(\n",
					"            \"SourceHash\",\n",
					"            md5(\n",
					"                concat_ws(\"|\",\n",
					"                    coalesce(col(\"NSIPProjectInfoInternalID\").cast(\"string\"), lit(\"\")),\n",
					"                    coalesce(col(\"caseId\").cast(\"string\"), lit(\"\")),\n",
					"                    coalesce(col(\"caseReference\").cast(\"string\"), lit(\"\")),\n",
					"                    coalesce(col(\"meetingAgenda\").cast(\"string\"), lit(\"\")),\n",
					"                    coalesce(col(\"planningInspectorateRole\").cast(\"string\"), lit(\"\")),\n",
					"                    coalesce(col(\"meetingDate\").cast(\"string\"), lit(\"\")),\n",
					"                    coalesce(col(\"meetingType\").cast(\"string\"), lit(\"\"))\n",
					"                )\n",
					"            )\n",
					"        )\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error reading source data: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Read existing target table for SCD2 comparison"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Check if target table exists\n",
					"        table_exists = spark.catalog.tableExists(spark_table_final)\n",
					"        \n",
					"        if table_exists:\n",
					"            # Read existing target data\n",
					"            target_df = spark.table(spark_table_final)\n",
					"\n",
					"            target_cols = target_df.columns\n",
					"\n",
					"            if \"NSIPProjectInfoInternalID\" not in target_cols:\n",
					"                target_df = target_df.withColumn(\"NSIPProjectInfoInternalID\", lit(\"\"))\n",
					"            \n",
					"            # Get only active records for comparison\n",
					"            target_active = target_df.filter(\"IsActive = 'Y'\")\n",
					"            \n",
					"            # Add hash for comparison\n",
					"            target_active = target_active.withColumn(\n",
					"                \"TargetHash\",\n",
					"                md5(\n",
					"                    concat_ws(\"|\",\n",
					"                        coalesce(col(\"NSIPProjectInfoInternalID\").cast(\"string\"), lit(\"\")),\n",
					"                        coalesce(col(\"caseId\").cast(\"string\"), lit(\"\")),\n",
					"                        coalesce(col(\"caseReference\").cast(\"string\"), lit(\"\")),\n",
					"                        coalesce(col(\"meetingAgenda\").cast(\"string\"), lit(\"\")),\n",
					"                        coalesce(col(\"planningInspectorateRole\").cast(\"string\"), lit(\"\")),\n",
					"                        coalesce(col(\"meetingDate\").cast(\"string\"), lit(\"\")),\n",
					"                        coalesce(col(\"meetingType\").cast(\"string\"), lit(\"\"))\n",
					"                    )\n",
					"                )\n",
					"            )\n",
					"            \n",
					"            # Get all historical records\n",
					"            target_historical = target_df.filter(\"IsActive = 'N'\")\n",
					"        else:\n",
					"            # First run - no existing data\n",
					"            target_active = None\n",
					"            target_historical = None\n",
					"            \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error reading target data: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Detect new and changed records with proper ValidTo dates for historical records"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        current_timestamp = datetime.now()\n",
					"        \n",
					"        if target_active is not None:\n",
					"            # Join source with active target to detect changes\n",
					"            # 1. Ensure both source and target have the same natural grain\n",
					"            key_cols = [\"meetingId\"]\n",
					"\n",
					"            # 2. Join on FULL key, not only meetingId\n",
					"            join_cond = [f\"src.{c} = tgt.{c}\" for c in key_cols]\n",
					"            comparison = source_df.alias(\"src\").join(\n",
					"                target_active.alias(\"tgt\"),\n",
					"                on=[col(f\"src.{c}\") == col(f\"tgt.{c}\") for c in key_cols],\n",
					"                how=\"full_outer\"\n",
					"            )\n",
					"            \n",
					"            # New records: meetingId exists in source but not in target\n",
					"            # IsActive='Y', ValidTo=NULL\n",
					"            new_records = comparison.filter(col(\"tgt.meetingId\").isNull()).select(\n",
					"                col(\"src.NSIPProjectInfoInternalID\"),\n",
					"                col(\"src.caseId\"),\n",
					"                col(\"src.caseReference\"),\n",
					"                col(\"src.meetingId\"),\n",
					"                col(\"src.meetingAgenda\"),\n",
					"                col(\"src.planningInspectorateRole\"),\n",
					"                col(\"src.meetingDate\"),\n",
					"                col(\"src.meetingType\"),\n",
					"                col(\"src.Migrated\"),\n",
					"                col(\"src.ODTSourceSystem\"),\n",
					"                col(\"src.SourceSystemID\"),\n",
					"                col(\"src.IngestionDate\"),\n",
					"                lit(None).cast(\"timestamp\").alias(\"ValidTo\"),\n",
					"                lit(\"Y\").alias(\"IsActive\")\n",
					"            )\n",
					"            \n",
					"            # Changed records: hash differs\n",
					"            # New version: IsActive='Y', ValidTo=NULL\n",
					"            changed_records = comparison.filter(\n",
					"                (col(\"tgt.meetingId\").isNotNull()) & \n",
					"                (col(\"src.SourceHash\") != col(\"tgt.TargetHash\"))\n",
					"            ).select(\n",
					"                col(\"src.NSIPProjectInfoInternalID\"),\n",
					"                col(\"src.caseId\"),\n",
					"                col(\"src.caseReference\"),\n",
					"                col(\"src.meetingId\"),\n",
					"                col(\"src.meetingAgenda\"),\n",
					"                col(\"src.planningInspectorateRole\"),\n",
					"                col(\"src.meetingDate\"),\n",
					"                col(\"src.meetingType\"),\n",
					"                col(\"src.Migrated\"),\n",
					"                col(\"src.ODTSourceSystem\"),\n",
					"                col(\"src.SourceSystemID\"),\n",
					"                col(\"src.IngestionDate\"),\n",
					"                lit(None).cast(\"timestamp\").alias(\"ValidTo\"),\n",
					"                lit(\"Y\").alias(\"IsActive\")\n",
					"            )\n",
					"            \n",
					"            # Expire old versions of changed records\n",
					"            # Old version: IsActive='N', ValidTo=current_timestamp (when new version created)\n",
					"            changed_meetingIds = changed_records.select(\"meetingId\").distinct()\n",
					"\n",
					"            changed_keys = changed_records.select(*key_cols).distinct()\n",
					"\n",
					"            expired_records = target_active.join(\n",
					"                changed_keys,\n",
					"                on=key_cols,\n",
					"                how=\"inner\"\n",
					"            ).withColumn(\"IsActive\", lit(\"N\")) \\\n",
					"             .withColumn(\"ValidTo\", lit(current_timestamp)) \\\n",
					"             .select(\n",
					"                \"NSIPProjectInfoInternalID\",\n",
					"                \"caseId\",\n",
					"                \"caseReference\",\n",
					"                \"meetingId\",\n",
					"                \"meetingAgenda\",\n",
					"                \"planningInspectorateRole\",\n",
					"                \"meetingDate\",\n",
					"                \"meetingType\",\n",
					"                \"Migrated\",\n",
					"                \"ODTSourceSystem\",\n",
					"                \"SourceSystemID\",\n",
					"                \"IngestionDate\",\n",
					"                \"ValidTo\",\n",
					"                \"IsActive\"\n",
					"            )\n",
					"            \n",
					"            # Unchanged active records\n",
					"            # Keep as-is: IsActive='Y', ValidTo=NULL\n",
					"            unchanged_records = comparison.filter(\n",
					"                (col(\"tgt.meetingId\").isNotNull()) & \n",
					"                (col(\"src.SourceHash\") == col(\"tgt.TargetHash\"))\n",
					"            ).select(\n",
					"                col(\"tgt.NSIPProjectInfoInternalID\"),\n",
					"                col(\"tgt.caseId\"),\n",
					"                col(\"tgt.caseReference\"),\n",
					"                col(\"tgt.meetingId\"),\n",
					"                col(\"tgt.meetingAgenda\"),\n",
					"                col(\"tgt.planningInspectorateRole\"),\n",
					"                col(\"tgt.meetingDate\"),\n",
					"                col(\"tgt.meetingType\"),\n",
					"                col(\"tgt.Migrated\"),\n",
					"                col(\"tgt.ODTSourceSystem\"),\n",
					"                col(\"tgt.SourceSystemID\"),\n",
					"                col(\"tgt.IngestionDate\"),\n",
					"                col(\"tgt.ValidTo\"),\n",
					"                col(\"tgt.IsActive\")\n",
					"            )\n",
					"            \n",
					"            # Combine all records\n",
					"            # Historical records already have ValidTo set from previous runs\n",
					"            if target_historical is not None:\n",
					"                combined_df = new_records.union(changed_records) \\\n",
					"                                         .union(expired_records) \\\n",
					"                                         .union(unchanged_records) \\\n",
					"                                         .union(target_historical.select(\n",
					"                                             \"NSIPProjectInfoInternalID\",\n",
					"                                             \"caseId\",\n",
					"                                             \"caseReference\",\n",
					"                                             \"meetingId\",\n",
					"                                             \"meetingAgenda\",\n",
					"                                             \"planningInspectorateRole\",\n",
					"                                             \"meetingDate\",\n",
					"                                             \"meetingType\",\n",
					"                                             \"Migrated\",\n",
					"                                             \"ODTSourceSystem\",\n",
					"                                             \"SourceSystemID\",\n",
					"                                             \"IngestionDate\",\n",
					"                                             \"ValidTo\",\n",
					"                                             \"IsActive\"\n",
					"                                         ))\n",
					"            else:\n",
					"                combined_df = new_records.union(changed_records) \\\n",
					"                                         .union(expired_records) \\\n",
					"                                         .union(unchanged_records)\n",
					"        else:\n",
					"            # First load - all records are new\n",
					"            # IsActive='Y', ValidTo=NULL\n",
					"            combined_df = source_df.select(\n",
					"                \"NSIPProjectInfoInternalID\",\n",
					"                \"caseId\",\n",
					"                \"caseReference\",\n",
					"                \"meetingId\",\n",
					"                \"meetingAgenda\",\n",
					"                \"planningInspectorateRole\",\n",
					"                \"meetingDate\",\n",
					"                \"meetingType\",\n",
					"                \"Migrated\",\n",
					"                \"ODTSourceSystem\",\n",
					"                \"SourceSystemID\",\n",
					"                \"IngestionDate\"\n",
					"            ).withColumn(\"ValidTo\", lit(None).cast(\"timestamp\")) \\\n",
					"             .withColumn(\"IsActive\", lit(\"Y\"))\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error detecting changes: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Generate NSIPMeetingID surrogate key and calculate RowID"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Generate NSIPMeetingID as incremental surrogate key\n",
					"        combined_df = combined_df.withColumn(\n",
					"            \"NSIPMeetingID\",\n",
					"            row_number().over(\n",
					"                Window.orderBy(\"IngestionDate\", \"meetingId\")\n",
					"            )\n",
					"        )\n",
					"        \n",
					"        \n",
					"        # Select final column order\n",
					"        final_df = combined_df.select(\n",
					"            \"NSIPMeetingID\",\n",
					"            \"NSIPProjectInfoInternalID\",\n",
					"            \"meetingId\",\n",
					"            \"caseId\",\n",
					"            \"caseReference\",\n",
					"            \"meetingAgenda\",\n",
					"            \"planningInspectorateRole\",\n",
					"            \"meetingDate\",\n",
					"            \"meetingType\",\n",
					"            \"Migrated\",\n",
					"            \"ODTSourceSystem\",\n",
					"            \"SourceSystemID\",\n",
					"            \"IngestionDate\",\n",
					"            \"ValidTo\",\n",
					"            \"IsActive\"\n",
					"        )\n",
					"        \n",
					"        final_df = final_df.dropDuplicates()\n",
					"        insert_count = final_df.count()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error generating surrogate keys: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"select * from odw_harmonised_db.nsip_meeting"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Write final SCD2 data to Delta table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        final_df.write.format(\"delta\") \\\n",
					"            .mode(\"Overwrite\") \\\n",
					"            .option(\"overwriteSchema\", \"true\") \\\n",
					"            .partitionBy(\"IsActive\") \\\n",
					"            .saveAsTable(f\"{spark_table_final}\")\n",
					"        \n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error writing to Delta table {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()\n",
					""
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Log telemetry and exit"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"Successfully loaded {insert_count} records into {spark_table_final} table with SCD2 logic\"\n",
					"    if not error_message\n",
					"    else f\"Failed to load data into {spark_table_final} table\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					"\n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    spark_table_final,\n",
					"    insert_count,\n",
					"    update_count,\n",
					"    delete_count,\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")\n",
					""
				],
				"execution_count": 13
			}
		]
	}
}