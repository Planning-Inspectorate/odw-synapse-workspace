{
	"name": "py_automated_script",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e266fea5-9019-4650-badc-670d661a9a06"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession, DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.types import *\n",
					"from datetime import datetime\n",
					"import json\n",
					"\n",
					"class DataQualityValidator:\n",
					"    \"\"\"\n",
					"    Automated Data Quality Validation Framework\n",
					"    \n",
					"    Usage:\n",
					"        validator = DataQualityValidator(spark)\n",
					"        results = validator.run_all_validations(\"database.table_name\")\n",
					"        validator.display_results(results)\n",
					"    \"\"\"\n",
					"    \n",
					"    def __init__(self, spark: SparkSession):\n",
					"        self.spark = spark\n",
					"        self.validation_results = []\n",
					"        \n",
					"    def run_all_validations(self, table_name: str, \n",
					"                           primary_keys: list = None,\n",
					"                           critical_columns: list = None,\n",
					"                           date_columns: list = None,\n",
					"                           numeric_columns: list = None):\n",
					"        \"\"\"\n",
					"        Run all validation checks on a table\n",
					"        \n",
					"        Args:\n",
					"            table_name: Full table name (e.g., 'database.table_name')\n",
					"            primary_keys: List of primary key columns\n",
					"            critical_columns: List of critical columns that shouldn't be null\n",
					"            date_columns: List of date columns to validate\n",
					"            numeric_columns: List of numeric columns to validate\n",
					"        \"\"\"\n",
					"        print(f\"{'='*80}\")\n",
					"        print(f\"Starting Data Quality Validation for: {table_name}\")\n",
					"        print(f\"Timestamp: {datetime.now()}\")\n",
					"        print(f\"{'='*80}\\n\")\n",
					"        \n",
					"        # Load table\n",
					"        df = self.spark.table(table_name)\n",
					"        \n",
					"        # Auto-detect column types if not provided\n",
					"        if date_columns is None:\n",
					"            date_columns = [field.name for field in df.schema.fields \n",
					"                          if isinstance(field.dataType, (DateType, TimestampType))]\n",
					"        \n",
					"        if numeric_columns is None:\n",
					"            numeric_columns = [field.name for field in df.schema.fields \n",
					"                             if isinstance(field.dataType, (IntegerType, LongType, \n",
					"                                                           FloatType, DoubleType, DecimalType))]\n",
					"        \n",
					"        # Run all validation checks\n",
					"        self.validation_results = []\n",
					"        \n",
					"        # 1. Basic Table Metrics\n",
					"        self._check_basic_metrics(df, table_name)\n",
					"        \n",
					"        # 2. Schema Validation\n",
					"        self._check_schema(df, table_name)\n",
					"        \n",
					"        # 3. Row Count and Duplicates\n",
					"        self._check_row_counts(df, table_name, primary_keys)\n",
					"        \n",
					"        # 4. Null Value Analysis\n",
					"        self._check_null_values(df, table_name, critical_columns)\n",
					"        \n",
					"        # 5. Data Type Validation\n",
					"        self._check_data_types(df, table_name)\n",
					"        \n",
					"        # 6. Completeness Check\n",
					"        self._check_completeness(df, table_name)\n",
					"        \n",
					"        # 7. Date Range Validation\n",
					"        if date_columns:\n",
					"            self._check_date_ranges(df, table_name, date_columns)\n",
					"        \n",
					"        # 8. Numeric Range Validation\n",
					"        if numeric_columns:\n",
					"            self._check_numeric_ranges(df, table_name, numeric_columns)\n",
					"        \n",
					"        # 9. Uniqueness Check\n",
					"        self._check_uniqueness(df, table_name)\n",
					"        \n",
					"        # 10. String Pattern Validation\n",
					"        self._check_string_patterns(df, table_name)\n",
					"        \n",
					"        # 11. Outlier Detection\n",
					"        if numeric_columns:\n",
					"            self._check_outliers(df, table_name, numeric_columns)\n",
					"        \n",
					"        # 12. Referential Integrity (if applicable)\n",
					"        self._check_referential_integrity(df, table_name)\n",
					"        \n",
					"        print(f\"\\n{'='*80}\")\n",
					"        print(f\"Validation Complete! Total Checks: {len(self.validation_results)}\")\n",
					"        print(f\"{'='*80}\\n\")\n",
					"        \n",
					"        return self.validation_results\n",
					"    \n",
					"    def _add_result(self, check_type: str, check_name: str, status: str, \n",
					"                    details: dict, severity: str = \"INFO\"):\n",
					"        \"\"\"Add validation result to results list\"\"\"\n",
					"        result = {\n",
					"            \"check_type\": check_type,\n",
					"            \"check_name\": check_name,\n",
					"            \"status\": status,\n",
					"            \"severity\": severity,\n",
					"            \"details\": details,\n",
					"            \"timestamp\": datetime.now().isoformat()\n",
					"        }\n",
					"        self.validation_results.append(result)\n",
					"        \n",
					"    def _check_basic_metrics(self, df: DataFrame, table_name: str):\n",
					"        \"\"\"Check basic table metrics\"\"\"\n",
					"        print(\"▶ Running Basic Metrics Check...\")\n",
					"        \n",
					"        row_count = df.count()\n",
					"        column_count = len(df.columns)\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Basic Metrics\",\n",
					"            check_name=\"Table Statistics\",\n",
					"            status=\"PASS\",\n",
					"            details={\n",
					"                \"table_name\": table_name,\n",
					"                \"row_count\": row_count,\n",
					"                \"column_count\": column_count\n",
					"            },\n",
					"            severity=\"INFO\"\n",
					"        )\n",
					"        \n",
					"        print(f\"  ✓ Row Count: {row_count:,}\")\n",
					"        print(f\"  ✓ Column Count: {column_count}\\n\")\n",
					"    \n",
					"    def _check_schema(self, df: DataFrame, table_name: str):\n",
					"        \"\"\"Validate table schema\"\"\"\n",
					"        print(\"▶ Running Schema Validation...\")\n",
					"        \n",
					"        schema_info = []\n",
					"        for field in df.schema.fields:\n",
					"            schema_info.append({\n",
					"                \"column_name\": field.name,\n",
					"                \"data_type\": str(field.dataType),\n",
					"                \"nullable\": field.nullable\n",
					"            })\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Schema Validation\",\n",
					"            check_name=\"Schema Structure\",\n",
					"            status=\"PASS\",\n",
					"            details={\"schema\": schema_info},\n",
					"            severity=\"INFO\"\n",
					"        )\n",
					"        \n",
					"        print(f\"  ✓ Schema validated with {len(schema_info)} columns\\n\")\n",
					"    \n",
					"    def _check_row_counts(self, df: DataFrame, table_name: str, primary_keys: list = None):\n",
					"        \"\"\"Check for duplicates and row counts\"\"\"\n",
					"        print(\"▶ Running Row Count & Duplicate Check...\")\n",
					"        \n",
					"        total_rows = df.count()\n",
					"        \n",
					"        if primary_keys:\n",
					"            distinct_rows = df.select(primary_keys).distinct().count()\n",
					"            duplicate_count = total_rows - distinct_rows\n",
					"            \n",
					"            status = \"PASS\" if duplicate_count == 0 else \"FAIL\"\n",
					"            severity = \"HIGH\" if duplicate_count > 0 else \"INFO\"\n",
					"            \n",
					"            self._add_result(\n",
					"                check_type=\"Duplicate Check\",\n",
					"                check_name=\"Primary Key Duplicates\",\n",
					"                status=status,\n",
					"                details={\n",
					"                    \"total_rows\": total_rows,\n",
					"                    \"distinct_rows\": distinct_rows,\n",
					"                    \"duplicate_count\": duplicate_count,\n",
					"                    \"primary_keys\": primary_keys\n",
					"                },\n",
					"                severity=severity\n",
					"            )\n",
					"            \n",
					"            print(f\"  {'✗' if duplicate_count > 0 else '✓'} Duplicates on {primary_keys}: {duplicate_count:,}\\n\")\n",
					"        else:\n",
					"            print(\"  ⚠ No primary keys specified, skipping duplicate check\\n\")\n",
					"    \n",
					"    def _check_null_values(self, df: DataFrame, table_name: str, critical_columns: list = None):\n",
					"        \"\"\"Check for null values in all columns\"\"\"\n",
					"        print(\"▶ Running Null Value Analysis...\")\n",
					"        \n",
					"        null_counts = df.select([\n",
					"            F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) \n",
					"            for c in df.columns\n",
					"        ]).collect()[0].asDict()\n",
					"        \n",
					"        total_rows = df.count()\n",
					"        null_analysis = []\n",
					"        critical_null_found = False\n",
					"        \n",
					"        for col, null_count in null_counts.items():\n",
					"            null_percentage = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
					"            \n",
					"            is_critical = critical_columns and col in critical_columns\n",
					"            \n",
					"            if null_count > 0:\n",
					"                null_analysis.append({\n",
					"                    \"column\": col,\n",
					"                    \"null_count\": null_count,\n",
					"                    \"null_percentage\": round(null_percentage, 2),\n",
					"                    \"is_critical\": is_critical\n",
					"                })\n",
					"                \n",
					"                if is_critical and null_count > 0:\n",
					"                    critical_null_found = True\n",
					"        \n",
					"        status = \"FAIL\" if critical_null_found else \"PASS\" if len(null_analysis) == 0 else \"WARNING\"\n",
					"        severity = \"HIGH\" if critical_null_found else \"MEDIUM\" if len(null_analysis) > 0 else \"INFO\"\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Null Check\",\n",
					"            check_name=\"Null Value Analysis\",\n",
					"            status=status,\n",
					"            details={\n",
					"                \"total_rows\": total_rows,\n",
					"                \"columns_with_nulls\": len(null_analysis),\n",
					"                \"null_details\": null_analysis[:10]  # Top 10\n",
					"            },\n",
					"            severity=severity\n",
					"        )\n",
					"        \n",
					"        print(f\"  {'✗' if critical_null_found else '✓'} Columns with nulls: {len(null_analysis)}\")\n",
					"        if null_analysis:\n",
					"            print(f\"  Top 5 columns with nulls:\")\n",
					"            for item in sorted(null_analysis, key=lambda x: x['null_percentage'], reverse=True)[:5]:\n",
					"                print(f\"    - {item['column']}: {item['null_count']:,} ({item['null_percentage']:.2f}%)\")\n",
					"        print()\n",
					"    \n",
					"    def _check_data_types(self, df: DataFrame, table_name: str):\n",
					"        \"\"\"Validate data types and conversions\"\"\"\n",
					"        print(\"▶ Running Data Type Validation...\")\n",
					"        \n",
					"        type_issues = []\n",
					"        \n",
					"        # Check for numeric columns with non-numeric values\n",
					"        for field in df.schema.fields:\n",
					"            if isinstance(field.dataType, StringType):\n",
					"                # Check if string column looks numeric\n",
					"                sample = df.select(field.name).filter(F.col(field.name).isNotNull()).limit(100)\n",
					"                numeric_like = sample.filter(\n",
					"                    F.col(field.name).rlike(r'^-?\\d+\\.?\\d*$')\n",
					"                ).count()\n",
					"                \n",
					"                if numeric_like > 80:  # 80% threshold\n",
					"                    type_issues.append({\n",
					"                        \"column\": field.name,\n",
					"                        \"issue\": \"String column appears to contain numeric data\",\n",
					"                        \"suggestion\": \"Consider converting to numeric type\"\n",
					"                    })\n",
					"        \n",
					"        status = \"WARNING\" if type_issues else \"PASS\"\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Data Type Validation\",\n",
					"            check_name=\"Type Consistency Check\",\n",
					"            status=status,\n",
					"            details={\"type_issues\": type_issues},\n",
					"            severity=\"LOW\" if type_issues else \"INFO\"\n",
					"        )\n",
					"        \n",
					"        print(f\"  {'⚠' if type_issues else '✓'} Data type issues found: {len(type_issues)}\\n\")\n",
					"    \n",
					"    def _check_completeness(self, df: DataFrame, table_name: str):\n",
					"        \"\"\"Calculate completeness percentage for each column\"\"\"\n",
					"        print(\"▶ Running Completeness Check...\")\n",
					"        \n",
					"        total_rows = df.count()\n",
					"        completeness = []\n",
					"        \n",
					"        for col in df.columns:\n",
					"            non_null_count = df.filter(F.col(col).isNotNull()).count()\n",
					"            completeness_pct = (non_null_count / total_rows * 100) if total_rows > 0 else 0\n",
					"            \n",
					"            completeness.append({\n",
					"                \"column\": col,\n",
					"                \"completeness_percentage\": round(completeness_pct, 2),\n",
					"                \"non_null_count\": non_null_count\n",
					"            })\n",
					"        \n",
					"        avg_completeness = sum(c['completeness_percentage'] for c in completeness) / len(completeness)\n",
					"        \n",
					"        status = \"PASS\" if avg_completeness >= 95 else \"WARNING\" if avg_completeness >= 80 else \"FAIL\"\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Completeness Check\",\n",
					"            check_name=\"Data Completeness\",\n",
					"            status=status,\n",
					"            details={\n",
					"                \"average_completeness\": round(avg_completeness, 2),\n",
					"                \"completeness_by_column\": completeness\n",
					"            },\n",
					"            severity=\"LOW\" if avg_completeness >= 80 else \"MEDIUM\"\n",
					"        )\n",
					"        \n",
					"        print(f\"  ✓ Average Completeness: {avg_completeness:.2f}%\\n\")\n",
					"    \n",
					"    def _check_date_ranges(self, df: DataFrame, table_name: str, date_columns: list):\n",
					"        \"\"\"Validate date ranges\"\"\"\n",
					"        print(\"▶ Running Date Range Validation...\")\n",
					"        \n",
					"        date_issues = []\n",
					"        \n",
					"        for col in date_columns:\n",
					"            try:\n",
					"                stats = df.select(\n",
					"                    F.min(col).alias(\"min_date\"),\n",
					"                    F.max(col).alias(\"max_date\"),\n",
					"                    F.count(F.when(F.col(col) > F.current_date(), 1)).alias(\"future_dates\"),\n",
					"                    F.count(F.when(F.col(col) < F.lit(\"1900-01-01\"), 1)).alias(\"very_old_dates\")\n",
					"                ).collect()[0]\n",
					"                \n",
					"                if stats.future_dates > 0 or stats.very_old_dates > 0:\n",
					"                    date_issues.append({\n",
					"                        \"column\": col,\n",
					"                        \"min_date\": str(stats.min_date),\n",
					"                        \"max_date\": str(stats.max_date),\n",
					"                        \"future_dates\": stats.future_dates,\n",
					"                        \"very_old_dates\": stats.very_old_dates\n",
					"                    })\n",
					"            except Exception as e:\n",
					"                date_issues.append({\n",
					"                    \"column\": col,\n",
					"                    \"error\": str(e)\n",
					"                })\n",
					"        \n",
					"        status = \"FAIL\" if date_issues else \"PASS\"\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Date Validation\",\n",
					"            check_name=\"Date Range Check\",\n",
					"            status=status,\n",
					"            details={\"date_issues\": date_issues},\n",
					"            severity=\"MEDIUM\" if date_issues else \"INFO\"\n",
					"        )\n",
					"        \n",
					"        print(f\"  {'✗' if date_issues else '✓'} Date range issues: {len(date_issues)}\\n\")\n",
					"    \n",
					"    def _check_numeric_ranges(self, df: DataFrame, table_name: str, numeric_columns: list):\n",
					"        \"\"\"Check numeric ranges and statistics\"\"\"\n",
					"        print(\"▶ Running Numeric Range Validation...\")\n",
					"        \n",
					"        numeric_stats = []\n",
					"        \n",
					"        for col in numeric_columns[:10]:  # Limit to first 10 numeric columns\n",
					"            try:\n",
					"                stats = df.select(\n",
					"                    F.min(col).alias(\"min_val\"),\n",
					"                    F.max(col).alias(\"max_val\"),\n",
					"                    F.avg(col).alias(\"avg_val\"),\n",
					"                    F.stddev(col).alias(\"stddev_val\"),\n",
					"                    F.count(F.when(F.col(col) < 0, 1)).alias(\"negative_count\")\n",
					"                ).collect()[0]\n",
					"                \n",
					"                numeric_stats.append({\n",
					"                    \"column\": col,\n",
					"                    \"min\": float(stats.min_val) if stats.min_val else None,\n",
					"                    \"max\": float(stats.max_val) if stats.max_val else None,\n",
					"                    \"avg\": float(stats.avg_val) if stats.avg_val else None,\n",
					"                    \"stddev\": float(stats.stddev_val) if stats.stddev_val else None,\n",
					"                    \"negative_count\": stats.negative_count\n",
					"                })\n",
					"            except Exception as e:\n",
					"                pass\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Numeric Validation\",\n",
					"            check_name=\"Numeric Statistics\",\n",
					"            status=\"PASS\",\n",
					"            details={\"numeric_statistics\": numeric_stats},\n",
					"            severity=\"INFO\"\n",
					"        )\n",
					"        \n",
					"        print(f\"  ✓ Analyzed {len(numeric_stats)} numeric columns\\n\")\n",
					"    \n",
					"    def _check_uniqueness(self, df: DataFrame, table_name: str):\n",
					"        \"\"\"Check uniqueness of key columns\"\"\"\n",
					"        print(\"▶ Running Uniqueness Check...\")\n",
					"        \n",
					"        uniqueness_results = []\n",
					"        \n",
					"        for col in df.columns[:20]:  # Check first 20 columns\n",
					"            total_count = df.count()\n",
					"            distinct_count = df.select(col).distinct().count()\n",
					"            uniqueness_pct = (distinct_count / total_count * 100) if total_count > 0 else 0\n",
					"            \n",
					"            if uniqueness_pct == 100 or uniqueness_pct < 50:\n",
					"                uniqueness_results.append({\n",
					"                    \"column\": col,\n",
					"                    \"total_count\": total_count,\n",
					"                    \"distinct_count\": distinct_count,\n",
					"                    \"uniqueness_percentage\": round(uniqueness_pct, 2),\n",
					"                    \"is_unique\": uniqueness_pct == 100\n",
					"                })\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Uniqueness Check\",\n",
					"            check_name=\"Column Uniqueness\",\n",
					"            status=\"PASS\",\n",
					"            details={\"uniqueness_analysis\": uniqueness_results},\n",
					"            severity=\"INFO\"\n",
					"        )\n",
					"        \n",
					"        print(f\"  ✓ Uniqueness checked for {len(uniqueness_results)} columns\\n\")\n",
					"    \n",
					"    def _check_string_patterns(self, df: DataFrame, table_name: str):\n",
					"        \"\"\"Validate string patterns\"\"\"\n",
					"        print(\"▶ Running String Pattern Validation...\")\n",
					"        \n",
					"        string_columns = [field.name for field in df.schema.fields \n",
					"                         if isinstance(field.dataType, StringType)]\n",
					"        \n",
					"        pattern_issues = []\n",
					"        \n",
					"        for col in string_columns[:10]:  # Check first 10 string columns\n",
					"            # Check for excessive whitespace\n",
					"            whitespace_count = df.filter(\n",
					"                F.col(col).rlike(r'^\\s+|\\s+$|  ')\n",
					"            ).count()\n",
					"            \n",
					"            if whitespace_count > 0:\n",
					"                pattern_issues.append({\n",
					"                    \"column\": col,\n",
					"                    \"issue\": \"Excessive whitespace detected\",\n",
					"                    \"affected_rows\": whitespace_count\n",
					"                })\n",
					"        \n",
					"        status = \"WARNING\" if pattern_issues else \"PASS\"\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"String Validation\",\n",
					"            check_name=\"String Pattern Check\",\n",
					"            status=status,\n",
					"            details={\"pattern_issues\": pattern_issues},\n",
					"            severity=\"LOW\" if pattern_issues else \"INFO\"\n",
					"        )\n",
					"        \n",
					"        print(f\"  {'⚠' if pattern_issues else '✓'} String pattern issues: {len(pattern_issues)}\\n\")\n",
					"    \n",
					"    def _check_outliers(self, df: DataFrame, table_name: str, numeric_columns: list):\n",
					"        \"\"\"Detect outliers in numeric columns using IQR method\"\"\"\n",
					"        print(\"▶ Running Outlier Detection...\")\n",
					"        \n",
					"        outlier_results = []\n",
					"        \n",
					"        for col in numeric_columns[:5]:  # Check first 5 numeric columns\n",
					"            try:\n",
					"                quantiles = df.stat.approxQuantile(col, [0.25, 0.75], 0.05)\n",
					"                if len(quantiles) == 2:\n",
					"                    q1, q3 = quantiles\n",
					"                    iqr = q3 - q1\n",
					"                    lower_bound = q1 - 1.5 * iqr\n",
					"                    upper_bound = q3 + 1.5 * iqr\n",
					"                    \n",
					"                    outlier_count = df.filter(\n",
					"                        (F.col(col) < lower_bound) | (F.col(col) > upper_bound)\n",
					"                    ).count()\n",
					"                    \n",
					"                    if outlier_count > 0:\n",
					"                        outlier_results.append({\n",
					"                            \"column\": col,\n",
					"                            \"outlier_count\": outlier_count,\n",
					"                            \"lower_bound\": lower_bound,\n",
					"                            \"upper_bound\": upper_bound\n",
					"                        })\n",
					"            except Exception as e:\n",
					"                pass\n",
					"        \n",
					"        status = \"WARNING\" if outlier_results else \"PASS\"\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Outlier Detection\",\n",
					"            check_name=\"Numeric Outliers\",\n",
					"            status=status,\n",
					"            details={\"outliers\": outlier_results},\n",
					"            severity=\"LOW\"\n",
					"        )\n",
					"        \n",
					"        print(f\"  {'⚠' if outlier_results else '✓'} Columns with outliers: {len(outlier_results)}\\n\")\n",
					"    \n",
					"    def _check_referential_integrity(self, df: DataFrame, table_name: str):\n",
					"        \"\"\"Check referential integrity (placeholder for custom logic)\"\"\"\n",
					"        print(\"▶ Running Referential Integrity Check...\")\n",
					"        \n",
					"        self._add_result(\n",
					"            check_type=\"Referential Integrity\",\n",
					"            check_name=\"Foreign Key Check\",\n",
					"            status=\"SKIPPED\",\n",
					"            details={\"message\": \"No foreign key relationships defined\"},\n",
					"            severity=\"INFO\"\n",
					"        )\n",
					"        \n",
					"        print(\"  ⚠ Skipped (no relationships defined)\\n\")\n",
					"    \n",
					"    def display_results(self, results: list = None):\n",
					"        \"\"\"Display validation results in a readable format\"\"\"\n",
					"        if results is None:\n",
					"            results = self.validation_results\n",
					"        \n",
					"        print(f\"\\n{'='*80}\")\n",
					"        print(\"DATA QUALITY VALIDATION RESULTS\")\n",
					"        print(f\"{'='*80}\\n\")\n",
					"        \n",
					"        # Group by severity\n",
					"        by_severity = {\"HIGH\": [], \"MEDIUM\": [], \"LOW\": [], \"INFO\": []}\n",
					"        \n",
					"        for result in results:\n",
					"            by_severity[result['severity']].append(result)\n",
					"        \n",
					"        # Display by severity\n",
					"        for severity in [\"HIGH\", \"MEDIUM\", \"LOW\", \"INFO\"]:\n",
					"            items = by_severity[severity]\n",
					"            if items:\n",
					"                print(f\"\\n{severity} SEVERITY ({len(items)} checks)\")\n",
					"                print(\"-\" * 80)\n",
					"                \n",
					"                for item in items:\n",
					"                    icon = \"✗\" if item['status'] == \"FAIL\" else \"⚠\" if item['status'] == \"WARNING\" else \"✓\"\n",
					"                    print(f\"{icon} {item['check_type']}: {item['check_name']} - {item['status']}\")\n",
					"                    \n",
					"                    # Print key details\n",
					"                    if item['details']:\n",
					"                        for key, value in list(item['details'].items())[:3]:\n",
					"                            if not isinstance(value, (list, dict)):\n",
					"                                print(f\"    {key}: {value}\")\n",
					"        \n",
					"        # Summary\n",
					"        total = len(results)\n",
					"        passed = sum(1 for r in results if r['status'] == 'PASS')\n",
					"        failed = sum(1 for r in results if r['status'] == 'FAIL')\n",
					"        warnings = sum(1 for r in results if r['status'] == 'WARNING')\n",
					"        \n",
					"        print(f\"\\n{'='*80}\")\n",
					"        print(f\"SUMMARY: {passed} Passed | {warnings} Warnings | {failed} Failed | {total} Total\")\n",
					"        print(f\"{'='*80}\\n\")\n",
					"    \n",
					"    def save_results_to_table(self, output_table: str):\n",
					"        \"\"\"Save validation results to a Delta table\"\"\"\n",
					"        results_data = []\n",
					"        \n",
					"        for result in self.validation_results:\n",
					"            results_data.append({\n",
					"                \"validation_timestamp\": datetime.now(),\n",
					"                \"check_type\": result['check_type'],\n",
					"                \"check_name\": result['check_name'],\n",
					"                \"status\": result['status'],\n",
					"                \"severity\": result['severity'],\n",
					"                \"details_json\": json.dumps(result['details'])\n",
					"            })\n",
					"        \n",
					"        results_df = self.spark.createDataFrame(results_data)\n",
					"        results_df.write.format(\"delta\").mode(\"append\").saveAsTable(output_table)\n",
					"        \n",
					"        print(f\"✓ Results saved to table: {output_table}\")\n",
					"    \n",
					"    def get_results_dataframe(self):\n",
					"        \"\"\"Convert results to PySpark DataFrame\"\"\"\n",
					"        results_data = []\n",
					"        \n",
					"        for result in self.validation_results:\n",
					"            results_data.append({\n",
					"                \"check_type\": result['check_type'],\n",
					"                \"check_name\": result['check_name'],\n",
					"                \"status\": result['status'],\n",
					"                \"severity\": result['severity'],\n",
					"                \"timestamp\": result['timestamp'],\n",
					"                \"details\": json.dumps(result['details'])\n",
					"            })\n",
					"        \n",
					"        return self.spark.createDataFrame(results_data)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"validator = DataQualityValidator(spark)\n",
					"results = validator.run_all_validations(\"odw_standardised_db.sap_hr_history_monthly\")\n",
					"validator.display_results()"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"validator = DataQualityValidator(spark)\n",
					"results = validator.run_all_validations(\n",
					"    table_name=\"odw_standardised_db.sap_hr_history_monthly\",\n",
					"    primary_keys=[\"PersNo\"],\n",
					"    critical_columns=[\"PersNo\", \"EmployeeNo\", \"CompanyCode\"],\n",
					"    date_columns=[\"CivilServiceStart\", \"Birthdate\", \"Report_MonthEnd_Date\"],\n",
					"    numeric_columns=[\"FTE\", \"Wkhrs\", \"Annualsalary\"]\n",
					")\n",
					"validator.display_results()"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}