{
	"name": "vw_casework_specialist",
	"properties": {
		"folder": {
			"name": "odw-curated/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "be59956e-20fc-46a2-8c7d-ed1a9caf6d30"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a view & table for Power BI use.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pay Roll Area  details Covered in here;"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum \n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    logInfo(\"Starting Casework Specialist curated table setup\")\n",
					"    \n",
					"    # Get storage account \n",
					"    storage_account = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"    logInfo(f\"Using storage account: {storage_account}\")\n",
					"    \n",
					"    # Fix the path\n",
					"    storage_account = storage_account.rstrip('/')\n",
					"    delta_table_path = f\"abfss://odw-curated@{storage_account}/casework_specialist/pbi_casework_specialist\"\n",
					"    logInfo(f\"Delta table will be created at: {delta_table_path}\")\n",
					"    \n",
					"    # Create/refresh view\n",
					"    logInfo(\"Creating view odw_curated_db.vw_casework_specialist\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE VIEW odw_curated_db.vw_casework_specialist AS\n",
					"    SELECT  \n",
					"        greenCaseType,\n",
					"        greenCaseId,\n",
					"        caseReference,\n",
					"        horizonId,\n",
					"        linkedGreenCaseId,\n",
					"        caseOfficerName,\n",
					"        caseOfficerEmail,\n",
					"        appealType,\n",
					"        `procedure`,\n",
					"        processingState,\n",
					"        pinsLpaCode,\n",
					"        pinsLpaName,\n",
					"        appellantName,\n",
					"        agentName,\n",
					"        siteAddressLine1,\n",
					"        siteAddressLine2,\n",
					"        siteTownCity,\n",
					"        sitePostcode,\n",
					"        otherPartyName,\n",
					"        receiptDate,\n",
					"        validDate,\n",
					"        startDate,\n",
					"        lpaQuestionnaireDue,\n",
					"        lpaQuestionnaireReceived,\n",
					"        week6Date,\n",
					"        week8Date,\n",
					"        week9Date,\n",
					"        eventDate,\n",
					"        eventTime,\n",
					"        inspectorName,\n",
					"        inspectorEmail,\n",
					"        decision,\n",
					"        decisionDate,\n",
					"        withdrawnOrTurnedAway,\n",
					"        withdrawnOrTurnedAwayDate,\n",
					"        comments,\n",
					"        ODTSourceSystem AS sourceSystem,\n",
					"        SourceSystemID AS sourceSuid\n",
					"    FROM odw_harmonised_db.load_casework_specialist\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created view odw_curated_db.vw_casework_specialist\")\n",
					"    \n",
					"    # Count records in view\n",
					"    casework_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_curated_db.vw_casework_specialist\").collect()[0]['count']\n",
					"    logInfo(f\"View contains {casework_count} casework specialist records\")\n",
					"    \n",
					"    # Drop the table if it exists\n",
					"    logInfo(\"Dropping table odw_curated_db.pbi_casework_specialist if it exists\")\n",
					"    spark.sql(\"\"\"\n",
					"    DROP TABLE IF EXISTS odw_curated_db.pbi_casework_specialist\n",
					"    \"\"\")\n",
					"    logInfo(\"Table dropped or did not exist\")\n",
					"    \n",
					"    # Delete the storage location to ensure clean state\n",
					"    logInfo(f\"Cleaning up storage location: {delta_table_path}\")\n",
					"    try:\n",
					"        mssparkutils.fs.rm(delta_table_path, recurse=True)\n",
					"        logInfo(\"Storage location cleaned successfully\")\n",
					"    except Exception as cleanup_error:\n",
					"        logInfo(f\"Storage location cleanup: {str(cleanup_error)} (location may not exist, proceeding)\")\n",
					"    \n",
					"    # Create table from view with specified location\n",
					"    logInfo(\"Creating table odw_curated_db.pbi_casework_specialist from view with specified location\")\n",
					"    spark.sql(f\"\"\"\n",
					"    CREATE TABLE odw_curated_db.pbi_casework_specialist\n",
					"    USING delta\n",
					"    LOCATION '{delta_table_path}'\n",
					"    AS SELECT * FROM odw_curated_db.vw_casework_specialist\n",
					"    \"\"\")\n",
					"    \n",
					"    # Count records in table - this is our final record count\n",
					"    table_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_curated_db.pbi_casework_specialist\").collect()[0]['count']\n",
					"    result[\"record_count\"] = table_count\n",
					"    logInfo(f\"Created table with {table_count} records at location: {delta_table_path}\")\n",
					"    \n",
					"    logInfo(\"Casework Specialist curated table setup completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information\n",
					"    error_msg = f\"Error in Casework Specialist curated table setup: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Try to get current record count even in case of error\n",
					"    try:\n",
					"        # Try view first as table might not exist yet\n",
					"        error_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_curated_db.vw_casework_specialist\").collect()[0]['count']\n",
					"        result[\"record_count\"] = error_count\n",
					"    except:\n",
					"        result[\"record_count\"] = 0\n",
					"    \n",
					"    # Update result for error case\n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = error_msg[:300]  # Truncate to 300 characters\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}