{
	"name": "User_Mpesc",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "hbtPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ca5d5c9d-f233-41c4-96c1-cd5cb5a9c896"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/hbtPool",
				"name": "hbtPool",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/hbtPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F, types as T\n",
					"import uuid\n",
					"\n",
					"# Horizon extract (has coEmailAddress)\n",
					"HZN_USERS_PATH = \"abfss://horizon-migration-poc@pinsstodwdevuks9h80mb.dfs.core.windows.net/MPESC-EXTRACT/Users.csv\"\n",
					"\n",
					"# Entra folder (your new folder)\n",
					"ENTRA_FOLDER_PATH = \"abfss://horizon-migration-poc@pinsstodwdevuks9h80mb.dfs.core.windows.net/MPESC-TRANSFORM/Entra/\"\n",
					"\n",
					"# Outputs\n",
					"OUT_MATCHED_DEBUG = \"abfss://horizon-migration-poc@pinsstodwdevuks9h80mb.dfs.core.windows.net/MPESC-TRANSFORM/UserEntraMatchDebug/\"\n",
					"OUT_USERS_FOR_LOAD = \"abfss://horizon-migration-poc@pinsstodwdevuks9h80mb.dfs.core.windows.net/MPESC-TRANSFORM/UserWithEntraId/\"\n",
					""
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"hzn = (spark.read\n",
					"    .option(\"header\", \"true\")\n",
					"    .csv(HZN_USERS_PATH)\n",
					")\n",
					"\n",
					"hzn = (hzn\n",
					"    .withColumn(\"coEmailAddress\", F.trim(F.col(\"coEmailAddress\")))\n",
					"    .withColumn(\"hzn_email_raw\", F.lower(F.col(\"coEmailAddress\")))\n",
					"    .withColumn(\"hzn_email_local\", F.split(F.col(\"hzn_email_raw\"), \"@\").getItem(0))\n",
					")\n",
					"\n",
					"print(\"Horizon rows:\", hzn.count())\n",
					"display(hzn.limit(20))\n",
					""
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"ENTRA_FOLDER_PATH = \"abfss://horizon-migration-poc@pinsstodwdevuks9h80mb.dfs.core.windows.net/MPESC-TRANSFORM/Entra/\"\n",
					"\n",
					"# 1) Try multiline JSON first (many Graph dumps are single JSON documents per file)\n",
					"entra_raw = (spark.read\n",
					"    .option(\"multiline\", \"true\")\n",
					"    .json(ENTRA_FOLDER_PATH)\n",
					")\n",
					"\n",
					"# If Spark still can't parse, it will have only _corrupt_record\n",
					"only_corrupt = (entra_raw.columns == [\"_corrupt_record\"]) or (set(entra_raw.columns) == {\"_corrupt_record\"})\n",
					"\n",
					"if not only_corrupt:\n",
					"    print(\"✅ Parsed as JSON. Columns:\", entra_raw.columns)\n",
					"else:\n",
					"    print(\"⚠️ JSON parse failed; falling back to TEXT parsing...\")\n",
					"\n",
					"    # 2) Fallback: read as text, pull out every \"value\": [...] block, parse those as JSON arrays\n",
					"    txt = spark.read.text(ENTRA_FOLDER_PATH)\n",
					"\n",
					"    # Concatenate all file text into one string\n",
					"    blob = txt.agg(F.concat_ws(\"\\n\", F.collect_list(\"value\")).alias(\"blob\")).collect()[0][\"blob\"]\n",
					"\n",
					"    # Extract each value-array inside pages: \"value\":[ ... ]\n",
					"    # This regex pulls the inside of the value array (non-greedy)\n",
					"    import re\n",
					"    chunks = re.findall(r'\"value\"\\s*:\\s*\\[(.*?)\\]\\s*(?:,|\\})', blob, flags=re.DOTALL)\n",
					"\n",
					"    if len(chunks) == 0:\n",
					"        raise Exception(\"Could not find any \\\"value\\\": [...] arrays in Entra export text.\")\n",
					"\n",
					"    # Turn each chunk into a valid JSON array string\n",
					"    arrays = [\"[\" + c + \"]\" for c in chunks]\n",
					"\n",
					"    # Parse arrays into rows (each row is one user object)\n",
					"    rdd = spark.sparkContext.parallelize(arrays)\n",
					"    pages = spark.read.json(rdd)   # rows are user objects now\n",
					"    entra_raw = pages\n",
					"\n",
					"    print(f\"✅ Fallback parsed users. Rows: {entra_raw.count()}\")\n",
					"    print(\"Columns:\", entra_raw.columns)\n",
					"\n",
					"# At this point entra_raw should be either:\n",
					"# - page objects with a `value` array, OR\n",
					"# - already user objects\n",
					""
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"entra = entra_raw.select(F.explode(\"value\").alias(\"u\")).select(\"u.*\")\n",
					"\n",
					"print(\"Entra user columns:\", entra.columns)\n",
					"print(\"Entra user rows:\", entra.count())"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# ensure optional columns exist\n",
					"if \"mail\" not in entra.columns:\n",
					"    entra = entra.withColumn(\"mail\", F.lit(None).cast(\"string\"))\n",
					"if \"userPrincipalName\" not in entra.columns:\n",
					"    entra = entra.withColumn(\"userPrincipalName\", F.lit(None).cast(\"string\"))\n",
					"if \"otherMails\" not in entra.columns:\n",
					"    entra = entra.withColumn(\"otherMails\", F.array().cast(\"array<string>\"))\n",
					"\n",
					"entra_keyed = (entra\n",
					"    .withColumn(\n",
					"        \"entra_email_raw\",\n",
					"        F.lower(F.trim(F.coalesce(\n",
					"            F.col(\"mail\"),\n",
					"            F.col(\"userPrincipalName\"),\n",
					"            F.element_at(F.col(\"otherMails\"), 1)  # first element (1-indexed)\n",
					"        )))\n",
					"    )\n",
					"    .withColumn(\"entra_email_local\", F.split(F.col(\"entra_email_raw\"), \"@\").getItem(0))\n",
					"    .select(\n",
					"        F.col(\"id\").alias(\"idpUserId\"),\n",
					"        \"entra_email_raw\",\n",
					"        \"entra_email_local\",\n",
					"        \"mail\",\n",
					"        \"userPrincipalName\",\n",
					"        \"displayName\"\n",
					"    )\n",
					")\n",
					"\n",
					"entra_dedup = entra_keyed.dropDuplicates([\"entra_email_local\"])\n",
					"\n",
					"print(\"Entra keyed rows:\", entra_keyed.count())\n",
					"print(\"Entra dedup rows:\", entra_dedup.count())\n",
					"display(entra_dedup.limit(20))\n",
					""
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"hzn_users_path = \"abfss://horizon-migration-poc@pinsstodwdevuks9h80mb.dfs.core.windows.net/MPESC-EXTRACT/Users.csv\"\n",
					"\n",
					"hzn = (spark.read.option(\"header\",\"true\").csv(hzn_users_path)\n",
					"    .withColumn(\"coEmailAddress\", F.trim(F.col(\"coEmailAddress\")))\n",
					"    .withColumn(\"hzn_email_raw\", F.lower(F.col(\"coEmailAddress\")))\n",
					"    .withColumn(\"hzn_email_local\", F.split(F.col(\"hzn_email_raw\"), \"@\").getItem(0))\n",
					")\n",
					"\n",
					"print(\"Horizon users rows:\", hzn.count())\n",
					"display(hzn.limit(20))\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"matched = (hzn.join(\n",
					"        entra_dedup,\n",
					"        hzn[\"hzn_email_local\"] == entra_dedup[\"entra_email_local\"],\n",
					"        how=\"left\"\n",
					"    )\n",
					"    .select(\n",
					"        \"caseOfficerId\",\n",
					"        \"caseOfficerLogin\",\n",
					"        \"caseOfficerName\",\n",
					"        \"coEmailAddress\",\n",
					"        \"idpUserId\",\n",
					"        \"userPrincipalName\",\n",
					"        \"mail\",\n",
					"        \"displayName\"\n",
					"    )\n",
					")\n",
					"\n",
					"display(matched.orderBy(\"caseOfficerName\").limit(200))\n",
					"print(\"Matched:\", matched.filter(F.col(\"idpUserId\").isNotNull()).count())\n",
					"print(\"Unmatched:\", matched.filter(F.col(\"idpUserId\").isNull()).count())\n",
					""
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"import uuid\n",
					"from pyspark.sql import types as T\n",
					"from pyspark.sql import functions as F\n",
					"\n",
					"NAMESPACE_UUID = uuid.UUID(\"6ba7b810-9dad-11d1-80b4-00c04fd430c8\")\n",
					"\n",
					"@F.udf(returnType=T.StringType())\n",
					"def uuid5_from_legacy(legacy_id: str):\n",
					"    if legacy_id is None:\n",
					"        return None\n",
					"    return str(uuid.uuid5(NAMESPACE_UUID, legacy_id))\n",
					"\n",
					"users_for_load = (matched\n",
					"    .withColumn(\"legacyId\", F.col(\"caseOfficerId\").cast(\"string\"))\n",
					"    .withColumn(\"id\", uuid5_from_legacy(F.col(\"legacyId\")))\n",
					"    .select(\n",
					"        \"id\",\n",
					"        \"idpUserId\",\n",
					"        \"legacyId\",\n",
					"        \"caseOfficerLogin\",\n",
					"        \"coEmailAddress\",\n",
					"        \"caseOfficerName\"\n",
					"    )\n",
					"    .dropDuplicates([\"id\"])\n",
					")\n",
					"\n",
					"display(users_for_load.orderBy(\"caseOfficerName\").limit(200))\n",
					"\n",
					"print(\"Rows ready:\", users_for_load.count())\n",
					"print(\"Missing idpUserId:\", users_for_load.filter(F.col(\"idpUserId\").isNull()).count())\n",
					"\n",
					""
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"# Add flag column\n",
					"users_all = (\n",
					"    users_for_load\n",
					"    .withColumn(\n",
					"        \"hasEntraId\",\n",
					"        F.when(F.col(\"idpUserId\").isNotNull(), F.lit(True)).otherwise(F.lit(False))\n",
					"    )\n",
					")\n",
					"\n",
					"# Output path\n",
					"BASE_PATH = \"abfss://horizon-migration-poc@pinsstodwdevuks9h80mb.dfs.core.windows.net\"\n",
					"OUT_USERS_CSV = f\"{BASE_PATH}/MPESC-TRANSFORM/UserAll_WithEntraFlag/\"\n",
					"\n",
					"# Save as single CSV\n",
					"(users_all\n",
					"    .coalesce(1)\n",
					"    .write\n",
					"    .mode(\"overwrite\")\n",
					"    .option(\"header\", \"true\")\n",
					"    .csv(OUT_USERS_CSV)\n",
					")\n",
					"\n",
					"print(\"Saved combined CSV to:\", OUT_USERS_CSV)\n",
					"print(\"Total users:\", users_all.count())\n",
					"print(\"With Entra:\", users_all.filter(F.col(\"hasEntraId\")).count())\n",
					"print(\"Without Entra:\", users_all.filter(~F.col(\"hasEntraId\")).count())\n",
					""
				],
				"execution_count": 25
			}
		]
	}
}