{
	"name": "py_casework_specialist",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4d752c26-4067-4876-9622-6a3bdefcca97"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Set time parser policy\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Delete existing data\n",
					"    logInfo(\"Starting deletion of all rows from odw_harmonised_db.casework_specialist\")\n",
					"    spark.sql(\"DELETE FROM odw_harmonised_db.casework_specialist\")\n",
					"    logInfo(\"Successfully deleted all rows from odw_harmonised_db.casework_specialist\")\n",
					"    \n",
					"    # Insert data from source table\n",
					"    logInfo(\"Starting data insertion into odw_harmonised_db.casework_specialist from standardised.casework_specialist\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.casework_specialist (\n",
					"        casework_specialist_id,\n",
					"        greenCaseType,\n",
					"        greenCaseId,\n",
					"        caseReference,\n",
					"        horizonId,\n",
					"        linkedGreenCaseId,\n",
					"        caseOfficerName,\n",
					"        caseOfficerEmail,\n",
					"        appealType,\n",
					"        procedure,\n",
					"        processingState,\n",
					"        pinsLpaCode,\n",
					"        pinsLpaName,\n",
					"        appellantName,\n",
					"        agentName,\n",
					"        siteAddressLine1,\n",
					"        siteAddressLine2,\n",
					"        siteTownCity,\n",
					"        sitePostcode,\n",
					"        otherPartyName,\n",
					"        receiptDate,\n",
					"        validDate,\n",
					"        startDate,\n",
					"        lpaQuestionnaireDue,\n",
					"        lpaQuestionnaireReceived,\n",
					"        week6Date,\n",
					"        week8Date,\n",
					"        week9Date,\n",
					"        eventDate,\n",
					"        eventTime,\n",
					"        inspectorName,\n",
					"        inspectorEmail,\n",
					"        decision,\n",
					"        decisionDate,\n",
					"        withdrawnOrTurnedAway,\n",
					"        withdrawnOrTurnedAwayDate,\n",
					"        comments,\n",
					"        Migrated,\n",
					"        ODTSourceSystem,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        message_id,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    )\n",
					"    SELECT\n",
					"        casework_specialist_id,\n",
					"        greenCaseType,\n",
					"        greenCaseId,\n",
					"        caseReference,\n",
					"        horizonId,\n",
					"        linkedGreenCaseId,\n",
					"        caseOfficerName,\n",
					"        caseOfficerEmail,\n",
					"        appealType,\n",
					"        procedure,\n",
					"        processingState,\n",
					"        pinsLpaCode,\n",
					"        pinsLpaName,\n",
					"        appellantName,\n",
					"        agentName,\n",
					"        siteAddressLine1,\n",
					"        siteAddressLine2,\n",
					"        siteTownCity,\n",
					"        sitePostcode,\n",
					"        otherPartyName,\n",
					"        to_date(receiptDate, 'dd/MM/yyyy') AS receiptDate,\n",
					"        to_date(validDate, 'dd/MM/yyyy') AS validDate,\n",
					"        to_date(startDate, 'dd/MM/yyyy') AS startDate,\n",
					"        to_date(lpaQuestionnaireDue, 'dd/MM/yyyy') AS lpaQuestionnaireDue,\n",
					"        to_date(lpaQuestionnaireReceived, 'dd/MM/yyyy') AS lpaQuestionnaireReceived,\n",
					"        to_date(week6Date, 'dd/MM/yyyy') AS week6Date,\n",
					"        to_date(week8Date, 'dd/MM/yyyy') AS week8Date,\n",
					"        to_date(week9Date, 'dd/MM/yyyy') AS week9Date,\n",
					"        to_date(eventDate, 'dd/MM/yyyy') AS eventDate,\n",
					"        eventTime,\n",
					"        inspectorName,\n",
					"        inspectorEmail,\n",
					"        decision,\n",
					"        to_date(decisionDate, 'dd/MM/yyyy') AS decisionDate,\n",
					"        withdrawnOrTurnedAway,\n",
					"        to_date(withdrawnOrTurnedAwayDate, 'dd/MM/yyyy') AS withdrawnOrTurnedAwayDate,\n",
					"        comments,\n",
					"        Migrated,\n",
					"        ODTSourceSystem,\n",
					"        'casework_specialist' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        message_id,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        NULL AS RowID,\n",
					"        'Y' AS IsActive\n",
					"    FROM odw_standardised_db.casework_specialist\n",
					"    \"\"\")\n",
					"    \n",
					"    # Check the number of rows inserted\n",
					"    inserted_rows = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.casework_specialist\").collect()[0]['count']\n",
					"    result[\"record_count\"] = inserted_rows\n",
					"    logInfo(f\"Successfully inserted {inserted_rows} rows into odw_harmonised_db.casework_specialist\")\n",
					"    \n",
					"    # Update RowID with MD5 hash\n",
					"    logInfo(\"Starting RowID update with MD5 hash values\")\n",
					"    spark.sql(\"\"\"\n",
					"    UPDATE odw_harmonised_db.casework_specialist\n",
					"    SET RowID = md5(concat_ws('|', \n",
					"        casework_specialist_id, greenCaseType, greenCaseId, caseReference, horizonId, linkedGreenCaseId, \n",
					"        caseOfficerName, caseOfficerEmail, appealType, procedure, processingState, pinsLpaCode, pinsLpaName, \n",
					"        appellantName, agentName, siteAddressLine1, siteAddressLine2, siteTownCity, sitePostcode, otherPartyName, \n",
					"        receiptDate, validDate, startDate, lpaQuestionnaireDue, lpaQuestionnaireReceived, week6Date, week8Date, \n",
					"        week9Date, eventDate, eventTime, inspectorName, inspectorEmail, decision, decisionDate, \n",
					"        withdrawnOrTurnedAway, withdrawnOrTurnedAwayDate, comments, Migrated, ODTSourceSystem\n",
					"    ))\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully updated all RowID values with MD5 hashes\")\n",
					"    \n",
					"    # Verify data quality\n",
					"    null_rowid_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.casework_specialist WHERE RowID IS NULL\").collect()[0]['count']\n",
					"    if null_rowid_count > 0:\n",
					"        logError(f\"Data quality issue: {null_rowid_count} rows have NULL RowID values\")\n",
					"        result[\"status\"] = \"warning\"\n",
					"    else:\n",
					"        logInfo(\"Data quality check passed: No NULL RowID values found\")\n",
					"    \n",
					"    # Final success message\n",
					"    logInfo(\"Casework specialist data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information and limit to 300 characters\n",
					"    error_msg = f\"Error in casework specialist data processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Truncate error message if it exceeds 300 characters\n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1  # Indicate failure with -1 count\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}