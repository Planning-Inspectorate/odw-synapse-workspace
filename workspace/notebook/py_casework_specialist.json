{
	"name": "py_casework_specialist",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f2554c7a-faf3-4ac6-a8af-bdc9b953070d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Set time parser policy\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Delete existing data\n",
					"    logInfo(\"Starting deletion of all rows from odw_harmonised_db.casework_specialist\")\n",
					"    spark.sql(\"DELETE FROM odw_harmonised_db.casework_specialist\")\n",
					"    logInfo(\"Successfully deleted all rows from odw_harmonised_db.casework_specialist\")\n",
					"    \n",
					"    # Insert data from source table\n",
					"    logInfo(\"Starting data insertion into odw_harmonised_db.casework_specialist from standardised.casework_specialist\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.casework_specialist (\n",
					"        casework_specialist_id,\n",
					"        greenCaseType,\n",
					"        greenCaseId,\n",
					"        caseReference,\n",
					"        horizonId,\n",
					"        linkedGreenCaseId,\n",
					"        caseOfficerName,\n",
					"        caseOfficerEmail,\n",
					"        appealType,\n",
					"        procedure,\n",
					"        processingState,\n",
					"        pinsLpaCode,\n",
					"        pinsLpaName,\n",
					"        appellantName,\n",
					"        agentName,\n",
					"        siteAddressLine1,\n",
					"        siteAddressLine2,\n",
					"        siteTownCity,\n",
					"        sitePostcode,\n",
					"        otherPartyName,\n",
					"        receiptDate,\n",
					"        validDate,\n",
					"        startDate,\n",
					"        lpaQuestionnaireDue,\n",
					"        lpaQuestionnaireReceived,\n",
					"        week6Date,\n",
					"        week8Date,\n",
					"        week9Date,\n",
					"        eventDate,\n",
					"        eventTime,\n",
					"        inspectorName,\n",
					"        inspectorEmail,\n",
					"        decision,\n",
					"        decisionDate,\n",
					"        withdrawnOrTurnedAway,\n",
					"        withdrawnOrTurnedAwayDate,\n",
					"        comments,\n",
					"        Migrated,\n",
					"        ODTSourceSystem,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        message_id,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    )\n",
					"    SELECT\n",
					"        CASE WHEN casework_specialist_id = '' THEN NULL ELSE TRY_CAST(casework_specialist_id AS INT) END AS casework_specialist_id,\n",
					"        CASE WHEN TRIM(greenCaseType) = '' THEN NULL ELSE greenCaseType END AS greenCaseType,\n",
					"        CASE WHEN greenCaseId = '' THEN NULL ELSE TRY_CAST(greenCaseId AS INT) END AS greenCaseId,\n",
					"        CASE WHEN TRIM(caseReference) = '' THEN NULL ELSE caseReference END AS caseReference,\n",
					"        CASE WHEN TRIM(horizonId) = '' THEN NULL ELSE horizonId END AS horizonId,\n",
					"        CASE WHEN linkedGreenCaseId = '' THEN NULL ELSE TRY_CAST(linkedGreenCaseId AS INT) END AS linkedGreenCaseId,\n",
					"        CASE WHEN TRIM(caseOfficerName) = '' THEN NULL ELSE caseOfficerName END AS caseOfficerName,\n",
					"        CASE WHEN TRIM(caseOfficerEmail) = '' THEN NULL ELSE caseOfficerEmail END AS caseOfficerEmail,\n",
					"        CASE WHEN TRIM(appealType) = '' THEN NULL ELSE appealType END AS appealType,\n",
					"        CASE WHEN TRIM(procedure) = '' THEN NULL ELSE procedure END AS procedure,\n",
					"        CASE WHEN TRIM(processingState) = '' THEN NULL ELSE processingState END AS processingState,\n",
					"        CASE WHEN TRIM(pinsLpaCode) = '' THEN NULL ELSE pinsLpaCode END AS pinsLpaCode,\n",
					"        CASE WHEN TRIM(pinsLpaName) = '' THEN NULL ELSE pinsLpaName END AS pinsLpaName,\n",
					"        CASE WHEN TRIM(appellantName) = '' THEN NULL ELSE appellantName END AS appellantName,\n",
					"        CASE WHEN TRIM(agentName) = '' THEN NULL ELSE agentName END AS agentName,\n",
					"        CASE WHEN TRIM(siteAddressLine1) = '' THEN NULL ELSE siteAddressLine1 END AS siteAddressLine1,\n",
					"        CASE WHEN TRIM(siteAddressLine2) = '' THEN NULL ELSE siteAddressLine2 END AS siteAddressLine2,\n",
					"        CASE WHEN TRIM(siteTownCity) = '' THEN NULL ELSE siteTownCity END AS siteTownCity,\n",
					"        CASE WHEN TRIM(sitePostcode) = '' THEN NULL ELSE sitePostcode END AS sitePostcode,\n",
					"        CASE WHEN TRIM(otherPartyName) = '' THEN NULL ELSE otherPartyName END AS otherPartyName,\n",
					"        receiptDate,\n",
					"        validDate,\n",
					"        startDate,\n",
					"        lpaQuestionnaireDue,\n",
					"        lpaQuestionnaireReceived,\n",
					"        week6Date,\n",
					"        week8Date,\n",
					"        week9Date,\n",
					"        eventDate,\n",
					"        CASE WHEN TRIM(eventTime) = '' THEN NULL ELSE eventTime END AS eventTime,\n",
					"        CASE WHEN TRIM(inspectorName) = '' THEN NULL ELSE inspectorName END AS inspectorName,\n",
					"        CASE WHEN TRIM(inspectorEmail) = '' THEN NULL ELSE inspectorEmail END AS inspectorEmail,\n",
					"        CASE WHEN TRIM(decision) = '' THEN NULL ELSE decision END AS decision,\n",
					"        decisionDate,\n",
					"        CASE WHEN TRIM(withdrawnOrTurnedAway) = '' THEN NULL ELSE withdrawnOrTurnedAway END AS withdrawnOrTurnedAway,\n",
					"        withdrawnOrTurnedAwayDate,\n",
					"        CASE WHEN TRIM(comments) = '' THEN NULL ELSE comments END AS comments,\n",
					"        CASE WHEN TRIM(Migrated) = '' THEN NULL ELSE Migrated END AS Migrated,\n",
					"        CASE WHEN TRIM(ODTSourceSystem) = '' THEN NULL ELSE ODTSourceSystem END AS ODTSourceSystem,\n",
					"        'casework_specialist' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        CASE WHEN TRIM(message_id) = '' THEN NULL ELSE message_id END AS message_id,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        NULL AS RowID,\n",
					"        'Y' AS IsActive\n",
					"    FROM odw_standardised_db.casework_specialist\n",
					"    \"\"\")\n",
					"    \n",
					"    # Check the number of rows inserted\n",
					"    inserted_rows = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.casework_specialist\").collect()[0]['count']\n",
					"    result[\"record_count\"] = inserted_rows\n",
					"    logInfo(f\"Successfully inserted {inserted_rows} rows into odw_harmonised_db.casework_specialist\")\n",
					"    \n",
					"    # Update RowID with MD5 hash\n",
					"    logInfo(\"Starting RowID update with MD5 hash values\")\n",
					"    spark.sql(\"\"\"\n",
					"    UPDATE odw_harmonised_db.casework_specialist\n",
					"    SET RowID = md5(concat_ws('|', \n",
					"        casework_specialist_id, greenCaseType, greenCaseId, caseReference, horizonId, linkedGreenCaseId, \n",
					"        caseOfficerName, caseOfficerEmail, appealType, procedure, processingState, pinsLpaCode, pinsLpaName, \n",
					"        appellantName, agentName, siteAddressLine1, siteAddressLine2, siteTownCity, sitePostcode, otherPartyName, \n",
					"        receiptDate, validDate, startDate, lpaQuestionnaireDue, lpaQuestionnaireReceived, week6Date, week8Date, \n",
					"        week9Date, eventDate, eventTime, inspectorName, inspectorEmail, decision, decisionDate, \n",
					"        withdrawnOrTurnedAway, withdrawnOrTurnedAwayDate, comments, Migrated, ODTSourceSystem\n",
					"    ))\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully updated all RowID values with MD5 hashes\")\n",
					"    \n",
					"    # Verify data quality\n",
					"    null_rowid_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.casework_specialist WHERE RowID IS NULL\").collect()[0]['count']\n",
					"    if null_rowid_count > 0:\n",
					"        logError(f\"Data quality issue: {null_rowid_count} rows have NULL RowID values\")\n",
					"        result[\"status\"] = \"warning\"\n",
					"    else:\n",
					"        logInfo(\"Data quality check passed: No NULL RowID values found\")\n",
					"    \n",
					"    # Final success message\n",
					"    logInfo(\"Casework specialist data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information and limit to 300 characters\n",
					"    error_msg = f\"Error in casework specialist data processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Truncate error message if it exceeds 300 characters\n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1  # Indicate failure with -1 count\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}