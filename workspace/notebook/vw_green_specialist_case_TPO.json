{
	"name": "vw_green_specialist_case_TPO",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "63f02738-872b-4ebc-8227-b55a88cd43b9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmonised layer and build a view & table for Power BI use.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Casework Specialist curated view and table for Power BI"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"PipelineName = \"pln_curated\"\n",
					"PipelineRunID = \"5b66ce96-e547-4303-954a-2bfe138924bb\"\n",
					"PipelineTriggerID = \"8eb8c2f00fa446e88812104b6c8fe493\"\n",
					"PipelineTriggerName = \"Sandbox\"\n",
					"PipelineTriggerType = \"Manual\"\n",
					"PipelineTriggeredbyPipelineName = None\n",
					"PipelineTriggeredbyPipelineRunID = None"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum \n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_curated_db.pbi_green_specialist_case_TPO\"\n",
					"view_name = \"odw_curated_db.vw_green_specialist_case_TPO\"\n",
					"source_table = \"odw_harmonised_db.load_green_specialist_case\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''\n",
					"storage_account = ''"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Starting Casework Specialist curated table setup\")\n",
					"        \n",
					"        # Get storage account \n",
					"        storage_account = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"        logInfo(f\"Using storage account: {storage_account}\")\n",
					"        \n",
					"        # Fix the path\n",
					"        storage_account = storage_account.rstrip('/')\n",
					"        delta_table_path = f\"abfss://odw-curated@{storage_account}/green_specialist_case/pbi_green_specialist_case_TPO\"\n",
					"        logInfo(f\"Delta table will be created at: {delta_table_path}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in getting storage account configuration: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Create/refresh view\n",
					"        logInfo(f\"Creating view {view_name}\")\n",
					"        spark.sql(f\"\"\"\n",
					"        CREATE OR REPLACE VIEW {view_name} AS\n",
					"        SELECT  \n",
					"            greenCaseType,\n",
					"            greenCaseId,\n",
					"            caseReference,\n",
					"            horizonId,\n",
					"            linkedGreenCaseId,\n",
					"            caseOfficerName,\n",
					"            caseOfficerEmail,\n",
					"            appealType,\n",
					"            `procedure`,\n",
					"            processingState,\n",
					"            pinsLpaCode,\n",
					"            pinsLpaName,\n",
					"            appellantName,\n",
					"            agentName,\n",
					"            siteAddressDescription,\n",
					"            sitePostcode,\n",
					"            otherPartyName,\n",
					"            receiptDate,\n",
					"            validDate,\n",
					"            startDate,\n",
					"            lpaQuestionnaireDue,\n",
					"            lpaQuestionnaireReceived,\n",
					"            week6Date,\n",
					"            week8Date,\n",
					"            week9Date,\n",
					"            eventType,\n",
					"            eventDate,\n",
					"            eventTime,\n",
					"            inspectorName,\n",
					"            inspectorStaffNumber,\n",
					"            decision,\n",
					"            decisionDate,\n",
					"            withdrawnOrTurnedAwayDate,\n",
					"            comments,\n",
					"            active\n",
					"        FROM {source_table}  where greenCaseType = 'TPO'\n",
					"        \"\"\")\n",
					"        logInfo(f\"Successfully created view {view_name}\")\n",
					"        \n",
					"        # Count records in view\n",
					"        casework_count = spark.sql(f\"SELECT COUNT(*) as count FROM {view_name}\").collect()[0]['count']\n",
					"        logInfo(f\"View contains {casework_count} casework specialist records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in creating view {view_name}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Drop the table if it exists\n",
					"        logInfo(f\"Dropping table {spark_table_final} if it exists\")\n",
					"        spark.sql(f\"DROP TABLE IF EXISTS {spark_table_final}\")\n",
					"        logInfo(\"Table dropped or did not exist\")\n",
					"        \n",
					"        # Check if storage location exists before attempting cleanup\n",
					"        logInfo(f\"Checking storage location: {delta_table_path}\")\n",
					"        try:\n",
					"            if mssparkutils.fs.exists(delta_table_path):\n",
					"                logInfo(\"Storage location exists, cleaning up...\")\n",
					"                mssparkutils.fs.rm(delta_table_path, recurse=True)\n",
					"                logInfo(\"Storage location cleaned successfully\")\n",
					"            else:\n",
					"                logInfo(\"Storage location does not exist, no cleanup needed\")\n",
					"        except Exception as cleanup_error:\n",
					"            # Handle edge cases where exists() might behave unexpectedly\n",
					"            logInfo(f\"Storage location cleanup skipped: {str(cleanup_error)[:200]}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in dropping table {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        # Create table from view with specified location\n",
					"        logInfo(f\"Creating table {spark_table_final} from view with specified location\")\n",
					"        spark.sql(f\"\"\"\n",
					"        CREATE TABLE {spark_table_final}\n",
					"        USING delta\n",
					"        LOCATION '{delta_table_path}'\n",
					"        AS SELECT * FROM {view_name}\n",
					"        \"\"\")\n",
					"        \n",
					"        # Count records in table - this is our final record count\n",
					"        insert_count = spark.sql(f\"SELECT COUNT(*) as count FROM {spark_table_final}\").collect()[0]['count']\n",
					"        logInfo(f\"Created table with {insert_count} records at location: {delta_table_path}\")\n",
					"        \n",
					"        logInfo(\"Casework Specialist curated table setup completed successfully\")\n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in creating table {spark_table_final}: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"Successfully created {spark_table_final} table\"\n",
					"    if not error_message\n",
					"    else f\"Failed to create {spark_table_final} table\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    spark_table_final,\n",
					"    insert_count,\n",
					"    update_count,\n",
					"    delete_count,\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")"
				],
				"execution_count": 20
			}
		]
	}
}