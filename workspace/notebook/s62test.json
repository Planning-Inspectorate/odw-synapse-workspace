{
	"name": "s62test",
	"properties": {
		"folder": {
			"name": "archive/odw-harmonised/s62a_casework"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "57f38597-7284-4f70-bbe4-1ff7865bf363"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_utils_common_logging_output"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"# Define target table\n",
					"target_table = \"odw_harmonised_db.s62a_view_cases_dim\"\n",
					"\n",
					"# Initialize tracking variables\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message = ''\n",
					"\n",
					"# Initialize Application Insights logger\n",
					"app_insight_logger = ProcessingLogger()\n",
					"\n",
					"# Log start of processing\n",
					"logInfo('Starting s62a_view_cases_dim processing')\n",
					""
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    db_name = 'odw_harmonised_db'\n",
					"    table_name = 's62a_view_cases_dim'\n",
					"\n",
					"    def test_table_exists(db_name: str, table_name: str) -> bool:\n",
					"        spark.sql(f\"USE {db_name}\")\n",
					"        tables_df = spark.sql(\"SHOW TABLES\")\n",
					"        table_names = [row['tableName'] for row in tables_df.collect()]\n",
					"        return table_name in table_names\n",
					"\n",
					"    if test_table_exists(db_name, table_name):\n",
					"        logInfo(f\"Table {db_name}.{table_name} exists in harmonised, updating the harmonised layer\")\n",
					"    else:\n",
					"        logInfo(f\"Table {db_name}.{table_name} does not exist, creating table first.\")\n",
					"        mssparkutils.notebook.run('/py_odw_harmonised_table_creation', 300, {'specific_table': table_name})\n",
					"        logInfo(f\"Table {db_name}.{table_name} created\")\n",
					"\n",
					"except Exception as e:\n",
					"    logError(f\"Error in table existence check: {str(e)}\")\n",
					"    raise\n",
					""
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW s62a_view_cases_dim_new AS\n",
					"SELECT DISTINCT\n",
					"  CASE WHEN T1.CaseDataID IS NULL THEN T2.S62AViewCasesId ELSE NULL END AS S62AViewCasesId,\n",
					"  T1.Name AS Name,\n",
					"  T1.CaseRef AS CaseReference,\n",
					"  T1.CommentsDataID AS CommentsDataID,\n",
					"  T1.CaseDataID AS CaseDataID,\n",
					"  T1.Description AS Description,\n",
					"  '0' AS Migrated,\n",
					"  'Casework' AS ODTSourceSystem,\n",
					"  T3.SourceSystemID AS SourceSystemID,\n",
					"  to_timestamp(T1.expected_from) AS IngestionDate,\n",
					"  NULL AS ValidTo,\n",
					"  md5(\n",
					"    concat(\n",
					"      IFNULL(T1.Name, '.'),\n",
					"      IFNULL(T1.CaseReference, '.'),\n",
					"      IFNULL(T1.CommentsDataID, '.'),\n",
					"      IFNULL(T1.CaseDataID, '.'),\n",
					"      IFNULL(T1.Description, '.')\n",
					"    )\n",
					"  ) AS RowID,\n",
					"  'Y' AS IsActive,\n",
					"  T2.IsActive AS HistoricIsActive\n",
					"FROM odw_standardised_db.horizon_s62a_view_cases T1\n",
					"LEFT JOIN odw_harmonised_db.main_sourcesystem_fact T3 ON 'Casework' = T3.Description AND T3.IsActive = 'Y'\n",
					"FULL JOIN odw_harmonised_db.s62a_view_cases_dim T2 ON T1.CaseDataID = T2.CaseDataID AND T2.IsActive = 'Y'\n",
					"WHERE\n",
					"  (CASE\n",
					"    WHEN T1.CaseDataID = T2.CaseDataID AND md5(\n",
					"      concat(\n",
					"        IFNULL(T1.Name, '.'),\n",
					"        IFNULL(T1.CaseReference, '.'),\n",
					"        IFNULL(T1.CommentsDataID, '.'),\n",
					"        IFNULL(T1.CaseDataID, '.'),\n",
					"        IFNULL(T1.Description, '.')\n",
					"      )\n",
					"    ) <> T2.RowID THEN 'Y'\n",
					"    WHEN T2.CaseDataID IS NULL THEN 'Y'\n",
					"    ELSE 'N'\n",
					"  END) = 'Y'\n",
					"  AND T1.CaseDataID IS NOT NULL\n",
					"  AND T1.expected_from = (SELECT MAX(expected_from) FROM odw_standardised_db.horizon_s62a_view_cases);\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW s62a_view_cases_dim_changed_rows AS\n",
					"SELECT\n",
					"  S62AViewCasesId,\n",
					"  Name,\n",
					"  CaseReference,\n",
					"  CommentsDataID,\n",
					"  CaseDataID,\n",
					"  Description,\n",
					"  Migrated,\n",
					"  ODTSourceSystem,\n",
					"  SourceSystemID,\n",
					"  IngestionDate,\n",
					"  ValidTo,\n",
					"  RowID,\n",
					"  IsActive\n",
					"FROM s62a_view_cases_dim_new\n",
					"WHERE HistoricIsActive = 'Y' OR HistoricIsActive IS NULL\n",
					"\n",
					"UNION ALL\n",
					"\n",
					"SELECT\n",
					"  S62AViewCasesId,\n",
					"  Name,\n",
					"  CaseReference,\n",
					"  CommentsDataID,\n",
					"  CaseDataID,\n",
					"  Description,\n",
					"  Migrated,\n",
					"  ODTSourceSystem,\n",
					"  SourceSystemID,\n",
					"  IngestionDate,\n",
					"  ValidTo,\n",
					"  RowID,\n",
					"  IsActive\n",
					"FROM odw_harmonised_db.s62a_view_cases_dim\n",
					"WHERE CaseDataID IN (SELECT CaseDataID FROM s62a_view_cases_dim_new WHERE CaseDataID IS NULL)\n",
					"  AND IsActive = 'Y';\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW Loading_month AS\n",
					"SELECT DISTINCT\n",
					"  IngestionDate,\n",
					"  to_timestamp(date_sub(IngestionDate,1)) AS ClosingDate,\n",
					"  'Y' AS IsActive\n",
					"FROM s62a_view_cases_dim_new;\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW s62a_view_cases_dim_changed_rows_final AS\n",
					"SELECT\n",
					"  T1.S62AViewCasesId,\n",
					"  T1.Name,\n",
					"  T1.CaseReference,\n",
					"  T1.CommentsDataID,\n",
					"  T1.CaseDataID,\n",
					"  T1.Description,\n",
					"  T1.Migrated,\n",
					"  T1.ODTSourceSystem,\n",
					"  T1.SourceSystemID,\n",
					"  T1.IngestionDate,\n",
					"  T1.ValidTo,\n",
					"  T1.RowID,\n",
					"  T1.IsActive,\n",
					"  T2.ClosingDate\n",
					"FROM s62a_view_cases_dim_changed_rows T1\n",
					"FULL JOIN Loading_month T2 ON T1.IsActive = T2.IsActive;\n",
					""
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Merge updates and inserts\n",
					"MERGE INTO odw_harmonised_db.s62a_view_cases_dim AS Target\n",
					"USING s62a_view_cases_dim_changed_rows_final AS Source\n",
					"ON Source.CaseDataID = Target.CaseDataID AND Target.IsActive = 'Y'\n",
					"WHEN MATCHED THEN\n",
					"  UPDATE SET\n",
					"    Target.ValidTo = to_timestamp(ClosingDate),\n",
					"    Target.IsActive = 'N'\n",
					"WHEN NOT MATCHED THEN\n",
					"  INSERT (\n",
					"    S62AViewCasesId,\n",
					"    Name,\n",
					"    CaseReference,\n",
					"    CommentsDataID,\n",
					"    CaseDataID,\n",
					"    Description,\n",
					"    Migrated,\n",
					"    ODTSourceSystem,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"  )\n",
					"  VALUES (\n",
					"    Source.S62AViewCasesId,\n",
					"    Source.Name,\n",
					"    Source.CaseReference,\n",
					"    Source.CommentsDataID,\n",
					"    Source.CaseDataID,\n",
					"    Source.Description,\n",
					"    Source.Migrated,\n",
					"    Source.ODTSourceSystem,\n",
					"    Source.SourceSystemID,\n",
					"    Source.IngestionDate,\n",
					"    Source.ValidTo,\n",
					"    Source.RowID,\n",
					"    Source.IsActive\n",
					"  );\n",
					"\n",
					"-- Fix surrogate keys by re-assigning row numbers\n",
					"INSERT OVERWRITE odw_harmonised_db.s62a_view_cases_dim\n",
					"SELECT\n",
					"  ROW_NUMBER() OVER (ORDER BY S62AViewCasesId NULLS LAST) AS S62AViewCasesId,\n",
					"  Name,\n",
					"  CaseReference,\n",
					"  CommentsDataID,\n",
					"  CaseDataID,\n",
					"  Description,\n",
					"  Migrated,\n",
					"  ODTSourceSystem,\n",
					"  SourceSystemID,\n",
					"  IngestionDate,\n",
					"  ValidTo,\n",
					"  RowID,\n",
					"  IsActive\n",
					"FROM odw_harmonised_db.s62a_view_cases_dim;\n",
					""
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    # Get counts for changed rows and active rows\n",
					"    changed_rows_df = spark.sql(\"SELECT COUNT(*) as count FROM s62a_view_cases_dim_changed_rows_final\")\n",
					"    insert_count = changed_rows_df.collect()[0]['count']\n",
					"\n",
					"    active_rows_df = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.s62a_view_cases_dim WHERE IsActive = 'Y'\")\n",
					"    active_count = active_rows_df.collect()[0]['count']\n",
					"\n",
					"    update_count = 0  # Combined with inserts in this case; adjust if separate counts required\n",
					"    delete_count = 0  # Deletions handled via IsActive flag\n",
					"\n",
					"    logInfo(f's62a_view_cases_dim processing completed successfully. Processed {insert_count} changed rows. Final active record count: {active_count}')\n",
					"\n",
					"    end_exec_time = datetime.now()\n",
					"    total_exec_time = (end_exec_time - start_exec_time).total_seconds()\n",
					"\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=insert_count,\n",
					"        update_count=update_count,\n",
					"        delete_count=delete_count,\n",
					"        table_result=\"success\",\n",
					"        start_exec_time=start_exec_time.isoformat(),\n",
					"        end_exec_time=end_exec_time.isoformat(),\n",
					"        total_exec_time=total_exec_time,\n",
					"        error_message=\"\"\n",
					"    )\n",
					"\n",
					"except Exception as processing_error:\n",
					"    error_message = f'Error processing s62a_view_cases_dim data: {str(processing_error)}'\n",
					"    logError(error_message)\n",
					"\n",
					"    end_exec_time = datetime.now()\n",
					"    total_exec_time = (end_exec_time - start_exec_time).total_seconds()\n",
					"\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=0,\n",
					"        update_count=0,\n",
					"        delete_count=0,\n",
					"        table_result=\"failed\",\n",
					"        start_exec_time=start_exec_time.isoformat(),\n",
					"        end_exec_time=end_exec_time.isoformat(),\n",
					"        total_exec_time=total_exec_time,\n",
					"        error_message=error_message\n",
					"    )\n",
					"    raise\n",
					"\n",
					"# Generate and return processing results for notebook exit\n",
					"processing_results = app_insight_logger.generate_processing_results()\n",
					"mssparkutils.notebook.exit(processing_results)\n",
					""
				],
				"execution_count": 16
			}
		]
	}
}