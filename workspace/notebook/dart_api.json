{
	"name": "dart_api",
	"properties": {
		"folder": {
			"name": "odw-curated"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7a36ce56-38b5-431e-a9c7-7e62c72f7996"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to ingest into a single delta table, odw_curated_db.dart_api.\r\n",
					"\r\n",
					"**Description**  \r\n",
					"The functionality of this notebook is to ingest data from multiple table sources into a single odw_curated_db.dart_api delta table, reading from multiple odw_harmonised layer delta tables.The addtitional functionality has been added to log the audit information to Application Insight by creating a Json dump at notebook exit.\r\n",
					"\r\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql import DataFrame\r\n",
					"import json\r\n",
					"from delta.tables import DeltaTable"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise variables"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"table_name = \"odw_curated_db.dart_api\"\r\n",
					"start_exec_time = datetime.now()\r\n",
					"insert_count = 0\r\n",
					"update_count = 0\r\n",
					"delete_count = 0\r\n",
					"error_message=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Create DataFrame of required data"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Additional functionality to include appeals for Dart API (union Appeal_s78)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\r\n",
					"    df = spark.sql(\"\"\"\r\n",
					"        SELECT\r\n",
					"        h.caseId\r\n",
					"        ,h.caseReference\r\n",
					"        ,h.caseStatus\r\n",
					"        ,h.caseType\r\n",
					"        ,h.caseProcedure\r\n",
					"        ,h.lpaCode\r\n",
					"        ,l.lpaName\r\n",
					"        ,h.allocationLevel\r\n",
					"        ,h.allocationBand\r\n",
					"        ,h.caseSpecialisms\r\n",
					"        ,h.caseSubmittedDate\r\n",
					"        ,h.caseCreatedDate\r\n",
					"        ,h.caseUpdatedDate\r\n",
					"        ,h.caseValidDate\r\n",
					"        ,h.caseValidationDate\r\n",
					"        ,h.caseValidationOutcome\r\n",
					"        ,h.caseValidationInvalidDetails\r\n",
					"        ,h.caseValidationIncompleteDetails\r\n",
					"        ,h.caseExtensionDate\r\n",
					"        ,h.caseStartedDate\r\n",
					"        ,h.casePublishedDate\r\n",
					"        ,h.linkedCaseStatus\r\n",
					"        ,h.leadCaseReference\r\n",
					"        ,h.caseWithdrawnDate\r\n",
					"        ,h.caseTransferredDate\r\n",
					"        ,h.transferredCaseClosedDate\r\n",
					"        ,h.caseDecisionOutcomeDate\r\n",
					"        ,h.caseDecisionPublishedDate\r\n",
					"        ,h.caseDecisionOutcome\r\n",
					"        ,h.caseCompletedDate\r\n",
					"        ,h.enforcementNotice\r\n",
					"        ,h.applicationReference\r\n",
					"        ,h.applicationDate\r\n",
					"        ,h.applicationDecision\r\n",
					"        ,h.applicationDecisionDate as lpaDecisionDate\r\n",
					"        ,h.caseSubmissionDueDate\r\n",
					"        ,h.siteAddressLine1\r\n",
					"        ,h.siteAddressLine2\r\n",
					"        ,h.siteAddressTown\r\n",
					"        ,h.siteAddressCounty\r\n",
					"        ,h.siteAddressPostcode\r\n",
					"        ,h.isCorrectAppealType\r\n",
					"        ,h.originalDevelopmentDescription\r\n",
					"        ,h.changedDevelopmentDescription\r\n",
					"        ,h.newConditionDetails\r\n",
					"        ,h.nearbyCaseReferences\r\n",
					"        ,h.neighbouringSiteAddresses\r\n",
					"        ,h.affectedListedBuildingNumbers\r\n",
					"        ,h.appellantCostsAppliedFor\r\n",
					"        ,h.lpaCostsAppliedFor\r\n",
					"        ,su.firstName + ' ' + su.LastName as appellantName\r\n",
					"        ,e.eventType as typeOfEvent\r\n",
					"        ,e.eventStartDateTime as startDateOfTheEvent\r\n",
					"        ,ent_ins.givenName + ' ' +  ent_ins.surname as inspectorName\r\n",
					"        ,ent_co.givenName + ' ' +  ent_co.surname as caseOfficerName\r\n",
					"        ,i.qualifications as inspectorQualifications\r\n",
					"    FROM \r\n",
					"        odw_harmonised_db.sb_appeal_has h\r\n",
					"            left join odw_harmonised_db.pins_lpa l on h.lpaCode = l.pinsLpaCode and l.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.sb_service_user su on h.caseReference = su.caseReference and su.serviceUserType = 'Appellant' and su.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.sb_appeal_event e on h.caseReference = e.caseReference and e.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.entraid ent_ins on h.inspectorId = ent_ins.id and ent_ins.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.entraid ent_co on h.caseOfficerId = ent_co.id and ent_co.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.pins_inspector i on ent_ins.userPrincipalName = i.email and i.isActive = 'Y'\r\n",
					"    WHERE\r\n",
					"        h.IsActive = 'Y'\r\n",
					"        and\r\n",
					"        h.lpaCode <> 'Q9999'\r\n",
					"    \r\n",
					"    UNION    \r\n",
					"        SELECT\r\n",
					"        h.caseId\r\n",
					"        ,h.caseReference\r\n",
					"        ,h.caseStatus\r\n",
					"        ,h.caseType\r\n",
					"        ,h.caseProcedure\r\n",
					"        ,h.lpaCode\r\n",
					"        ,l.lpaName\r\n",
					"        ,h.allocationLevel\r\n",
					"        ,h.allocationBand\r\n",
					"        ,h.caseSpecialisms\r\n",
					"        ,h.caseSubmittedDate\r\n",
					"        ,h.caseCreatedDate\r\n",
					"        ,h.caseUpdatedDate\r\n",
					"        ,h.caseValidDate\r\n",
					"        ,h.caseValidationDate\r\n",
					"        ,h.caseValidationOutcome\r\n",
					"        ,h.caseValidationInvalidDetails\r\n",
					"        ,h.caseValidationIncompleteDetails\r\n",
					"        ,h.caseExtensionDate\r\n",
					"        ,h.caseStartedDate\r\n",
					"        ,h.casePublishedDate\r\n",
					"        ,h.linkedCaseStatus\r\n",
					"        ,h.leadCaseReference\r\n",
					"        ,h.caseWithdrawnDate\r\n",
					"        ,h.caseTransferredDate\r\n",
					"        ,h.transferredCaseClosedDate\r\n",
					"        ,h.caseDecisionOutcomeDate\r\n",
					"        ,h.caseDecisionPublishedDate\r\n",
					"        ,h.caseDecisionOutcome\r\n",
					"        ,h.caseCompletedDate\r\n",
					"        ,h.enforcementNotice\r\n",
					"        ,h.applicationReference\r\n",
					"        ,h.applicationDate\r\n",
					"        ,h.applicationDecision\r\n",
					"        ,h.applicationDecisionDate as lpaDecisionDate\r\n",
					"        ,h.caseSubmissionDueDate\r\n",
					"        ,h.siteAddressLine1\r\n",
					"        ,h.siteAddressLine2\r\n",
					"        ,h.siteAddressTown\r\n",
					"        ,h.siteAddressCounty\r\n",
					"        ,h.siteAddressPostcode\r\n",
					"        ,h.isCorrectAppealType\r\n",
					"        ,h.originalDevelopmentDescription\r\n",
					"        ,h.changedDevelopmentDescription\r\n",
					"        ,h.newConditionDetails\r\n",
					"        ,h.nearbyCaseReferences\r\n",
					"        ,h.neighbouringSiteAddresses\r\n",
					"        ,h.affectedListedBuildingNumbers\r\n",
					"        ,h.appellantCostsAppliedFor\r\n",
					"        ,h.lpaCostsAppliedFor\r\n",
					"        ,su.firstName + ' ' + su.LastName as appellantName\r\n",
					"        ,e.eventType as typeOfEvent\r\n",
					"        ,e.eventStartDateTime as startDateOfTheEvent\r\n",
					"        ,ent_ins.givenName + ' ' +  ent_ins.surname as inspectorName\r\n",
					"        ,ent_co.givenName + ' ' +  ent_co.surname as caseOfficerName\r\n",
					"        ,i.qualifications as inspectorQualifications\r\n",
					"    FROM \r\n",
					"        odw_harmonised_db.sb_appeal_s78 h\r\n",
					"            left join odw_harmonised_db.pins_lpa l on h.lpaCode = l.pinsLpaCode and l.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.sb_service_user su on h.caseReference = su.caseReference and su.serviceUserType = 'Appellant' and su.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.sb_appeal_event e on h.caseReference = e.caseReference and e.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.entraid ent_ins on h.inspectorId = ent_ins.id and ent_ins.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.entraid ent_co on h.caseOfficerId = ent_co.id and ent_co.isActive = 'Y'\r\n",
					"            left join odw_harmonised_db.pins_inspector i on ent_ins.userPrincipalName = i.email and i.isActive = 'Y'\r\n",
					"    WHERE\r\n",
					"        h.IsActive = 'Y'\r\n",
					"        and\r\n",
					"        h.lpaCode <> 'Q9999'\r\n",
					"        \"\"\"\r\n",
					"    )\r\n",
					"except Exception as e:\r\n",
					"    error_message = f\"Error appending data to the curated layer table : {str(e)[:800]}\"\r\n",
					"    end_exec_time = datetime.now()\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Write DataFrame to delta table\r\n",
					"Partition by `applicationReference` as this field only contains around 50 unique values and will be included in the search filters by users."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if not error_message:\r\n",
					"    try:\r\n",
					"    insert_count = df.count()\r\n",
					"    print(insert_count)\r\n",
					"\r\n",
					"    df.write.mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"applicationReference\") \\\r\n",
					"        .option(\"overwriteSchema\", \"true\") \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .saveAsTable(table_name)\r\n",
					"\r\n",
					"    print(f\"Written to {table_name}\")\r\n",
					"\r\n",
					"    end_exec_time = datetime.now()\r\n",
					"\r\n",
					"except Exception as e:\r\n",
					"    error_message = f\"Error appending data to the curated layer {table_name} table : {str(e)[:800]}\"\r\n",
					"    end_exec_time = datetime.now()\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Optimise by adding `ZOrderBy` for the column `caseReference`. This column has too many values for partitioning to be useful but is one of the columns used in the query filters by users. This ensures the minimum number of files are read when querying for a `caseReference`."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"delta_table = DeltaTable.forName(spark, table_name)\r\n",
					"delta_table.optimize().executeZOrderBy(\"caseReference\")\r\n",
					"delta_table.optimize()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#####  Logging Execution Metadata to Azure Application Insights"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\r\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\r\n",
					"stage = \"Success\" if not error_message else \"Failed\"\r\n",
					"status_message = (\r\n",
					"    f\"Successfully loaded data into {table_name} table\"\r\n",
					"    if not error_message\r\n",
					"    else f\"Failed to load data from {table_name} table\"\r\n",
					")\r\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\r\n",
					"\r\n",
					"log_telemetry_and_exit(\r\n",
					"    stage,\r\n",
					"    start_exec_time,\r\n",
					"    end_exec_time,\r\n",
					"    error_message,\r\n",
					"    table_name,\r\n",
					"    insert_count,\r\n",
					"    update_count,\r\n",
					"    delete_count,\r\n",
					"    PipelineName,\r\n",
					"    PipelineRunID,\r\n",
					"    PipelineTriggerID,\r\n",
					"    PipelineTriggerName,\r\n",
					"    PipelineTriggerType,\r\n",
					"    PipelineTriggeredbyPipelineName,\r\n",
					"    PipelineTriggeredbyPipelineRunID,\r\n",
					"    activity_type,\r\n",
					"    duration_seconds,\r\n",
					"    status_message,\r\n",
					"    status_code\r\n",
					")"
				],
				"execution_count": null
			}
		]
	}
}