{
	"name": "lineage_test",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "sparkConfiguration1",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ac7f202b-2639-4fa9-9cfd-dcdc00793dc7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "sparkConfiguration1"
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from notebookutils import mssparkutils\n",
					"\n",
					"# List and delete orphaned files at month level\n",
					"base_path = \"abfss://openlineage-events@pinsstodwdevuksl92q7c.dfs.core.windows.net/y=2025/m=11\"\n",
					"\n",
					"files = mssparkutils.fs.ls(base_path)\n",
					"\n",
					"for file in files:\n",
					"    # Delete files (not directories) that look like d=XX\n",
					"    if file.isFile and file.name.startswith(\"d=\"):\n",
					"        print(f\"Deleting orphaned file: {file.name}\")\n",
					"        mssparkutils.fs.rm(file.path)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"\"\"\"\n",
					"SAP HR Monthly Lineage Generator - FINAL CORRECTED VERSION\n",
					"Uses EXACT qualified names from Purview\n",
					"\"\"\"\n",
					"\n",
					"import json\n",
					"import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"print(\"=\"*80)\n",
					"print(\"SAP HR MONTHLY LINEAGE GENERATION - FINAL\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"# ============================================================================\n",
					"# CONFIGURATION - CORRECTED QUALIFIED NAMES!\n",
					"# ============================================================================\n",
					"STORAGE_ACCOUNT = \"pinsstodwdevuksl92q7c\"\n",
					"\n",
					"# Schema file paths\n",
					"SOURCE_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/standardised_table_definitions/saphr/sap_hr_history_monthly.json\"\n",
					"TARGET_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/harmonised_table_definitions/saphr/load_sap_hr_monthly.json\"\n",
					"\n",
					"# Events storage\n",
					"EVENTS_PATH = f\"abfss://openlineage-events@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
					"\n",
					"# CORRECTED: Exact qualified names from Purview (with -ondemand and /dbo/)\n",
					"SOURCE_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_standardised_db/dbo/sap_hr_history_monthly\"\n",
					"TARGET_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_harmonised_db/dbo/load_sap_hr_monthly\"\n",
					"\n",
					"# Namespace for the job\n",
					"JOB_NAMESPACE = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net\"\n",
					"\n",
					"print(\"\\n✅ Configuration:\")\n",
					"print(f\"   Source QN: {SOURCE_QUALIFIED_NAME}\")\n",
					"print(f\"   Target QN: {TARGET_QUALIFIED_NAME}\")\n",
					"\n",
					"# ============================================================================\n",
					"# STEP 1: Load Schema Files\n",
					"# ============================================================================\n",
					"print(\"\\nStep 1: Loading schema files...\")\n",
					"\n",
					"# Load source schema\n",
					"try:\n",
					"    source_content = mssparkutils.fs.head(SOURCE_SCHEMA_PATH, 100000)\n",
					"    source_schema = json.loads(source_content)\n",
					"    source_columns = len(source_schema.get('fields', []))\n",
					"    print(f\" Source schema loaded: {source_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load source schema: {e}\")\n",
					"    print(f\"   Continuing without schema details...\")\n",
					"    source_schema = None\n",
					"\n",
					"# Load target schema\n",
					"try:\n",
					"    target_content = mssparkutils.fs.head(TARGET_SCHEMA_PATH, 100000)\n",
					"    target_schema = json.loads(target_content)\n",
					"    target_columns = len(target_schema.get('fields', []))\n",
					"    print(f\" Target schema loaded: {target_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load target schema: {e}\")\n",
					"    print(f\"   Continuing without schema details...\")\n",
					"    target_schema = None\n",
					"\n",
					"# ============================================================================\n",
					"# STEP 2: Build OpenLineage Event with Correct Qualified Names\n",
					"# ============================================================================\n",
					"print(\"\\nStep 2: Building OpenLineage event...\")\n",
					"\n",
					"run_id = f\"load_sap_hr_monthly-{datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')}\"\n",
					"print(f\"   Run ID: {run_id}\")\n",
					"\n",
					"# Build INPUT dataset - using EXACT qualified name from Purview\n",
					"input_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": SOURCE_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"# Add source schema if available\n",
					"if source_schema:\n",
					"    input_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"string\")\n",
					"                }\n",
					"                for field in source_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Build OUTPUT dataset - using EXACT qualified name from Purview\n",
					"output_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": TARGET_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"# Add target schema if available\n",
					"if target_schema:\n",
					"    output_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"STRING\")\n",
					"                }\n",
					"                for field in target_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Create complete OpenLineage event\n",
					"event = {\n",
					"    \"eventType\": \"COMPLETE\",\n",
					"    \"eventTime\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
					"    \"producer\": \"sap-hr-lineage-generator\",\n",
					"    \"job\": {\n",
					"        \"namespace\": JOB_NAMESPACE,\n",
					"        \"name\": \"load_sap_hr_monthly\",\n",
					"        \"facets\": {\n",
					"            \"documentation\": {\n",
					"                \"description\": \"SAP HR monthly data transformation from standardised to harmonised layer\"\n",
					"            }\n",
					"        }\n",
					"    },\n",
					"    \"run\": {\n",
					"        \"runId\": run_id\n",
					"    },\n",
					"    \"inputs\": [input_dataset],\n",
					"    \"outputs\": [output_dataset]\n",
					"}\n",
					"\n",
					"print(\" Event created with correct qualified names!\")\n",
					"\n",
					"# ============================================================================\n",
					"# STEP 3: Write Event to Storage\n",
					"# ============================================================================\n",
					"print(\"\\nStep 3: Writing event to blob storage...\")\n",
					"\n",
					"now = datetime.datetime.utcnow()\n",
					"year, month, day = now.strftime(\"%Y,%m,%d\").split(',')\n",
					"filename = f\"y={year}/m={month}/d={day}/{run_id}.json\"\n",
					"full_path = f\"{EVENTS_PATH}/{filename}\"\n",
					"\n",
					"print(f\"   Writing to: {filename}\")\n",
					"\n",
					"try:\n",
					"    mssparkutils.fs.put(full_path, json.dumps(event, indent=2), True)\n",
					"    print(f\" Event written successfully!\")\n",
					"except Exception as e:\n",
					"    print(f\" ERROR writing event: {e}\")\n",
					"    raise\n",
					"\n",
					"# ============================================================================\n",
					"# STEP 4: Verify Event Was Created\n",
					"# ============================================================================\n",
					"print(\"\\nStep 4: Verifying event file...\")\n",
					"\n",
					"try:\n",
					"    folder_path = f\"{EVENTS_PATH}/y={year}/m={month}/d={day}/\"\n",
					"    files = mssparkutils.fs.ls(folder_path)\n",
					"    our_event = [f for f in files if run_id in f.name]\n",
					"    \n",
					"    if our_event:\n",
					"        print(f\" Event file verified!\")\n",
					"        print(f\"   File name: {our_event[0].name}\")\n",
					"        print(f\"   File size: {our_event[0].size} bytes\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not verify (file may still exist): {e}\")\n",
					"\n",
					"# ============================================================================\n",
					"# SUMMARY\n",
					"# ============================================================================\n",
					"print(\"\\n\" + \"=\"*80)\n",
					"print(\"LINEAGE EVENT GENERATION COMPLETE!\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"print(f\"\"\"\n",
					" Event Created with CORRECT Qualified Names:\n",
					"   \n",
					"   Source: {SOURCE_QUALIFIED_NAME}\n",
					"   Target: {TARGET_QUALIFIED_NAME}\n",
					"   \n",
					"   Job Name: load_sap_hr_monthly\n",
					"   Run ID: {run_id}\n",
					"   Event File: {filename}\n",
					"\n",
					" \n",
					"\"\"\")\n",
					"\n",
					"print(\"=\"*80)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"\"\"\"\n",
					"SAP HR Monthly Lineage Generator - DATE-BASED RUN IDS\n",
					"Uses date instead of timestamp to avoid duplicate operations\n",
					"\"\"\"\n",
					"\n",
					"import json\n",
					"import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"print(\"=\"*80)\n",
					"print(\"SAP HR MONTHLY LINEAGE GENERATION - DATE-BASED\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"\n",
					"# CONFIGURATION\n",
					"\n",
					"STORAGE_ACCOUNT = \"pinsstodwdevuksl92q7c\"\n",
					"\n",
					"# Schema file paths\n",
					"SOURCE_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/standardised_table_definitions/saphr/sap_hr_history_monthly.json\"\n",
					"TARGET_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/harmonised_table_definitions/saphr/load_sap_hr_monthly.json\"\n",
					"\n",
					"# Events storage\n",
					"EVENTS_PATH = f\"abfss://openlineage-events@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
					"\n",
					"# Qualified names\n",
					"SOURCE_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_standardised_db/dbo/sap_hr_history_monthly\"\n",
					"TARGET_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_harmonised_db/dbo/load_sap_hr_monthly\"\n",
					"\n",
					"# Namespace\n",
					"JOB_NAMESPACE = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net\"\n",
					"\n",
					"print(\"\\n Configuration loaded\")\n",
					"\n",
					"\n",
					"# LOAD SCHEMAS\n",
					"\n",
					"print(\"\\nStep 1: Loading schema files...\")\n",
					"\n",
					"try:\n",
					"    source_content = mssparkutils.fs.head(SOURCE_SCHEMA_PATH, 100000)\n",
					"    source_schema = json.loads(source_content)\n",
					"    source_columns = len(source_schema.get('fields', []))\n",
					"    print(f\"Source schema loaded: {source_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load source schema: {e}\")\n",
					"    source_schema = None\n",
					"\n",
					"try:\n",
					"    target_content = mssparkutils.fs.head(TARGET_SCHEMA_PATH, 100000)\n",
					"    target_schema = json.loads(target_content)\n",
					"    target_columns = len(target_schema.get('fields', []))\n",
					"    print(f\" Target schema loaded: {target_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load target schema: {e}\")\n",
					"    target_schema = None\n",
					"\n",
					"\n",
					"# BUILD EVENT WITH DATE-BASED RUN ID\n",
					"\n",
					"print(\"\\nStep 2: Building OpenLineage event...\")\n",
					"\n",
					"# DATE-BASED RUN ID - Same for all runs on same day\n",
					"# This prevents duplicate operation nodes!\n",
					"today = datetime.datetime.utcnow().strftime('%Y-%m-%d')\n",
					"run_id = f\"load_sap_hr_monthly-{today}\"\n",
					"\n",
					"print(f\"   Run ID: {run_id}\")\n",
					"print(f\"    All runs today will use same ID - avoids duplicates!\")\n",
					"\n",
					"# Build INPUT dataset\n",
					"input_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": SOURCE_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"if source_schema:\n",
					"    input_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"string\")\n",
					"                }\n",
					"                for field in source_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Build OUTPUT dataset\n",
					"output_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": TARGET_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"if target_schema:\n",
					"    output_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"STRING\")\n",
					"                }\n",
					"                for field in target_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Create complete OpenLineage event\n",
					"event = {\n",
					"    \"eventType\": \"COMPLETE\",\n",
					"    \"eventTime\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
					"    \"producer\": \"sap-hr-lineage-generator\",\n",
					"    \"job\": {\n",
					"        \"namespace\": JOB_NAMESPACE,\n",
					"        \"name\": \"load_sap_hr_monthly\",\n",
					"        \"facets\": {\n",
					"            \"documentation\": {\n",
					"                \"description\": \"SAP HR monthly data transformation from standardised to harmonised layer\"\n",
					"            }\n",
					"        }\n",
					"    },\n",
					"    \"run\": {\n",
					"        \"runId\": run_id\n",
					"    },\n",
					"    \"inputs\": [input_dataset],\n",
					"    \"outputs\": [output_dataset]\n",
					"}\n",
					"\n",
					"print(\" Event created with date-based run ID\")\n",
					"\n",
					"\n",
					"# WRITE EVENT TO STORAGE\n",
					"\n",
					"print(\"\\nStep 3: Writing event to blob storage...\")\n",
					"\n",
					"now = datetime.datetime.utcnow()\n",
					"year, month, day = now.strftime(\"%Y,%m,%d\").split(',')\n",
					"\n",
					"# Use run_id as filename - overwrites previous runs from same day\n",
					"filename = f\"y={year}/m={month}/d={day}/{run_id}.json\"\n",
					"full_path = f\"{EVENTS_PATH}/{filename}\"\n",
					"\n",
					"print(f\"   Writing to: {filename}\")\n",
					"\n",
					"try:\n",
					"    mssparkutils.fs.put(full_path, json.dumps(event, indent=2), True)\n",
					"    print(f\"Event written successfully!\")\n",
					"    print(f\"   Running again today will OVERWRITE this file (not create duplicate)\")\n",
					"except Exception as e:\n",
					"    print(f\" ERROR writing event: {e}\")\n",
					"    raise\n",
					"\n",
					"\n",
					"# SUMMARY\n",
					"\n",
					"print(\"\\n\" + \"=\"*80)\n",
					"print(\"LINEAGE EVENT GENERATION COMPLETE!\")\n",
					"print(\"=\"*80)\n",
					"\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"\"\"\"\n",
					"SAP HR Monthly Lineage Generator - DATE-BASED RUN IDS\n",
					"Uses date instead of timestamp to avoid duplicate operations\n",
					"\"\"\"\n",
					"\n",
					"import json\n",
					"import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"print(\"=\"*80)\n",
					"print(\"SAP HR MONTHLY LINEAGE GENERATION - DATE-BASED\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"\n",
					"# CONFIGURATION\n",
					"\n",
					"STORAGE_ACCOUNT = \"pinsstodwdevuksl92q7c\"\n",
					"\n",
					"# Schema file paths\n",
					"SOURCE_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/standardised_table_definitions/saphr/sap_hr_weekly.json\"\n",
					"TARGET_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/harmonised_table_definitions/saphr/load_sap_hr_weekly.json\"\n",
					"\n",
					"# Events storage\n",
					"EVENTS_PATH = f\"abfss://openlineage-events@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
					"\n",
					"# Qualified names\n",
					"SOURCE_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_standardised_db/dbo/sap_hr_weekly\"\n",
					"TARGET_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_harmonised_db/dbo/load_sap_hr_weekly\"\n",
					"\n",
					"# Namespace\n",
					"JOB_NAMESPACE = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net\"\n",
					"\n",
					"print(\"\\n Configuration loaded\")\n",
					"\n",
					"\n",
					"# LOAD SCHEMAS\n",
					"\n",
					"print(\"\\nStep 1: Loading schema files...\")\n",
					"\n",
					"try:\n",
					"    source_content = mssparkutils.fs.head(SOURCE_SCHEMA_PATH, 100000)\n",
					"    source_schema = json.loads(source_content)\n",
					"    source_columns = len(source_schema.get('fields', []))\n",
					"    print(f\"Source schema loaded: {source_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load source schema: {e}\")\n",
					"    source_schema = None\n",
					"\n",
					"try:\n",
					"    target_content = mssparkutils.fs.head(TARGET_SCHEMA_PATH, 100000)\n",
					"    target_schema = json.loads(target_content)\n",
					"    target_columns = len(target_schema.get('fields', []))\n",
					"    print(f\" Target schema loaded: {target_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load target schema: {e}\")\n",
					"    target_schema = None\n",
					"\n",
					"\n",
					"# BUILD EVENT WITH DATE-BASED RUN ID\n",
					"\n",
					"print(\"\\nStep 2: Building OpenLineage event...\")\n",
					"\n",
					"# DATE-BASED RUN ID - Same for all runs on same day\n",
					"# This prevents duplicate operation nodes!\n",
					"today = datetime.datetime.utcnow().strftime('%Y-%m-%d')\n",
					"run_id = f\"load_sap_hr_weekly-{today}\"\n",
					"\n",
					"print(f\"   Run ID: {run_id}\")\n",
					"print(f\"    All runs today will use same ID - avoids duplicates!\")\n",
					"\n",
					"# Build INPUT dataset\n",
					"input_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": SOURCE_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"if source_schema:\n",
					"    input_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"string\")\n",
					"                }\n",
					"                for field in source_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Build OUTPUT dataset\n",
					"output_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": TARGET_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"if target_schema:\n",
					"    output_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"STRING\")\n",
					"                }\n",
					"                for field in target_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Create complete OpenLineage event\n",
					"event = {\n",
					"    \"eventType\": \"COMPLETE\",\n",
					"    \"eventTime\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
					"    \"producer\": \"sap-hr-lineage-generator\",\n",
					"    \"job\": {\n",
					"        \"namespace\": JOB_NAMESPACE,\n",
					"        \"name\": \"load_sap_hr_weekly\",\n",
					"        \"facets\": {\n",
					"            \"documentation\": {\n",
					"                \"description\": \"SAP HR monthly data transformation from standardised to harmonised layer\"\n",
					"            }\n",
					"        }\n",
					"    },\n",
					"    \"run\": {\n",
					"        \"runId\": run_id\n",
					"    },\n",
					"    \"inputs\": [input_dataset],\n",
					"    \"outputs\": [output_dataset]\n",
					"}\n",
					"\n",
					"print(\" Event created with date-based run ID\")\n",
					"\n",
					"\n",
					"# WRITE EVENT TO STORAGE\n",
					"\n",
					"print(\"\\nStep 3: Writing event to blob storage...\")\n",
					"\n",
					"now = datetime.datetime.utcnow()\n",
					"year, month, day = now.strftime(\"%Y,%m,%d\").split(',')\n",
					"\n",
					"# Use run_id as filename - overwrites previous runs from same day\n",
					"filename = f\"y={year}/m={month}/d={day}/{run_id}.json\"\n",
					"full_path = f\"{EVENTS_PATH}/{filename}\"\n",
					"\n",
					"print(f\"   Writing to: {filename}\")\n",
					"\n",
					"try:\n",
					"    mssparkutils.fs.put(full_path, json.dumps(event, indent=2), True)\n",
					"    print(f\"Event written successfully!\")\n",
					"    print(f\"   Running again today will OVERWRITE this file (not create duplicate)\")\n",
					"except Exception as e:\n",
					"    print(f\" ERROR writing event: {e}\")\n",
					"    raise\n",
					"\n",
					"\n",
					"# SUMMARY\n",
					"\n",
					"print(\"\\n\" + \"=\"*80)\n",
					"print(\"LINEAGE EVENT GENERATION COMPLETE!\")\n",
					"print(\"=\"*80)\n",
					"\n",
					""
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"\"\"\"\n",
					"Inspector Specialisms Lineage Generator - DATE-BASED RUN IDS\n",
					"Uses date instead of timestamp to avoid duplicate operations\n",
					"\"\"\"\n",
					"\n",
					"import json\n",
					"import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"print(\"=\"*80)\n",
					"print(\"INSPECTOR SPECIALISMS LINEAGE GENERATION - DATE-BASED\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"# ============================================================================\n",
					"# CONFIGURATION\n",
					"# ============================================================================\n",
					"STORAGE_ACCOUNT = \"pinsstodwdevuksl92q7c\"\n",
					"\n",
					"# Schema file paths\n",
					"SOURCE_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/standardised_table_definitions/saphr/inspector_specialisms_monthly.json\"\n",
					"TARGET_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/harmonised_table_definitions/saphr/sap_hr_inspector_Specialisms.json\"\n",
					"\n",
					"# Events storage\n",
					"EVENTS_PATH = f\"abfss://openlineage-events@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
					"\n",
					"# Qualified names (matching Purview format)\n",
					"SOURCE_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_standardised_db/dbo/inspector_specialisms_monthly\"\n",
					"TARGET_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_harmonised_db/dbo/sap_hr_inspector_specialisms\"\n",
					"\n",
					"# Namespace\n",
					"JOB_NAMESPACE = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net\"\n",
					"\n",
					"print(\"\\n Configuration loaded\")\n",
					"\n",
					"# ============================================================================\n",
					"# LOAD SCHEMAS\n",
					"# ============================================================================\n",
					"print(\"\\nStep 1: Loading schema files...\")\n",
					"\n",
					"try:\n",
					"    source_content = mssparkutils.fs.head(SOURCE_SCHEMA_PATH, 100000)\n",
					"    source_schema = json.loads(source_content)\n",
					"    source_columns = len(source_schema.get('fields', []))\n",
					"    print(f\" Source schema loaded: {source_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load source schema: {e}\")\n",
					"    source_schema = None\n",
					"\n",
					"try:\n",
					"    target_content = mssparkutils.fs.head(TARGET_SCHEMA_PATH, 100000)\n",
					"    target_schema = json.loads(target_content)\n",
					"    target_columns = len(target_schema.get('fields', []))\n",
					"    print(f\" Target schema loaded: {target_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load target schema: {e}\")\n",
					"    target_schema = None\n",
					"\n",
					"# ============================================================================\n",
					"# BUILD EVENT WITH DATE-BASED RUN ID\n",
					"# ============================================================================\n",
					"print(\"\\nStep 2: Building OpenLineage event...\")\n",
					"\n",
					"# DATE-BASED RUN ID - Same for all runs on same day\n",
					"# This prevents duplicate operation nodes!\n",
					"today = datetime.datetime.utcnow().strftime('%Y-%m-%d')\n",
					"run_id = f\"load_inspector_specialisms-{today}\"\n",
					"\n",
					"print(f\"   Run ID: {run_id}\")\n",
					"print(f\"    All runs today will use same ID - avoids duplicates!\")\n",
					"\n",
					"# Build INPUT dataset\n",
					"input_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": SOURCE_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"if source_schema:\n",
					"    input_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"string\")\n",
					"                }\n",
					"                for field in source_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Build OUTPUT dataset\n",
					"output_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": TARGET_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"if target_schema:\n",
					"    output_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"STRING\")\n",
					"                }\n",
					"                for field in target_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Create complete OpenLineage event\n",
					"event = {\n",
					"    \"eventType\": \"COMPLETE\",\n",
					"    \"eventTime\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
					"    \"producer\": \"inspector-specialisms-lineage-generator\",\n",
					"    \"job\": {\n",
					"        \"namespace\": JOB_NAMESPACE,\n",
					"        \"name\": \"load_inspector_specialisms\",\n",
					"        \"facets\": {\n",
					"            \"documentation\": {\n",
					"                \"description\": \"Inspector specialisms data transformation from standardised to harmonised layer\"\n",
					"            }\n",
					"        }\n",
					"    },\n",
					"    \"run\": {\n",
					"        \"runId\": run_id\n",
					"    },\n",
					"    \"inputs\": [input_dataset],\n",
					"    \"outputs\": [output_dataset]\n",
					"}\n",
					"\n",
					"print(\" Event created with date-based run ID\")\n",
					"\n",
					"# ============================================================================\n",
					"# WRITE EVENT TO STORAGE\n",
					"# ============================================================================\n",
					"print(\"\\nStep 3: Writing event to blob storage...\")\n",
					"\n",
					"now = datetime.datetime.utcnow()\n",
					"year, month, day = now.strftime(\"%Y,%m,%d\").split(',')\n",
					"\n",
					"# Use run_id as filename - overwrites previous runs from same day\n",
					"filename = f\"y={year}/m={month}/d={day}/{run_id}.json\"\n",
					"full_path = f\"{EVENTS_PATH}/{filename}\"\n",
					"\n",
					"print(f\"   Writing to: {filename}\")\n",
					"\n",
					"try:\n",
					"    mssparkutils.fs.put(full_path, json.dumps(event, indent=2), True)\n",
					"    print(f\" Event written successfully!\")\n",
					"    print(f\"    Running again today will OVERWRITE this file (not create duplicate)\")\n",
					"except Exception as e:\n",
					"    print(f\" ERROR writing event: {e}\")\n",
					"    raise\n",
					"\n",
					"# ============================================================================\n",
					"# SUMMARY\n",
					"# ============================================================================\n",
					"print(\"\\n\" + \"=\"*80)\n",
					"print(\"LINEAGE EVENT GENERATION COMPLETE!\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"print(f\"\"\"\n",
					"Event Created with DATE-BASED Run ID:\n",
					"   \n",
					"\n",
					"\"\"\")\n",
					"\n",
					"print(\"=\"*80)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"\"\"\"\n",
					"Inspector Specialisms Lineage Generator - DATE-BASED RUN IDS\n",
					"Uses date instead of timestamp to avoid duplicate operations\n",
					"\"\"\"\n",
					"\n",
					"import json\n",
					"import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"print(\"=\"*80)\n",
					"print(\"INSPECTOR SPECIALISMS LINEAGE GENERATION - DATE-BASED\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"# ============================================================================\n",
					"# CONFIGURATION\n",
					"# ============================================================================\n",
					"STORAGE_ACCOUNT = \"pinsstodwdevuksl92q7c\"\n",
					"\n",
					"# Schema file paths\n",
					"SOURCE_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/standardised_table_definitions/saphr/inspector_specialisms_monthly.json\"\n",
					"TARGET_SCHEMA_PATH = f\"abfss://odw-config@{STORAGE_ACCOUNT}.dfs.core.windows.net/harmonised_table_definitions/saphr/sap_hr_inspector_Specialisms.json\"\n",
					"\n",
					"# Events storage\n",
					"EVENTS_PATH = f\"abfss://openlineage-events@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
					"\n",
					"# Qualified names (matching Purview format)\n",
					"SOURCE_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_standardised_db/dbo/inspector_specialisms_monthly\"\n",
					"TARGET_QUALIFIED_NAME = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net/odw_harmonised_db/dbo/sap_hr_inspector_specialisms\"\n",
					"\n",
					"# Namespace\n",
					"JOB_NAMESPACE = \"mssql://pins-synw-odw-dev-uks-ondemand.sql.azuresynapse.net\"\n",
					"\n",
					"print(\"\\n✅ Configuration loaded\")\n",
					"\n",
					"# ============================================================================\n",
					"# LOAD SCHEMAS\n",
					"# ============================================================================\n",
					"print(\"\\nStep 1: Loading schema files...\")\n",
					"\n",
					"try:\n",
					"    source_content = mssparkutils.fs.head(SOURCE_SCHEMA_PATH, 100000)\n",
					"    source_schema = json.loads(source_content)\n",
					"    source_columns = len(source_schema.get('fields', []))\n",
					"    print(f\" Source schema loaded: {source_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load source schema: {e}\")\n",
					"    source_schema = None\n",
					"\n",
					"try:\n",
					"    target_content = mssparkutils.fs.head(TARGET_SCHEMA_PATH, 100000)\n",
					"    target_schema = json.loads(target_content)\n",
					"    target_columns = len(target_schema.get('fields', []))\n",
					"    print(f\" Target schema loaded: {target_columns} columns\")\n",
					"except Exception as e:\n",
					"    print(f\"  Could not load target schema: {e}\")\n",
					"    target_schema = None\n",
					"\n",
					"# ============================================================================\n",
					"# BUILD EVENT WITH DATE-BASED RUN ID\n",
					"# ============================================================================\n",
					"print(\"\\nStep 2: Building OpenLineage event...\")\n",
					"\n",
					"# DATE-BASED RUN ID - Same for all runs on same day\n",
					"# This prevents duplicate operation nodes!\n",
					"today = datetime.datetime.utcnow().strftime('%Y-%m-%d')\n",
					"run_id = f\"load_inspector_specialisms-{today}\"\n",
					"\n",
					"print(f\"   Run ID: {run_id}\")\n",
					"print(f\"    All runs today will use same ID - avoids duplicates!\")\n",
					"\n",
					"# Build INPUT dataset\n",
					"input_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": SOURCE_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"if source_schema:\n",
					"    input_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"string\")\n",
					"                }\n",
					"                for field in source_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Build OUTPUT dataset\n",
					"output_dataset = {\n",
					"    \"namespace\": JOB_NAMESPACE,\n",
					"    \"name\": TARGET_QUALIFIED_NAME\n",
					"}\n",
					"\n",
					"if target_schema:\n",
					"    output_dataset[\"facets\"] = {\n",
					"        \"schema\": {\n",
					"            \"fields\": [\n",
					"                {\n",
					"                    \"name\": field.get(\"name\"),\n",
					"                    \"type\": field.get(\"type\", \"STRING\")\n",
					"                }\n",
					"                for field in target_schema.get(\"fields\", [])\n",
					"            ]\n",
					"        }\n",
					"    }\n",
					"\n",
					"# Create complete OpenLineage event\n",
					"event = {\n",
					"    \"eventType\": \"COMPLETE\",\n",
					"    \"eventTime\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
					"    \"producer\": \"inspector-specialisms-lineage-generator\",\n",
					"    \"job\": {\n",
					"        \"namespace\": JOB_NAMESPACE,\n",
					"        \"name\": \"load_inspector_specialisms\",\n",
					"        \"facets\": {\n",
					"            \"documentation\": {\n",
					"                \"description\": \"Inspector specialisms data transformation from standardised to harmonised layer\"\n",
					"            }\n",
					"        }\n",
					"    },\n",
					"    \"run\": {\n",
					"        \"runId\": run_id\n",
					"    },\n",
					"    \"inputs\": [input_dataset],\n",
					"    \"outputs\": [output_dataset]\n",
					"}\n",
					"\n",
					"print(\" Event created with date-based run ID\")\n",
					"\n",
					"# ============================================================================\n",
					"# WRITE EVENT TO STORAGE\n",
					"# ============================================================================\n",
					"print(\"\\nStep 3: Writing event to blob storage...\")\n",
					"\n",
					"now = datetime.datetime.utcnow()\n",
					"year, month, day = now.strftime(\"%Y,%m,%d\").split(',')\n",
					"\n",
					"# Use run_id as filename - overwrites previous runs from same day\n",
					"filename = f\"y={year}/m={month}/d={day}/{run_id}.json\"\n",
					"full_path = f\"{EVENTS_PATH}/{filename}\"\n",
					"\n",
					"print(f\"   Writing to: {filename}\")\n",
					"\n",
					"try:\n",
					"    mssparkutils.fs.put(full_path, json.dumps(event, indent=2), True)\n",
					"    print(f\" Event written successfully!\")\n",
					"    print(f\"    Running again today will OVERWRITE this file (not create duplicate)\")\n",
					"except Exception as e:\n",
					"    print(f\" ERROR writing event: {e}\")\n",
					"    raise\n",
					"\n",
					"# ============================================================================\n",
					"# SUMMARY\n",
					"# ============================================================================\n",
					"print(\"\\n\" + \"=\"*80)\n",
					"print(\"LINEAGE EVENT GENERATION COMPLETE!\")\n",
					"print(\"=\"*80)\n",
					"\n",
					"print(f\"\"\"\n",
					" Event Created with DATE-BASED Run ID:\n",
					"   \n",
					"   Run ID: {run_id}\n",
					"   File: {filename}\n",
					"   \n",
					"   Source: {SOURCE_QUALIFIED_NAME}\n",
					"   Target: {TARGET_QUALIFIED_NAME}\n",
					"\n",
					" KEY BENEFIT:\n",
					"   • All runs TODAY use same run_id: {run_id}\n",
					"   • File gets OVERWRITTEN (not duplicated)\n",
					"   • Purview shows ONE operation per day (not multiple!)\n",
					"\n",
					" What Happens Next:\n",
					"   1. Parser picks up this event (5-10 minutes)\n",
					"   2. Creates/updates lineage in Purview\n",
					"   3. Only ONE operation node per day!\n",
					"\n",
					"  Wait 5-10 minutes and check Purview lineage for inspector_specialisms\n",
					"\"\"\")\n",
					"\n",
					"print(\"=\"*80)"
				],
				"execution_count": 1
			}
		]
	}
}