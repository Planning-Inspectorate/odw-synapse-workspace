{
	"name": "py_sb_write_manifest_and_watermark",
	"properties": {
		"folder": {
			"name": "service-bus"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d9202604-d279-4522-9535-c3fda45e7939"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import Row\n",
					"from pyspark.sql import functions as F, types as T\n",
					"import json, datetime, uuid\n",
					"\n",
					"# Parameters from pipeline\n",
					"getp = mssparkutils.notebook.getNotebookParameter\n",
					"\n",
					"entity_name     = getp(\"entity_name\", \"\").strip()\n",
					"run_context_raw = getp(\"runContextJson\", \"{}\")\n",
					"records_ingested = int(getp(\"recordsIngested\", \"0\"))\n",
					"\n",
					"# Parse runContextJson safely\n",
					"def _j(obj, *keys, default=None):\n",
					"    \"\"\"Nested json get with multiple candidate keys.\"\"\"\n",
					"    for k in keys:\n",
					"        if isinstance(obj, dict) and k in obj and obj[k]:\n",
					"            return obj[k]\n",
					"    return default\n",
					"\n",
					"try:\n",
					"    ctx = json.loads(run_context_raw or \"{}\")\n",
					"except Exception:\n",
					"    ctx = {}\n",
					"\n",
					"# Expect these to be present from pln_vault_and_env_init\n",
					"# Adjust key aliases defensively so the notebook is resilient to naming differences\n",
					"run_id = _j(ctx, \"runId\", \"run_id\", default=str(uuid.uuid4()))\n",
					"raw_root = (_j(ctx, \"rawBasePath\", \"raw_base_path\", \"rawPath\", \"raw_path\", default=\"\") or \"\").rstrip(\"/\")\n",
					"\n",
					"if not entity_name:\n",
					"    raise ValueError(\"Parameter 'entity_name' is required.\")\n",
					"\n",
					"if not raw_root:\n",
					"    raise ValueError(\"runContextJson is missing RAW base path (rawBasePath/rawPath).\")\n",
					"\n",
					"# Compose paths & timestamps\n",
					"utc_now = datetime.datetime.utcnow()\n",
					"ingest_date = utc_now.strftime(\"%Y-%m-%d\")\n",
					"written_at  = utc_now.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
					"\n",
					"# raw/<entity>/ingest_date=YYYY-MM-DD/run_id=<RUN_ID>/\n",
					"entity_root    = f\"{raw_root}/{entity_name}\"\n",
					"manifest_dir   = f\"{entity_root}/ingest_date={ingest_date}/run_id={run_id}\"\n",
					"manifest_path  = f\"{manifest_dir}/manifest.json\"\n",
					"\n",
					"# Ensure the run folder exists\n",
					"mssparkutils.fs.mkdirs(manifest_dir)\n",
					"\n",
					"# Count files written (data files only)\n",
					"def count_data_files(folder: str) -> int:\n",
					"    try:\n",
					"        entries = mssparkutils.fs.ls(folder)\n",
					"    except Exception:\n",
					"        return 0\n",
					"    # Exclude the manifest itself and common non-data markers\n",
					"    data_exts = (\".parquet\", \".json\", \".csv\", \".avro\", \".orc\")\n",
					"    bad_names = {\"manifest.json\", \"_SUCCESS\", \"_committed_\", \"_started_\", \"_delta_log\"}\n",
					"    count = 0\n",
					"    for e in entries:\n",
					"        name = e.name or \"\"\n",
					"        if name in bad_names or name.startswith(\"_delta_log\"):\n",
					"            continue\n",
					"        if name.endswith(data_exts):\n",
					"            count += 1\n",
					"    return count\n",
					"\n",
					"files_written = count_data_files(manifest_dir)\n",
					"\n",
					"# Derive status\n",
					"status = \"Succeeded\"\n",
					"\n",
					"# Write manifest.json\n",
					"manifest = {\n",
					"    \"entity\":            entity_name,\n",
					"    \"source\":            \"service-bus\",\n",
					"    \"runId\":             run_id,\n",
					"    \"ingestDate\":        ingest_date,\n",
					"    \"writtenAt\":         written_at,\n",
					"    \"recordsIngested\":   records_ingested,\n",
					"    \"filesWritten\":      files_written,\n",
					"    \"status\":            status,\n",
					"}\n",
					"\n",
					"mssparkutils.fs.put(manifest_path, json.dumps(manifest, separators=(\",\", \":\")), True)\n",
					"\n",
					"# Watermark table (Delta in the Spark Metastore)\n",
					"#spark.sql(\"CREATE DATABASE IF NOT EXISTS odw_meta_db\")\n",
					"spark.sql(\"USE odw_meta_db\")\n",
					"\n",
					"spark.sql(\"\"\"\n",
					"CREATE TABLE IF NOT EXISTS watermarks (\n",
					"  entity            STRING,\n",
					"  source            STRING,\n",
					"  last_ingest_time  TIMESTAMP,\n",
					"  ingest_date       STRING,\n",
					"  run_id            STRING,\n",
					"  manifest_path     STRING,\n",
					"  records_ingested  BIGINT,\n",
					"  files_written     BIGINT,\n",
					"  status            STRING,\n",
					"  updated_at        TIMESTAMP\n",
					") USING DELTA\n",
					"\"\"\")\n",
					"\n",
					"now_ts = F.current_timestamp()\n",
					"\n",
					"wm_df = spark.createDataFrame([\n",
					"    Row(\n",
					"        entity=entity_name,\n",
					"        source=\"service-bus\",\n",
					"        last_ingest_time=None,\n",
					"        ingest_date=ingest_date,\n",
					"        run_id=run_id,\n",
					"        manifest_path=manifest_path,\n",
					"        records_ingested=records_ingested,\n",
					"        files_written=files_written,\n",
					"        status=status,\n",
					"        updated_at=None\n",
					"    )\n",
					"])\n",
					"\n",
					"wm_df = (wm_df\n",
					"         .withColumn(\"last_ingest_time\", now_ts)\n",
					"         .withColumn(\"updated_at\",       now_ts))\n",
					"\n",
					"wm_df.createOrReplaceTempView(\"wm_upd\")\n",
					"\n",
					"# Merge key: (entity, source, ingest_date) â€” one row per entity/date/source\n",
					"spark.sql(\"\"\"\n",
					"MERGE INTO odw_meta_db.watermarks AS tgt\n",
					"USING wm_upd AS src\n",
					"ON  tgt.entity = src.entity\n",
					"AND tgt.source = src.source\n",
					"AND tgt.ingest_date = src.ingest_date\n",
					"WHEN MATCHED THEN UPDATE SET\n",
					"  tgt.last_ingest_time = src.last_ingest_time,\n",
					"  tgt.run_id           = src.run_id,\n",
					"  tgt.manifest_path    = src.manifest_path,\n",
					"  tgt.records_ingested = src.records_ingested,\n",
					"  tgt.files_written    = src.files_written,\n",
					"  tgt.status           = src.status,\n",
					"  tgt.updated_at       = src.updated_at\n",
					"WHEN NOT MATCHED THEN INSERT *\n",
					"\"\"\")\n",
					"\n",
					"print(f\"manifest: {manifest_path}\")\n",
					"print(f\"watermark upserted for {entity_name} / {ingest_date} (files={files_written}, records={records_ingested})\")\n",
					"\n",
					"# Return to pipeline (available as activity('...').output.runOutput.manifestPath)\n",
					"mssparkutils.notebook.exit(json.dumps({\n",
					"    \"manifestPath\": manifest_path,\n",
					"    \"records\": records_ingested,\n",
					"    \"filesWritten\": files_written,\n",
					"    \"status\": status\n",
					"}))\n",
					""
				],
				"execution_count": null
			}
		]
	}
}