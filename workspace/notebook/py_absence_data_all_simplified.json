{
	"name": "py_absence_data_all_simplified",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e801963a-564e-4aa1-ae13-27711eda7c1c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Harmonised Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Monthly processing and harmonization of absence data.\n",
					"\n",
					"**Modified Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Modified By** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Change Description**  \n",
					"11-December-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Simplified logic:\n",
					"- Use ingested_datetime for incremental filtering\n",
					"- For each RecordUUID, keep only the LATEST record (by LastModifiedDate)\n",
					"- Only keep APPROVED and PENDING records in harmonised\n",
					"- Delete older versions and CANCELLED/REJECTED records from harmonised\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Initializations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from datetime import datetime, timedelta\n",
					"import json\n",
					"import re\n",
					"from pyspark.sql.functions import col\n",
					"from pyspark.sql.types import StringType"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Initialize Result and UDF"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"inserted_count\": 0,\n",
					"    \"updated_count\": 0,\n",
					"    \"deleted_count\": 0,\n",
					"    \"skipped_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"def extract_uuid_from_abstype(abs_type):\n",
					"    \"\"\"\n",
					"    Extract UUID from AbsType field.\n",
					"    The UUID is the unique identifier that remains constant even when status changes.\n",
					"    \n",
					"    \"\"\"\n",
					"    if abs_type is None or (isinstance(abs_type, str) and abs_type.strip() == ''):\n",
					"        import hashlib\n",
					"        return f\"NULL_ABSTYPE_{hashlib.md5('NULL_RECORD'.encode()).hexdigest()}\"\n",
					"    \n",
					"    # Pattern 1: Extract UUID after timestamp pattern\n",
					"    pattern1 = r'^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}[.][0-9]+-(.+)$'\n",
					"    match1 = re.match(pattern1, abs_type)\n",
					"    if match1:\n",
					"        return match1.group(1)\n",
					"    \n",
					"    # Pattern 2: 32-character hex string\n",
					"    pattern2 = r'[a-f0-9]{32}'\n",
					"    match2 = re.search(pattern2, abs_type)\n",
					"    if match2:\n",
					"        return match2.group(0)\n",
					"    \n",
					"    # Pattern 3: Suffix after last dash (4+ characters)\n",
					"    pattern3 = r'.*-([A-Za-z0-9_]{4,})$'\n",
					"    match3 = re.match(pattern3, abs_type)\n",
					"    if match3:\n",
					"        return match3.group(1)\n",
					"    \n",
					"    # Default: return MD5 hash\n",
					"    import hashlib\n",
					"    return hashlib.md5(abs_type.encode()).hexdigest()\n",
					"\n",
					"# Register UDF\n",
					"spark.udf.register(\"extract_uuid\", extract_uuid_from_abstype, StringType())\n",
					"\n",
					"# Set legacy time parser\n",
					"logInfo(\"Setting legacy time parser policy\")\n",
					"spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"logInfo(\"Initialization complete\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Determine Incremental Watermark"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"logInfo(\"Determining incremental processing watermark\")\n",
					"\n",
					"try:\n",
					"    last_processed_result = spark.sql(\"\"\"\n",
					"        SELECT COALESCE(MAX(CAST(IngestionDate AS DATE)), DATE('1900-01-01')) as last_processed_date\n",
					"        FROM odw_harmonised_db.sap_hr_absence_all\n",
					"        WHERE IsActive = 'Y'\n",
					"    \"\"\").collect()\n",
					"    \n",
					"    last_processed_date = last_processed_result[0]['last_processed_date']\n",
					"    logInfo(f\"Last processed date: {last_processed_date}\")\n",
					"    \n",
					"    if last_processed_date.year > 1900:\n",
					"        safe_start_date = last_processed_date - timedelta(days=7)\n",
					"    else:\n",
					"        safe_start_date = datetime(1900, 1, 1).date()\n",
					"    \n",
					"    logInfo(f\"Processing records from: {safe_start_date} (safe start date)\")\n",
					"    \n",
					"except Exception as e:\n",
					"    logInfo(f\"Target table empty or error getting watermark: {e}\")\n",
					"    safe_start_date = datetime(1900, 1, 1).date()"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Create Incremental Source View\n",
					"Uses ingested_datetime instead of StartDate/EndDate to capture all newly ingested records"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    logInfo(\"Creating incremental source view\")\n",
					"    \n",
					"    source_total_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_standardised_db.hr_absence_monthly\").collect()[0]['count']\n",
					"    logInfo(f\"Total source records: {source_total_count}\")\n",
					"    \n",
					"    # Use ingested_datetime for incremental filtering - captures backdated absences\n",
					"    spark.sql(f\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW incremental_source AS\n",
					"    SELECT * FROM odw_standardised_db.hr_absence_monthly\n",
					"    WHERE \n",
					"        StaffNumber IS NOT NULL \n",
					"        AND StartDate IS NOT NULL \n",
					"        AND EndDate IS NOT NULL\n",
					"        AND (\n",
					"            CAST(ingested_datetime AS DATE) >= DATE('{safe_start_date}')\n",
					"            OR AbsType IS NULL\n",
					"        )\n",
					"    \"\"\")\n",
					"    \n",
					"    incremental_source_count = spark.sql(\"SELECT COUNT(*) as count FROM incremental_source\").collect()[0]['count']\n",
					"    logInfo(f\"Incremental source records to process: {incremental_source_count}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error creating incremental source view: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    raise e"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Create Staging View with Transformations\n",
					"Extracts RecordUUID and LastModifiedDate for proper deduplication"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count == 0:\n",
					"        logInfo(\"No new records to process.\")\n",
					"        result[\"skipped_count\"] = source_total_count\n",
					"    else:\n",
					"        logInfo(\"Creating staging view\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW staging_absence AS\n",
					"        SELECT  \n",
					"            StaffNumber,\n",
					"            COALESCE(AbsType, '') AS AbsType,\n",
					"            COALESCE(Illness, SicknessGroup, '') AS SicknessGroup,\n",
					"            CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"            CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"            AttendanceorAbsenceType,\n",
					"            ROUND(CAST(REPLACE(Days, ',', '') AS DOUBLE), 2) AS Days,\n",
					"            ROUND(CAST(REPLACE(Hrs, ',', '') AS DOUBLE), 2) AS Hrs,\n",
					"            TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"            TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"            Caldays,\n",
					"            WorkScheduleRule,\n",
					"            ROUND(TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE), 2) AS Wkhrs,  \n",
					"            ROUND(TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE), 2) AS HrsDay,  \n",
					"            TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"            TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"            'saphr' AS SourceSystemID,\n",
					"            CURRENT_DATE() AS IngestionDate,\n",
					"            CURRENT_TIMESTAMP() AS ValidTo,\n",
					"            \n",
					"            -- ApprovalStatus: APPROVED, CANCELLED, PENDING, REJECTED, etc.\n",
					"            CASE \n",
					"                WHEN AbsType IS NULL OR TRIM(AbsType) = '' THEN 'NULL_ABSTYPE'\n",
					"                WHEN AbsType LIKE '%-%' THEN UPPER(TRIM(SPLIT(AbsType, '-')[0]))\n",
					"                ELSE 'UNKNOWN'\n",
					"            END AS ApprovalStatus,\n",
					"            \n",
					"            -- RecordUUID: Unique ID that stays constant across status changes\n",
					"            extract_uuid(AbsType) AS RecordUUID,\n",
					"            \n",
					"            -- LastModifiedDate: Extracted from AbsType timestamp\n",
					"            COALESCE(\n",
					"                TRY_CAST(\n",
					"                    REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                    AS TIMESTAMP\n",
					"                ),\n",
					"                CURRENT_TIMESTAMP()\n",
					"            ) AS LastModifiedDate,\n",
					"            \n",
					"            -- RowID: Hash for detecting exact record changes\n",
					"            MD5(CONCAT_WS('|',\n",
					"                StaffNumber,            \n",
					"                COALESCE(AbsType, 'NULL_ABSTYPE'),                \n",
					"                COALESCE(SicknessGroup, ''),          \n",
					"                TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"                TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"                AttendanceorAbsenceType,\n",
					"                REPLACE(Days, ',', ''),                   \n",
					"                REPLACE(Hrs, ',', ''),                    \n",
					"                Caldays,                \n",
					"                WorkScheduleRule,       \n",
					"                REPLACE(Wkhrs, ',', ''),                  \n",
					"                REPLACE(HrsDay, ',', ''),                 \n",
					"                REPLACE(WkDys, ',', '')\n",
					"            )) AS RowID,\n",
					"            'Y' AS IsActive\n",
					"            \n",
					"        FROM incremental_source\n",
					"        \"\"\")\n",
					"        \n",
					"        logInfo(\"Staging view created\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error creating staging view: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    raise e"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Get Latest Record per RecordUUID\n",
					"For each UUID, keep only the record with the latest LastModifiedDate.\n",
					"This handles both scenarios:\n",
					"1. APPROVED -> CANCELLED: The CANCELLED record has later timestamp, so it wins\n",
					"2. Sickness Extended: The latest extension has later timestamp, so it wins"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0:\n",
					"        logInfo(\"Getting latest record per RecordUUID (handles extensions and status changes)\")\n",
					"        \n",
					"        # For each RecordUUID, keep only the latest record by LastModifiedDate\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW latest_per_uuid AS\n",
					"        SELECT *\n",
					"        FROM (\n",
					"            SELECT *,\n",
					"                ROW_NUMBER() OVER (\n",
					"                    PARTITION BY RecordUUID \n",
					"                    ORDER BY LastModifiedDate DESC\n",
					"                ) AS rn\n",
					"            FROM staging_absence\n",
					"            WHERE RecordUUID IS NOT NULL\n",
					"        ) ranked\n",
					"        WHERE rn = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        latest_count = spark.sql(\"SELECT COUNT(*) as count FROM latest_per_uuid\").collect()[0]['count']\n",
					"        logInfo(f\"Latest records per UUID: {latest_count}\")\n",
					"        \n",
					"        # Show status breakdown\n",
					"        status_breakdown = spark.sql(\"\"\"\n",
					"            SELECT ApprovalStatus, COUNT(*) as cnt \n",
					"            FROM latest_per_uuid \n",
					"            GROUP BY ApprovalStatus \n",
					"            ORDER BY cnt DESC\n",
					"        \"\"\").collect()\n",
					"        for row in status_breakdown:\n",
					"            logInfo(f\"  - {row['ApprovalStatus']}: {row['cnt']}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error getting latest records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    raise e"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"## DELETE Existing Records for UUIDs That Are Now CANCELLED/REJECTED\n",
					"If the latest status for a UUID is CANCELLED/REJECTED, delete any existing record with that UUID from harmonised"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0:\n",
					"        logInfo(\"=== Deleting existing records where latest status is CANCELLED/REJECTED ===\")\n",
					"        \n",
					"        # Find UUIDs that are now CANCELLED/REJECTED\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW cancelled_uuids AS\n",
					"        SELECT RecordUUID\n",
					"        FROM latest_per_uuid\n",
					"        WHERE UPPER(ApprovalStatus) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"        \"\"\")\n",
					"        \n",
					"        cancelled_count = spark.sql(\"SELECT COUNT(*) as count FROM cancelled_uuids\").collect()[0]['count']\n",
					"        logInfo(f\"UUIDs with CANCELLED/REJECTED status: {cancelled_count}\")\n",
					"        \n",
					"        if cancelled_count > 0:\n",
					"            # Delete existing harmonised records for these UUIDs\n",
					"            delete_result = spark.sql(\"\"\"\n",
					"            DELETE FROM odw_harmonised_db.sap_hr_absence_all\n",
					"            WHERE REVERSE(SPLIT(REVERSE(AbsType), '-')[0]) IN (\n",
					"                SELECT RecordUUID FROM cancelled_uuids\n",
					"            )\n",
					"            \"\"\")\n",
					"            \n",
					"            logInfo(f\"Deleted existing records for {cancelled_count} CANCELLED/REJECTED UUIDs\")\n",
					"            result[\"deleted_count\"] = cancelled_count\n",
					"        \n",
					"        spark.sql(\"DROP VIEW IF EXISTS cancelled_uuids\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error deleting cancelled records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    spark.sql(\"DROP VIEW IF EXISTS cancelled_uuids\")\n",
					"    # Continue processing even if this fails\n",
					"    logInfo(\"Continuing despite error...\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"## DELETE Older Versions for UUIDs with Updates (Sickness Extensions)\n",
					"If a UUID exists in both source and harmonised, delete the harmonised version (will be replaced with latest)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0:\n",
					"        logInfo(\"=== Deleting older versions for UUIDs being updated (extensions) ===\")\n",
					"        \n",
					"        # Find UUIDs in staging that are APPROVED/PENDING (will be inserted/updated)\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW approved_uuids AS\n",
					"        SELECT RecordUUID, LastModifiedDate\n",
					"        FROM latest_per_uuid\n",
					"        WHERE UPPER(ApprovalStatus) IN ('APPROVED', 'PENDING', 'PENDING_APPROVAL')\n",
					"        \"\"\")\n",
					"        \n",
					"        # Delete existing harmonised records that have older versions of these UUIDs\n",
					"        spark.sql(\"\"\"\n",
					"        DELETE FROM odw_harmonised_db.sap_hr_absence_all AS target\n",
					"        WHERE EXISTS (\n",
					"            SELECT 1 FROM approved_uuids src\n",
					"            WHERE REVERSE(SPLIT(REVERSE(target.AbsType), '-')[0]) = src.RecordUUID\n",
					"            AND (\n",
					"                -- Delete if source has newer LastModifiedDate\n",
					"                COALESCE(\n",
					"                    TRY_CAST(\n",
					"                        REGEXP_EXTRACT(target.AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                        AS TIMESTAMP\n",
					"                    ),\n",
					"                    TIMESTAMP('1900-01-01')\n",
					"                ) < src.LastModifiedDate\n",
					"            )\n",
					"        )\n",
					"        \"\"\")\n",
					"        \n",
					"        logInfo(\"Deleted older versions of records being updated\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS approved_uuids\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error deleting older versions: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    spark.sql(\"DROP VIEW IF EXISTS approved_uuids\")\n",
					"    logInfo(\"Continuing despite error...\")"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Filter to Keep Only APPROVED and PENDING Records"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0:\n",
					"        logInfo(\"Filtering to keep only APPROVED and PENDING records\")\n",
					"        \n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW final_staging_records AS\n",
					"        SELECT * FROM latest_per_uuid\n",
					"        WHERE UPPER(ApprovalStatus) IN ('APPROVED', 'PENDING', 'PENDING_APPROVAL')\n",
					"        \"\"\")\n",
					"        \n",
					"        final_count = spark.sql(\"SELECT COUNT(*) as count FROM final_staging_records\").collect()[0]['count']\n",
					"        filtered_out = latest_count - final_count\n",
					"        \n",
					"        logInfo(f\"Final records to process (APPROVED/PENDING only): {final_count}\")\n",
					"        logInfo(f\"Records filtered out (CANCELLED/REJECTED/OTHER): {filtered_out}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error filtering records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    raise e"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Identify Records to Insert"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0:\n",
					"        logInfo(\"Identifying records to insert\")\n",
					"        \n",
					"        # Records to insert - new RowIDs that don't exist in harmonised\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_insert AS\n",
					"        SELECT s.StaffNumber, s.AbsType, s.SicknessGroup, s.StartDate, s.EndDate,\n",
					"               s.AttendanceorAbsenceType, s.Days, s.Hrs, s.Start, s.Endtime,\n",
					"               s.Caldays, s.WorkScheduleRule, s.Wkhrs, s.HrsDay, s.WkDys,\n",
					"               s.AnnualLeaveStart, s.SourceSystemID, s.IngestionDate, s.ValidTo,\n",
					"               s.RowID, s.IsActive\n",
					"        FROM final_staging_records s\n",
					"        LEFT JOIN odw_harmonised_db.sap_hr_absence_all t ON s.RowID = t.RowID\n",
					"        WHERE t.RowID IS NULL\n",
					"        \"\"\")\n",
					"        \n",
					"        insert_count = spark.sql(\"SELECT COUNT(*) as count FROM records_to_insert\").collect()[0]['count']\n",
					"        logInfo(f\"Records to insert: {insert_count}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error identifying records to insert: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    raise e"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Perform Inserts"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    if incremental_source_count > 0 and insert_count > 0:\n",
					"        logInfo(f\"Inserting {insert_count} records\")\n",
					"        \n",
					"        spark.sql(\"\"\"\n",
					"        INSERT INTO odw_harmonised_db.sap_hr_absence_all (\n",
					"            StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"            AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"            Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"            AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"            RowID, IsActive\n",
					"        )\n",
					"        SELECT \n",
					"            StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"            AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"            Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"            AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"            RowID, IsActive\n",
					"        FROM records_to_insert\n",
					"        \"\"\")\n",
					"        \n",
					"        logInfo(f\"Successfully inserted {insert_count} records\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error inserting records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    raise e"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Final Counts and Cleanup"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    # Final counts\n",
					"    final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all WHERE IsActive = 'Y'\").collect()[0]['count']\n",
					"    \n",
					"    result[\"record_count\"] = final_record_count\n",
					"    result[\"inserted_count\"] = insert_count if incremental_source_count > 0 else 0\n",
					"    \n",
					"    logInfo(\"=\" * 60)\n",
					"    logInfo(\"Processing completed successfully:\")\n",
					"    logInfo(f\"- Total source records: {source_total_count}\")\n",
					"    logInfo(f\"- Incremental records processed: {incremental_source_count}\")\n",
					"    logInfo(f\"- Records inserted: {result['inserted_count']}\")\n",
					"    logInfo(f\"- Records deleted (CANCELLED/older versions): {result.get('deleted_count', 0)}\")\n",
					"    logInfo(f\"- Total active records in harmonised: {final_record_count}\")\n",
					"    logInfo(\"=\" * 60)\n",
					"    \n",
					"    # Cleanup temporary views\n",
					"    for view in ['incremental_source', 'staging_absence', 'latest_per_uuid', \n",
					"                 'cancelled_uuids', 'approved_uuids', 'final_staging_records', 'records_to_insert']:\n",
					"        spark.sql(f\"DROP VIEW IF EXISTS {view}\")\n",
					"    \n",
					"    logInfo(\"Cleanup complete\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in final processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = error_msg[:300]\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 13
			}
		]
	}
}