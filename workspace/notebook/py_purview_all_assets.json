{
	"name": "py_purview_all_assets",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3b7c4b96-fb73-466d-b50c-a339041bca3c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# CONF\n",
					"\n",
					"TENANT_ID       = \"5878df98-6f88-48ab-9322-998ce557088d\"\n",
					"CLIENT_ID       = \"65b82a9c-11cd-411f-b5d1-c4385ef4a085\"\n",
					"CLIENT_SECRET   = \"8FT8Q~xlvm-JQVrzWRbAzmXQ6BBnHw42inI4bdnK\"\n",
					"PURVIEW_NAME    = \"pins-pview\"\n",
					"API_VERSION     = \"2023-09-01\"\n",
					"\n",
					"# Paging controls \n",
					"PAGE_SIZE     = 200     \n",
					"MAX_PAGES     = 500   \n",
					"\n",
					"# HTTP/retry\n",
					"TIMEOUT_SEC   = 60\n",
					"MAX_RETRIES   = 3\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"# AUTH\n",
					"\n",
					"import requests, json, time\n",
					"from urllib.parse import quote\n",
					"\n",
					"def get_token_sp(tenant_id, client_id, client_secret):\n",
					"   token_url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n",
					"   data = {\n",
					"       \"grant_type\": \"client_credentials\",\n",
					"       \"client_id\": client_id,\n",
					"       \"client_secret\": client_secret,\n",
					"       \"scope\": \"https://purview.azure.net/.default\"\n",
					"   }\n",
					"   r = requests.post(token_url, data=data, timeout=TIMEOUT_SEC)\n",
					"   if not r.ok:\n",
					"       raise Exception(f\"Token request failed [{r.status_code}]: {r.text[:800]}\")\n",
					"   return r.json()[\"access_token\"]\n",
					"\n",
					"ACCESS_TOKEN = get_token_sp(TENANT_ID, CLIENT_ID, CLIENT_SECRET)\n",
					"HDRS = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
					"\n",
					"# Try both bases for compatibility across classic/unified\n",
					"ATLAS_BASES = [\n",
					"   f\"https://{PURVIEW_NAME}.catalog.purview.azure.com/api/atlas/v2\",\n",
					"   f\"https://{PURVIEW_NAME}.datamap.purview.azure.com/atlas\"\n",
					"]\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"# HTTP Helpers\n",
					"\n",
					"def _get_any(path_with_query: str):\n",
					"   \"\"\"\n",
					"   Try GET on both atlas bases with light retry. Return first success JSON.\n",
					"   \"\"\"\n",
					"   last = None\n",
					"   for base in ATLAS_BASES:\n",
					"       url = f\"{base}{path_with_query}\"\n",
					"       for attempt in range(1, MAX_RETRIES + 1):\n",
					"           r = requests.get(url, headers=HDRS, timeout=TIMEOUT_SEC)\n",
					"           if r.ok:\n",
					"               return r.json()\n",
					"           # Backoff on 429 / 5xx\n",
					"           if r.status_code in (429, 500, 502, 503, 504) and attempt < MAX_RETRIES:\n",
					"               time.sleep(1.5 * attempt)\n",
					"               continue\n",
					"           last = (r.status_code, r.text[:800], url)\n",
					"           break\n",
					"   raise Exception(f\"GET failed. Last error: HTTP {last[0]} {last[2]} â€“ {last[1]}\")\n",
					"\n",
					"def get_bulk_entities(guids):\n",
					"   \"\"\"\n",
					"   Enrich a list of GUIDs via /entity/bulk (ignoreRelationships=true for speed).\n",
					"   \"\"\"\n",
					"   if not guids:\n",
					"       return []\n",
					"   joined = \",\".join(guids)\n",
					"   path = f\"/entity/bulk?guid={quote(joined)}&minExtInfo=false&ignoreRelationships=true&api-version={API_VERSION}\"\n",
					"   data = _get_any(path)\n",
					"   return data.get(\"entities\", []) or []\n",
					""
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"# Pull DataAssets\n",
					"\n",
					"def dsl_search_page(offset: int, limit: int):\n",
					"   # Ask DSL for name, qualifiedName, and __guid\n",
					"   q = f\"from DataSet select name, qualifiedName, __guid limit {limit} offset {offset}\"\n",
					"   path = f\"/search/dsl?query={quote(q, safe='')}&api-version={API_VERSION}\"\n",
					"   data = _get_any(path)\n",
					"   return data.get(\"entities\", []) or [], data.get(\"approximateCount\")\n",
					"\n",
					"all_hits = []\n",
					"for p in range(MAX_PAGES):\n",
					"   offset = p * PAGE_SIZE\n",
					"   page, approx = dsl_search_page(offset, PAGE_SIZE)\n",
					"   if not page:\n",
					"       break\n",
					"   all_hits.extend(page)\n",
					"   if len(page) < PAGE_SIZE:\n",
					"       break\n",
					"\n",
					"print(f\"Collected {len(all_hits)} DataSet hits.\")\n",
					""
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"# Build Spark DF\n",
					"\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
					"\n",
					"def _stringify(x):\n",
					"   try:\n",
					"       if x is None: return None\n",
					"       if isinstance(x, (str, int, float, bool)): return str(x)\n",
					"       return json.dumps(x, ensure_ascii=False)\n",
					"   except Exception:\n",
					"       return str(x)\n",
					"\n",
					"def _extract_guid(hit):\n",
					"   # DSL usually returns 'guid', but we also try attributes.__guid as fallback\n",
					"   gid = hit.get(\"guid\") or hit.get(\"id\")\n",
					"   if not gid and isinstance(hit.get(\"attributes\"), dict):\n",
					"       gid = hit[\"attributes\"].get(\"__guid\")\n",
					"   return gid\n",
					"\n",
					"rows = []\n",
					"chunk = 50\n",
					"for i in range(0, len(all_hits), chunk):\n",
					"   guids = [g for g in (_extract_guid(h) for h in all_hits[i:i+chunk]) if g]\n",
					"   enriched = get_bulk_entities(guids)\n",
					"\n",
					"   for e in enriched:\n",
					"       attrs = e.get(\"attributes\", {}) or {}\n",
					"       rattrs = e.get(\"relationshipAttributes\", {}) or {}\n",
					"\n",
					"       # Best-effort collection/anchor name\n",
					"       collection_name = None\n",
					"       for key in (\"collection\", \"anchor\"):\n",
					"           rel = rattrs.get(key)\n",
					"           if isinstance(rel, dict) and rel.get(\"displayText\"):\n",
					"               collection_name = rel[\"displayText\"]\n",
					"               if collection_name: break\n",
					"\n",
					"       # Classifications\n",
					"       cls_names = e.get(\"classificationNames\") or []\n",
					"       if not cls_names and e.get(\"classifications\"):\n",
					"           try:\n",
					"               cls_names = [c.get(\"typeName\") for c in e[\"classifications\"] if c.get(\"typeName\")]\n",
					"           except Exception:\n",
					"               pass\n",
					"\n",
					"       rows.append(Row(\n",
					"           guid                = e.get(\"guid\"),\n",
					"           typeName            = e.get(\"typeName\"),\n",
					"           name                = attrs.get(\"name\") or e.get(\"displayText\"),\n",
					"           qualifiedName       = attrs.get(\"qualifiedName\"),\n",
					"           collection          = collection_name,\n",
					"           owner               = attrs.get(\"owner\"),\n",
					"           description         = attrs.get(\"description\"),\n",
					"           classificationNames = \",\".join(cls_names) if cls_names else None,\n",
					"           createdBy           = e.get(\"createdBy\"),\n",
					"           updatedBy           = e.get(\"updatedBy\"),\n",
					"           createTime          = e.get(\"createTime\"),  # epoch millis\n",
					"           updateTime          = e.get(\"updateTime\"),  # epoch millis\n",
					"           status              = e.get(\"status\"),\n",
					"           connectorName       = attrs.get(\"connectorName\"),\n",
					"           dataSource          = attrs.get(\"dataSource\") or attrs.get(\"source\"),\n",
					"           rawAttributes       = _stringify(attrs)     # JSON snapshot for additional details\n",
					"       ))\n",
					"\n",
					"schema = StructType([\n",
					"   StructField(\"guid\",               StringType(), True),\n",
					"   StructField(\"typeName\",           StringType(), True),\n",
					"   StructField(\"name\",               StringType(), True),\n",
					"   StructField(\"qualifiedName\",      StringType(), True),\n",
					"   StructField(\"collection\",         StringType(), True),\n",
					"   StructField(\"owner\",              StringType(), True),\n",
					"   StructField(\"description\",        StringType(), True),\n",
					"   StructField(\"classificationNames\", StringType(), True),\n",
					"   StructField(\"createdBy\",          StringType(), True),\n",
					"   StructField(\"updatedBy\",          StringType(), True),\n",
					"   StructField(\"createTime\",         LongType(),  True),\n",
					"   StructField(\"updateTime\",         LongType(),  True),\n",
					"   StructField(\"status\",             StringType(), True),\n",
					"   StructField(\"connectorName\",      StringType(), True),\n",
					"   StructField(\"dataSource\",         StringType(), True),\n",
					"   StructField(\"rawAttributes\",      StringType(), True),\n",
					"])\n",
					"\n",
					"assets_df = spark.createDataFrame(rows, schema) if rows else spark.createDataFrame([], schema)\n",
					"assets_df.createOrReplaceTempView(\"purview_assets\")\n",
					"\n",
					"display(\n",
					"   assets_df\n",
					"     .select(\"guid\",\"typeName\",\"name\",\"qualifiedName\",\"collection\",\"classificationNames\",\"owner\",\"createdBy\",\"createTime\")\n",
					"     .orderBy(\"typeName\",\"name\")\n",
					")\n",
					""
				],
				"execution_count": null
			}
		]
	}
}