{
	"name": "py_Inspector_Specialisms",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "028c201b-e056-4de2-b735-fca57f83e60f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-Feb-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This template is designed to facilitate the monthly processing and harmonization of Inspector Specialism. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The template ensures that Inspector Specialism data is accurately transformed, stored, and made available for reporting and analysis.\n",
					"\n",
					"Rohit Shukla    &nbsp;&nbsp;&nbsp;04-Nov-2025&nbsp;&nbsp;&nbsp;     ValidFrom and ValidTo columns sourced from odw_standardised_db.inspector_specialisms_monthly\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.column import Column\n",
					"from datetime import datetime, timedelta\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialise variable and storage account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": \"\"\n",
					"}\n",
					"\n",
					"actual_inserts = 0\n",
					"actual_updates = 0\n",
					"error_message = \"\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Prepare a data frame to drop exact duplicates from the standardised layer"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Create a unique records data frame and a temporary view\n",
					"\n",
					"try:\n",
					"    #Filter conditions added based on the source data quality issues\n",
					"    df_src_std_specialism = spark.sql(f\"SELECT * FROM odw_standardised_db.inspector_specialisms_monthly\")\n",
					"\n",
					"    df_src_std_specialism = df_src_std_specialism.dropDuplicates()\n",
					"\n",
					"    df_src_std_specialism.createOrReplaceTempView(\"vw_inspector_specialisms_monthly\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error creating temporary view vw_inspector_specialisms_monthly: {str(e)}\")\n",
					"    error_message = f\"Error creating temporary view vw_inspector_specialisms_monthly: {str(e)[:800]}\"\n",
					"    logException(e)\n",
					"\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Prepare Transform Table for the Harmonised layer Ingestion"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"\n",
					"try:\n",
					"\n",
					"    logInfo(\"Starting inspector specialisms data processing\")\n",
					"\n",
					"    create_view_latest_specialisms_sql = \"\"\"\n",
					"        CREATE OR REPLACE TEMP VIEW vw_latest_inspector_specialisms AS\n",
					"        SELECT ROW_NUMBER() OVER (PARTITION BY a.staffnumber,a.QualificationName ORDER BY a.validFrom Desc) Rnk,  \n",
					"            a.StaffNumber,\n",
					"            a.Firstname,\n",
					"            a.Lastname,\n",
					"            a.QualificationName,\n",
					"            a.Proficien,\n",
					"            a.ValidFrom,\n",
					"            a.ValidTo\n",
					"        FROM vw_inspector_specialisms_monthly a\n",
					"        \"\"\"\n",
					"    spark.sql(create_view_latest_specialisms_sql)\n",
					"\n",
					"    # Step 1: Delete all records from the transform table\n",
					"    logInfo(\"Step 1: Deleting records from transform_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    DELETE FROM odw_harmonised_db.transform_inspector_Specialisms\n",
					"    \"\"\")\n",
					"    logInfo(\"Records deleted from transform table\")\n",
					"\n",
					"    # Step 2: Insert new records into the transform table\n",
					"    logInfo(\"Step 2: Inserting records into transform_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.transform_inspector_Specialisms (\n",
					"        StaffNumber,\n",
					"        Firstname,\n",
					"        Lastname,\n",
					"        QualificationName,\n",
					"        Proficien,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidFrom,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    )\n",
					"    SELECT \n",
					"        -- Format StaffNumber based on length and prefix\n",
					"        CASE \n",
					"            WHEN LENGTH(StaffNumber) = 6 THEN\n",
					"                CASE \n",
					"                    WHEN StaffNumber LIKE '50%' THEN CONCAT('00', StaffNumber)\n",
					"                    WHEN StaffNumber LIKE '42%' THEN CONCAT('50', StaffNumber)\n",
					"                    ELSE StaffNumber\n",
					"                END\n",
					"            WHEN LENGTH(StaffNumber) = 8 THEN StaffNumber\n",
					"            ELSE StaffNumber\n",
					"        END AS StaffNumber,\n",
					"        Firstname,\n",
					"        Lastname,\n",
					"        QualificationName,\n",
					"        Proficien,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        CURRENT_TIMESTAMP() AS IngestionDate,\n",
					"        cast(to_date(ValidFrom, \"d/M/yyyy\") as timestamp) AS ValidFrom,\n",
					"        CASE \n",
					"        WHEN Rnk = 1 \n",
					"        THEN           \n",
					"            cast(to_date(ValidTo, \"d/M/yyyy\") as timestamp) \n",
					"        ELSE\n",
					"            cast(DATE_TRUNC('day', CURRENT_DATE() - INTERVAL 1 DAY) as timestamp)\n",
					"        END AS ValidTo,\n",
					"        -- Generate RowID during insert instead of separate update\n",
					"        md5(concat_ws('|', \n",
					"            coalesce(cast(\n",
					"                CASE \n",
					"                    WHEN LENGTH(StaffNumber) = 6 THEN\n",
					"                        CASE \n",
					"                            WHEN StaffNumber LIKE '50%' THEN CONCAT('00', StaffNumber)\n",
					"                            WHEN StaffNumber LIKE '42%' THEN CONCAT('50', StaffNumber)\n",
					"                            ELSE StaffNumber\n",
					"                        END\n",
					"                    WHEN LENGTH(StaffNumber) = 8 THEN StaffNumber\n",
					"                    ELSE StaffNumber\n",
					"                END as string), ''), \n",
					"            coalesce(cast(Firstname as string), ''), \n",
					"            coalesce(cast(Lastname as string), ''), \n",
					"            coalesce(cast(QualificationName as string), ''), \n",
					"            coalesce(cast(Proficien as string), '')\n",
					"        )) AS RowID,\n",
					"        CASE\n",
					"        WHEN Rnk = 1 \n",
					"        THEN           \n",
					"            'Y'\n",
					"        ELSE\n",
					"            'N'\n",
					"        END AS IsActive \n",
					"    FROM vw_latest_inspector_specialisms t1\n",
					"    -- Filter conditions added based on the source data quality issues\n",
					"    WHERE StaffNumber IS NOT NULL \n",
					"    AND QualificationName IS NOT NULL\n",
					"    \"\"\")\n",
					"\n",
					"    # Get count of inserted records\n",
					"    transformed_record_count = spark.sql(\"SELECT COUNT(*) as record_count FROM odw_harmonised_db.transform_inspector_Specialisms\").collect()[0]['record_count']\n",
					"    logInfo(f\"Inserted {transformed_record_count} records into transform_inspector_Specialisms\")\n",
					"\n",
					"    logInfo(\"Inspector specialisms data processing completed successfully\")\n",
					"\n",
					"    end_exec_time = datetime.now()\n",
					"\n",
					"    # Ensure logs are flushed\n",
					"    #flushLogging()\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting into odw_harmonised_db.transform_inspector_Specialisms: {str(e)}\")\n",
					"    error_message = f\"Error inserting into odw_harmonised_db.transform_inspector_Specialisms: {str(e)[:800]}\"\n",
					"    logException(e)\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Create Temporary View with changed and new records"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"\n",
					"try:\n",
					"    # Create Temporary View with SELECT\n",
					"\n",
					"    create_view_specialisms_sql = \"\"\"\n",
					"        CREATE OR REPLACE TEMP VIEW vw_merged_inspector_specialisms AS\n",
					"        SELECT \n",
					"            souSpe.StaffNumber,\n",
					"            souSpe.Firstname,\n",
					"            souSpe.Lastname,\n",
					"            souSpe.QualificationName,\n",
					"            souSpe.Proficien,\n",
					"            souSpe.ValidFrom,\n",
					"            souSpe.ValidTo,\n",
					"            souSpe.RowID,\n",
					"            'saphr' AS SourceSystemID,\n",
					"            current_timestamp() AS IngestionDate,\n",
					"            1 AS Current,\n",
					"            'Y' AS IsActive,\n",
					"            -- Target columns for matching\n",
					"            tarSpe.StaffNumber AS Target_StaffNumber,\n",
					"            tarSpe.ValidFrom AS Target_ValidFrom,\n",
					"            tarSpe.IsActive AS Target_IsActive,\n",
					"            tarSpe.Firstname AS Target_Firstname,\n",
					"            tarSpe.Lastname AS Target_Lastname,\n",
					"            tarSpe.Proficien AS Target_Proficien,\n",
					"            -- Change detection\n",
					"            CASE \n",
					"                WHEN tarSpe.StaffNumber IS NULL THEN 'New_Record'\n",
					"                WHEN tarSpe.ValidFrom != souSpe.ValidFrom THEN 'New_Version'\n",
					"                WHEN tarSpe.Firstname != souSpe.Firstname \n",
					"                    OR tarSpe.Lastname != souSpe.Lastname \n",
					"                    OR tarSpe.Proficien != souSpe.Proficien THEN 'Record_Changed'\n",
					"                ELSE 'No_Change'\n",
					"            END AS ChangeType\n",
					"        FROM odw_harmonised_db.transform_inspector_Specialisms AS souSpe\n",
					"        LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms AS tarSpe\n",
					"            ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"            AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"            AND tarSpe.IsActive = 'Y' \n",
					"        WHERE souSpe.IsActive = 'Y'\n",
					"        \"\"\"\n",
					"\n",
					"    spark.sql(create_view_specialisms_sql)\n",
					"    print(\" Temporary view created\")\n",
					"\n",
					"    # Get individual counts\n",
					"    counts = spark.sql(\"\"\"\n",
					"        SELECT \n",
					"            SUM(CASE WHEN ChangeType = 'New_Record' THEN 1 ELSE 0 END) as New_Records,\n",
					"            SUM(CASE WHEN ChangeType = 'New_Version' THEN 1 ELSE 0 END) as New_Versions,\n",
					"            SUM(CASE WHEN ChangeType = 'Record_Changed' THEN 1 ELSE 0 END) as Record_Changed\n",
					"        FROM vw_merged_inspector_specialisms\n",
					"    \"\"\").collect()[0]\n",
					"\n",
					"    expected_updates = counts['New_Versions'] + counts['Record_Changed']\n",
					"    expected_inserts = counts['New_Records'] + counts['New_Versions'] + counts['Record_Changed']\n",
					"\n",
					"    print(f\"expected_updates : {expected_updates}\")\n",
					"    print(f\"expected_inserts : {expected_inserts}\")\n",
					"\n",
					"    #test_df = spark.sql(\"\"\" SELECT \n",
					"    #        ChangeType, count(1) as Cnt\n",
					"    #    FROM vw_merged_inspector_specialisms\n",
					"    #    WHERE ChangeType IN ('New_Record','New_Version', 'Record_Changed','No_Change')\n",
					"    #    group by ChangeType\n",
					"    #    \"\"\")\n",
					"\n",
					"    #test_df = spark.sql(\"\"\" SELECT *        \n",
					"    #    FROM vw_merged_inspector_specialisms\n",
					"    #    WHERE ChangeType IN ('New_Record','New_Version', 'Record_Changed')\n",
					"    #    \"\"\")\n",
					"    #display(test_df)\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error creating temporary view vw_merged_inspector_specialisms: {str(e)}\")\n",
					"    error_message = f\"Error creating temporary view vw_merged_inspector_specialisms: {str(e)[:800]}\"\n",
					"    logException(e)\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Update and close existing Specialism records"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# MERGE for UPDATE only (close old records)\n",
					"\n",
					"try:\n",
					"\n",
					"    if expected_updates > 0:\n",
					"\n",
					"        # Count before update\n",
					"        before_update = spark.sql(\"\"\"\n",
					"            SELECT COUNT(*) as cnt \n",
					"            FROM odw_harmonised_db.sap_hr_inspector_Specialisms \n",
					"            WHERE IsActive = 'Y'\n",
					"        \"\"\").collect()[0]['cnt']\n",
					"\n",
					"        print(f\"Active records before: {before_update}\")\n",
					"        print(f\"Expected to close: {expected_updates}\")\n",
					"\n",
					"        merge_update_sql = \"\"\"\n",
					"            MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS tgt\n",
					"            USING (\n",
					"                SELECT \n",
					"                    Target_StaffNumber,\n",
					"                    QualificationName,\n",
					"                    ChangeType\n",
					"                FROM vw_merged_inspector_specialisms\n",
					"                WHERE ChangeType IN ('New_Version', 'Record_Changed')\n",
					"                AND Target_StaffNumber IS NOT NULL\n",
					"            ) AS src\n",
					"            ON tgt.StaffNumber = src.Target_StaffNumber\n",
					"            AND tgt.QualificationName = src.QualificationName\n",
					"            AND tgt.IsActive = 'Y'\n",
					"            WHEN MATCHED THEN\n",
					"                UPDATE SET\n",
					"                    tgt.ValidTo = current_date(),\n",
					"                    tgt.Current = 0,\n",
					"                    tgt.IngestionDate = current_timestamp(),\n",
					"                    tgt.LastUpdated = current_timestamp(),\n",
					"                    tgt.IsActive = 'N'\n",
					"            \"\"\"\n",
					"\n",
					"        spark.sql(merge_update_sql)\n",
					"\n",
					"        # Count after update\n",
					"        after_update = spark.sql(\"\"\"\n",
					"            SELECT COUNT(*) as cnt \n",
					"            FROM odw_harmonised_db.sap_hr_inspector_Specialisms \n",
					"            WHERE IsActive = 'Y'\n",
					"        \"\"\").collect()[0]['cnt']\n",
					"\n",
					"        actual_updates = before_update - after_update\n",
					"        print(f\" UPDATE completed\")\n",
					"        print(f\" Records closed: {actual_updates}\")\n",
					"\n",
					"        if actual_updates == expected_updates:\n",
					"            print(f\" Validation passed ({actual_updates} = {expected_updates})\")\n",
					"        else:\n",
					"            print(f\" Validation failed! Expected: {expected_updates}, Actual: {actual_updates}\")\n",
					"    else:\n",
					"        print(f\" Zero records found for updating  odw_harmonised_db.sap_hr_inspector_Specialisms\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error updating odw_harmonised_db.sap_hr_inspector_Specialisms: {str(e)}\")\n",
					"    error_message = f\"Error updating odw_harmonised_db.sap_hr_inspector_Specialisms: {str(e)[:800]}\"\n",
					"    logException(e)\n",
					"    flushLogging()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Insert New Specialism records"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# STEP 4: INSERT - Add new records\n",
					"\n",
					"try:\n",
					"\n",
					"    if expected_inserts > 0:\n",
					"\n",
					"        # Count before insert\n",
					"        before_insert = spark.sql(\"\"\"\n",
					"            SELECT COUNT(*) as cnt \n",
					"            FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        \"\"\").collect()[0]['cnt']\n",
					"\n",
					"        print(f\"Total records before: {before_insert}\")\n",
					"        print(f\"Expected to insert: {expected_inserts}\")\n",
					"\n",
					"        insert_sql = \"\"\"\n",
					"            INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms \n",
					"                (\n",
					"                    StaffNumber, \n",
					"                    Firstname, \n",
					"                    Lastname, \n",
					"                    QualificationName, \n",
					"                    Proficien,\n",
					"                    SourceSystemID, \n",
					"                    IngestionDate, \n",
					"                    ValidFrom, \n",
					"                    ValidTo, \n",
					"                    Current, \n",
					"                    RowID, \n",
					"                    IsActive\n",
					"                )\n",
					"            SELECT  StaffNumber, \n",
					"                    Firstname, \n",
					"                    Lastname, \n",
					"                    QualificationName, \n",
					"                    Proficien,\n",
					"                    SourceSystemID, \n",
					"                    IngestionDate, \n",
					"                    ValidFrom, \n",
					"                    ValidTo, \n",
					"                    Current, \n",
					"                    RowID, \n",
					"                    IsActive\n",
					"            FROM vw_merged_inspector_specialisms\n",
					"            WHERE ChangeType IN ('New_Record', 'New_Version', 'Record_Changed')\n",
					"            \"\"\"\n",
					"\n",
					"        spark.sql(insert_sql)\n",
					"\n",
					"        # Count after insert\n",
					"        after_insert = spark.sql(\"\"\"\n",
					"            SELECT COUNT(*) as cnt \n",
					"            FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        \"\"\").collect()[0]['cnt']\n",
					"\n",
					"        actual_inserts = after_insert - before_insert\n",
					"        print(f\" INSERT completed\")\n",
					"        print(f\" Total records after: {after_insert}\")\n",
					"        print(f\" Records inserted: {actual_inserts}\")\n",
					"\n",
					"        if actual_inserts == expected_inserts:\n",
					"            print(f\" Validation passed ({actual_inserts} = {expected_inserts})\")\n",
					"        else:\n",
					"            print(f\" Validation failed! Expected: {expected_inserts}, Actual: {actual_inserts}\")\n",
					"\n",
					"        #final_counts = spark.sql(\"\"\"\n",
					"        #    SELECT IsActive, COUNT(*) as Count\n",
					"        #    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        #    GROUP BY IsActive\n",
					"        #    ORDER BY IsActive DESC\n",
					"        #\"\"\")\n",
					"        #final_counts.show()\n",
					"\n",
					"    else:\n",
					"        print(f\" Zero records found for inserting into  odw_harmonised_db.sap_hr_inspector_Specialisms\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting into odw_harmonised_db.sap_hr_inspector_Specialisms: {str(e)}\")\n",
					"    error_message = f\"Error inserting into odw_harmonised_db.sap_hr_inspector_Specialisms: {str(e)[:800]}\"\n",
					"    logException(e)\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Update Statements"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting insertion of new inspector specialisms records\")\n",
					"    \n",
					"    # First get count of potential new records\n",
					"    new_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.StaffNumber IS NULL\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {new_records_count} new inspector specialisms to insert\")\n",
					"    \n",
					"    # Insert new records that don't exist in the target table\n",
					"    logInfo(\"Inserting new records into sap_hr_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms (\n",
					"        StaffNumber, \n",
					"        Firstname, \n",
					"        Lastname, \n",
					"        QualificationName, \n",
					"        Proficien, \n",
					"        SourceSystemID, \n",
					"        IngestionDate, \n",
					"        ValidFrom, \n",
					"        ValidTo, \n",
					"        Current, \n",
					"        RowID, \n",
					"        IsActive, \n",
					"        LastUpdated\n",
					"    )\n",
					"    SELECT \n",
					"        souSpe.StaffNumber, \n",
					"        souSpe.Firstname, \n",
					"        souSpe.Lastname, \n",
					"        souSpe.QualificationName, \n",
					"        souSpe.Proficien, \n",
					"        'saphr', \n",
					"        current_timestamp(), \n",
					"        souSpe.ValidFrom, \n",
					"        souSpe.ValidTo, \n",
					"        1, \n",
					"        souSpe.RowID, \n",
					"        'Y', \n",
					"        current_timestamp()\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.StaffNumber IS NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Verify the insertion was successful\n",
					"    actual_inserted = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE IngestionDate = current_date()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Successfully inserted {actual_inserted} new inspector specialisms records\")\n",
					"    \n",
					"    if actual_inserted != new_records_count:\n",
					"        logInfo(f\"Note: Expected to insert {new_records_count} records but actually inserted {actual_inserted}\")\n",
					"    \n",
					"    logInfo(\"Inspector specialisms insertion completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting inspector specialisms records: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting update of obsolete inspector specialisms records\")\n",
					"    \n",
					"    # First, let's check for potential duplicates in the source data\n",
					"    logInfo(\"Checking for duplicate records in source data\")\n",
					"    duplicate_check = spark.sql(\"\"\"\n",
					"    SELECT \n",
					"        StaffNumber, \n",
					"        QualificationName,\n",
					"        COUNT(*) as duplicate_count\n",
					"    FROM (\n",
					"        SELECT \n",
					"            oldSpe.StaffNumber, \n",
					"            oldSpe.QualificationName\n",
					"        FROM \n",
					"            odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"        LEFT OUTER JOIN \n",
					"            odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"        ON \n",
					"            oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"            AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"        WHERE \n",
					"            souSpe.StaffNumber IS NULL\n",
					"            AND oldSpe.Current = 1\n",
					"    ) source_data\n",
					"    GROUP BY StaffNumber, QualificationName\n",
					"    HAVING COUNT(*) > 1\n",
					"    ORDER BY duplicate_count DESC\n",
					"    \"\"\")\n",
					"    \n",
					"    duplicate_count = duplicate_check.count()\n",
					"    if duplicate_count > 0:\n",
					"        logInfo(f\"Found {duplicate_count} duplicate combinations in source data - using DISTINCT to resolve\")\n",
					"        # Show some examples of duplicates for debugging\n",
					"        duplicate_examples = duplicate_check.limit(5).collect()\n",
					"        for row in duplicate_examples:\n",
					"            logInfo(f\"Duplicate: StaffNumber={row['StaffNumber']}, QualificationName={row['QualificationName']}, Count={row['duplicate_count']}\")\n",
					"    else:\n",
					"        logInfo(\"No duplicates found in source data\")\n",
					"    \n",
					"    # Get count of records to be updated (using DISTINCT to match our MERGE logic)\n",
					"    logInfo(\"Counting obsolete inspector specialisms records to update\")\n",
					"    obsolete_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM (\n",
					"        SELECT DISTINCT\n",
					"            oldSpe.StaffNumber, \n",
					"            oldSpe.QualificationName\n",
					"        FROM \n",
					"            odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"        LEFT OUTER JOIN \n",
					"            odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"        ON \n",
					"            oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"            AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"        WHERE \n",
					"            souSpe.StaffNumber IS NULL\n",
					"            AND oldSpe.Current = 1\n",
					"    ) distinct_obsolete\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {obsolete_records_count} obsolete inspector specialisms to update\")\n",
					"    \n",
					"    if obsolete_records_count == 0:\n",
					"        logInfo(\"No obsolete records found to update. Skipping MERGE operation.\")\n",
					"    else:\n",
					"        # Update records that are no longer present in the source data\n",
					"        logInfo(\"Updating obsolete records in sap_hr_inspector_Specialisms using MERGE with DISTINCT source\")\n",
					"        \n",
					"        merge_result = spark.sql(\"\"\"\n",
					"        MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS target\n",
					"        USING (\n",
					"            SELECT DISTINCT\n",
					"                oldSpe.StaffNumber, \n",
					"                oldSpe.QualificationName\n",
					"            FROM \n",
					"                odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"            LEFT OUTER JOIN \n",
					"                odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"            ON \n",
					"                oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"                AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"            WHERE \n",
					"                souSpe.StaffNumber IS NULL\n",
					"                AND oldSpe.Current = 1\n",
					"        ) AS source\n",
					"        ON \n",
					"            target.StaffNumber = source.StaffNumber\n",
					"            AND target.QualificationName = source.QualificationName\n",
					"            AND target.Current = 1\n",
					"        WHEN MATCHED THEN\n",
					"        UPDATE SET \n",
					"            target.Current = 0,\n",
					"            target.ValidTo = CURRENT_DATE()\n",
					"        \"\"\")\n",
					"        \n",
					"        logInfo(\"MERGE operation completed successfully\")\n",
					"        \n",
					"        # Verify the update was successful\n",
					"        logInfo(\"Verifying update results\")\n",
					"        updated_records = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE ValidTo = CURRENT_DATE() AND Current = 0\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        logInfo(f\"Total records now marked as obsolete today: {updated_records}\")\n",
					"        \n",
					"        # Additional verification - check if the expected records were updated\n",
					"        remaining_obsolete = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count\n",
					"        FROM (\n",
					"            SELECT DISTINCT\n",
					"                oldSpe.StaffNumber, \n",
					"                oldSpe.QualificationName\n",
					"            FROM \n",
					"                odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"            LEFT OUTER JOIN \n",
					"                odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"            ON \n",
					"                oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"                AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"            WHERE \n",
					"                souSpe.StaffNumber IS NULL\n",
					"                AND oldSpe.Current = 1\n",
					"        ) still_current_obsolete\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        if remaining_obsolete == 0:\n",
					"            logInfo(\"✓ All expected obsolete records have been successfully updated\")\n",
					"        else:\n",
					"            logError(f\"⚠ Warning: {remaining_obsolete} obsolete records are still marked as Current = 1\")\n",
					"        \n",
					"        # Log summary statistics\n",
					"        total_current_records = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 1\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        total_obsolete_records = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 0\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        logInfo(f\"Final state - Current records: {total_current_records}, Obsolete records: {total_obsolete_records}\")\n",
					"    \n",
					"    logInfo(\"Obsolete inspector specialisms update completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail with context\n",
					"    error_msg = f\"Error updating obsolete inspector specialisms records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logError(f\"Exception type: {type(e).__name__}\")\n",
					"    \n",
					"    # If it's a Spark SQL exception, try to extract more meaningful error info\n",
					"    if hasattr(e, 'java_exception') and e.java_exception is not None:\n",
					"        java_exception = str(e.java_exception)\n",
					"        if \"DeltaUnsupportedOperationException\" in java_exception:\n",
					"            logError(\"This appears to be a Delta Lake MERGE conflict error\")\n",
					"            logError(\"Possible causes: duplicate source rows, concurrent operations, or schema issues\")\n",
					"        elif \"multipleSourceRowMatchingTargetRowInMergeException\" in java_exception:\n",
					"            logError(\"Multiple source rows are matching the same target row - source data needs deduplication\")\n",
					"    \n",
					"    # Log the full stack trace for debugging\n",
					"    logException(e)\n",
					"    \n",
					"    # Provide recovery suggestions\n",
					"\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"    \n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs and cleaning up\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting insertion of changed inspector specialisms records\")\n",
					"    \n",
					"    # First get count of changed records to be inserted\n",
					"    changed_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    INNER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.RowID <> souSpe.RowID\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {changed_records_count} changed inspector specialisms to insert\")\n",
					"    \n",
					"    # Insert new versions of records that have changed (different RowID)\n",
					"    logInfo(\"Inserting changed records into sap_hr_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms (\n",
					"        StaffNumber, \n",
					"        Firstname, \n",
					"        Lastname, \n",
					"        QualificationName, \n",
					"        Proficien, \n",
					"        SourceSystemID, \n",
					"        IngestionDate, \n",
					"        ValidFrom, \n",
					"        ValidTo, \n",
					"        Current, \n",
					"        RowID, \n",
					"        IsActive, \n",
					"        LastUpdated\n",
					"    )\n",
					"    SELECT \n",
					"        souSpe.StaffNumber, \n",
					"        souSpe.Firstname, \n",
					"        souSpe.Lastname, \n",
					"        souSpe.QualificationName, \n",
					"        souSpe.Proficien, \n",
					"        'saphr', \n",
					"        current_timestamp(), \n",
					"        souSpe.ValidFrom, \n",
					"        souSpe.ValidTo, \n",
					"        1, \n",
					"        souSpe.RowID, \n",
					"        'Y', \n",
					"        current_timestamp()\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.RowID <> souSpe.RowID\n",
					"    \"\"\")\n",
					"    \n",
					"    # Verify the insertion was successful\n",
					"    inserted_records = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE IngestionDate = CURRENT_DATE() AND Current = 1\n",
					"    \"\"\").collect()[0]['count'] - spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE ValidFrom = CURRENT_DATE() AND Current = 1\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Successfully inserted {inserted_records} changed inspector specialisms records\")\n",
					"    \n",
					"    if inserted_records != changed_records_count:\n",
					"        logInfo(f\"Note: Expected to insert {changed_records_count} records but actual count differs\")\n",
					"    \n",
					"    logInfo(\"Changed inspector specialisms insertion completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting changed inspector specialisms records: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"try:\n",
					"\n",
					"    total_record_count = actual_inserts + actual_updates    \n",
					"\n",
					"    # Update status\n",
					"    if not error_message:        \n",
					"        status = \"success\"    \n",
					"    else:        \n",
					"        status = \"failed\"    \n",
					"\n",
					"    # Update result\n",
					"    result[\"status\"] = status\n",
					"    result[\"error_message\"] = error_message\n",
					"    result[\"record_count\"] = total_record_count\n",
					"\n",
					"    \n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting into odw_harmonised_db.sap_hr_inspector_Specialisms: {str(e)}\")\n",
					"    error_message = f\"Error inserting into odw_harmonised_db.sap_hr_inspector_Specialisms: {str(e)[:800]}\"\n",
					"    print(error_message)\n",
					"    logException(e)\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Output the result as JSON for ADF to capture\n",
					"mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}