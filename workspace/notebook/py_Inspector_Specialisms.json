{
	"name": "py_Inspector_Specialisms",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "389dbc9e-4044-4c96-b140-c94d9d68d5de"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-Feb-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This template is designed to facilitate the monthly processing and harmonization of Inspector Specialism. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The template ensures that Inspector Specialism data is accurately transformed, stored, and made available for reporting and analysis.\n",
					"\n",
					"Rohit Shukla    &nbsp;&nbsp;&nbsp;04-Nov-2025&nbsp;&nbsp;&nbsp;     ValidFrom and ValidTo columns sourced from odw_standardised_db.inspector_specialisms_monthly\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.column import Column\n",
					"from datetime import datetime, timedelta"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Entity Name : inspector_Specialisms"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"\n",
					"logInfo(\"Starting inspector specialisms data processing\")\n",
					"\n",
					"# Step 1: Delete all records from the transform table\n",
					"logInfo(\"Step 1: Deleting records from transform_inspector_Specialisms\")\n",
					"spark.sql(\"\"\"\n",
					"DELETE FROM odw_harmonised_db.transform_inspector_Specialisms\n",
					"\"\"\")\n",
					"logInfo(\"Records deleted from transform table\")\n",
					"\n",
					"# Step 2: Insert new records into the transform table\n",
					"logInfo(\"Step 2: Inserting records into transform_inspector_Specialisms\")\n",
					"spark.sql(\"\"\"\n",
					"INSERT INTO odw_harmonised_db.transform_inspector_Specialisms (\n",
					"    StaffNumber,\n",
					"    Firstname,\n",
					"    Lastname,\n",
					"    QualificationName,\n",
					"    Proficien,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidFrom,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					")\n",
					"SELECT \n",
					"    -- Format StaffNumber based on length and prefix\n",
					"    CASE \n",
					"        WHEN LENGTH(StaffNumber) = 6 THEN\n",
					"            CASE \n",
					"                WHEN StaffNumber LIKE '50%' THEN CONCAT('00', StaffNumber)\n",
					"                WHEN StaffNumber LIKE '42%' THEN CONCAT('50', StaffNumber)\n",
					"                ELSE StaffNumber\n",
					"            END\n",
					"        WHEN LENGTH(StaffNumber) = 8 THEN StaffNumber\n",
					"        ELSE StaffNumber\n",
					"    END AS StaffNumber,\n",
					"    Firstname,\n",
					"    Lastname,\n",
					"    QualificationName,\n",
					"    Proficien,\n",
					"    'saphr' AS SourceSystemID,\n",
					"    CURRENT_TIMESTAMP() AS IngestionDate,\n",
					"    cast(to_date(ValidFrom, \"d/M/yyyy\") as timestamp) AS ValidFrom,\n",
					"    cast(to_date(ValidTo, \"d/M/yyyy\") as timestamp) AS ValidTo,\n",
					"    -- Generate RowID during insert instead of separate update\n",
					"    md5(concat_ws('|', \n",
					"        coalesce(cast(\n",
					"            CASE \n",
					"                WHEN LENGTH(StaffNumber) = 6 THEN\n",
					"                    CASE \n",
					"                        WHEN StaffNumber LIKE '50%' THEN CONCAT('00', StaffNumber)\n",
					"                        WHEN StaffNumber LIKE '42%' THEN CONCAT('50', StaffNumber)\n",
					"                        ELSE StaffNumber\n",
					"                    END\n",
					"                WHEN LENGTH(StaffNumber) = 8 THEN StaffNumber\n",
					"                ELSE StaffNumber\n",
					"            END as string), ''), \n",
					"        coalesce(cast(Firstname as string), ''), \n",
					"        coalesce(cast(Lastname as string), ''), \n",
					"        coalesce(cast(QualificationName as string), ''), \n",
					"        coalesce(cast(Proficien as string), '')\n",
					"    )) AS RowID,\n",
					"    'Y' AS IsActive\n",
					"FROM \n",
					"    odw_standardised_db.inspector_specialisms_monthly t1\n",
					"WHERE \n",
					"    StaffNumber IS NOT NULL\n",
					"\"\"\")\n",
					"\n",
					"# Get count of inserted records\n",
					"record_count = spark.sql(\"SELECT COUNT(*) as record_count FROM odw_harmonised_db.transform_inspector_Specialisms\").collect()[0]['record_count']\n",
					"logInfo(f\"Inserted {record_count} records into transform_inspector_Specialisms\")\n",
					"\n",
					"logInfo(\"Inspector specialisms data processing completed successfully\")\n",
					"\n",
					"# Ensure logs are flushed\n",
					"flushLogging()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Modified and fixed version"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"spark.sql(\"\"\"\n",
					"    MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS tarSpe\n",
					"    USING (\n",
					"        SELECT \n",
					"            souSpe.StaffNumber,\n",
					"            souSpe.Firstname,\n",
					"            souSpe.Lastname,\n",
					"            souSpe.QualificationName,\n",
					"            souSpe.Proficien,\n",
					"            souSpe.ValidFrom,\n",
					"            souSpe.ValidTo,\n",
					"            souSpe.RowID,\n",
					"            'saphr' AS SourceSystemID,\n",
					"            current_timestamp() AS IngestionDate,\n",
					"            1 AS Current,\n",
					"            'Y' AS IsActive,\n",
					"            -- Join with tarSpe to identify scenarios\n",
					"            tarSpe.StaffNumber AS Target_StaffNumber,\n",
					"            tarSpe.ValidFrom AS Target_ValidFrom,\n",
					"            tarSpe.IsActive AS Target_IsActive,\n",
					"            tarSpe.Firstname AS Target_Firstname,\n",
					"            tarSpe.Lastname AS Target_Lastname,\n",
					"            tarSpe.Proficien AS Target_Proficien,\n",
					"            -- Identify SCD Type-2 scenarios\n",
					"            CASE \n",
					"            WHEN tarSpe.StaffNumber IS NULL THEN 'New_Record'\n",
					"            WHEN tarSpe.ValidFrom != souSpe.ValidFrom THEN 'New_Version'\n",
					"            WHEN tarSpe.Firstname != souSpe.Firstname \n",
					"                OR tarSpe.Lastname != souSpe.Lastname \n",
					"                OR tarSpe.Proficien != souSpe.Proficien THEN 'Data_Changed'\n",
					"            ELSE 'No_Change'\n",
					"            END AS ChangeType\n",
					"        FROM odw_harmonised_db.transform_inspector_Specialisms AS souSpe\n",
					"        LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms AS tarSpe\n",
					"            ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"            AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"            AND tarSpe.IsActive = 'Y'\n",
					"    ) AS merged_records\n",
					"    ON tarSpe.StaffNumber = merged_records.Target_StaffNumber\n",
					"        AND tarSpe.QualificationName = merged_records.QualificationName\n",
					"        AND tarSpe.IsActive = 'Y'\n",
					"        AND merged_records.ChangeType IN ('New_Version', 'Data_Changed')\n",
					"\n",
					"    WHEN MATCHED THEN\n",
					"        UPDATE SET\n",
					"            tarSpe.ValidTo = current_date(),\n",
					"            tarSpe.LastUpdated = current_timestamp(),\n",
					"            tarSpe.Current = 0,\n",
					"            tarSpe.IngestionDate = current_timestamp(),\n",
					"            tarSpe.IsActive = 'N'        \n",
					"\n",
					"    WHEN NOT MATCHED AND merged_records.ChangeType IN ('New_Record', 'New_Version', 'Data_Changed') THEN\n",
					"        INSERT (\n",
					"            StaffNumber, \n",
					"            Firstname, \n",
					"            Lastname, \n",
					"            QualificationName, \n",
					"            Proficien,\n",
					"            SourceSystemID, \n",
					"            IngestionDate, \n",
					"            ValidFrom, \n",
					"            ValidTo, \n",
					"            Current, \n",
					"            RowID, \n",
					"            IsActive\n",
					"        )\n",
					"        VALUES (\n",
					"            merged_records.StaffNumber,\n",
					"            merged_records.Firstname,\n",
					"            merged_records.Lastname,\n",
					"            merged_records.QualificationName,\n",
					"            merged_records.Proficien,\n",
					"            merged_records.SourceSystemID,\n",
					"            merged_records.IngestionDate,\n",
					"            merged_records.ValidFrom,\n",
					"            merged_records.ValidTo,\n",
					"            merged_records.Current,\n",
					"            merged_records.RowID,\n",
					"            merged_records.IsActive\n",
					"        )\n",
					"    \"\"\")\n",
					"\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Create Temporary View with changed and new records"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"\n",
					"# Create Temporary View with SELECT\n",
					"\n",
					"create_view_specialisms_sql = \"\"\"\n",
					"    CREATE OR REPLACE TEMP VIEW vw_merged_inspector_specialisms AS\n",
					"    SELECT \n",
					"        souSpe.StaffNumber,\n",
					"        souSpe.Firstname,\n",
					"        souSpe.Lastname,\n",
					"        souSpe.QualificationName,\n",
					"        souSpe.Proficien,\n",
					"        souSpe.ValidFrom,\n",
					"        souSpe.ValidTo,\n",
					"        souSpe.RowID,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        current_timestamp() AS IngestionDate,\n",
					"        1 AS Current,\n",
					"        'Y' AS IsActive,\n",
					"        -- Target columns for matching\n",
					"        tarSpe.StaffNumber AS Target_StaffNumber,\n",
					"        tarSpe.ValidFrom AS Target_ValidFrom,\n",
					"        tarSpe.IsActive AS Target_IsActive,\n",
					"        tarSpe.Firstname AS Target_Firstname,\n",
					"        tarSpe.Lastname AS Target_Lastname,\n",
					"        tarSpe.Proficien AS Target_Proficien,\n",
					"        -- Change detection\n",
					"        CASE \n",
					"            WHEN tarSpe.StaffNumber IS NULL THEN 'New_Record'\n",
					"            WHEN tarSpe.ValidFrom != souSpe.ValidFrom THEN 'New_Version'\n",
					"            WHEN tarSpe.Firstname != souSpe.Firstname \n",
					"                OR tarSpe.Lastname != souSpe.Lastname \n",
					"                OR tarSpe.Proficien != souSpe.Proficien THEN 'Data_Changed'\n",
					"            ELSE 'No_Change'\n",
					"        END AS ChangeType\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms AS souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms AS tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"        AND tarSpe.IsActive = 'Y'\n",
					"    \"\"\"\n",
					"\n",
					"spark.sql(create_view_specialisms_sql)\n",
					"print(\" Temporary view created\")\n",
					"\n",
					"# Get individual counts\n",
					"counts = spark.sql(\"\"\"\n",
					"    SELECT \n",
					"        SUM(CASE WHEN ChangeType = 'New_Record' THEN 1 ELSE 0 END) as New_Records,\n",
					"        SUM(CASE WHEN ChangeType = 'New_Version' THEN 1 ELSE 0 END) as New_Versions,\n",
					"        SUM(CASE WHEN ChangeType = 'Data_Changed' THEN 1 ELSE 0 END) as Data_Changed\n",
					"    FROM vw_merged_inspector_specialisms\n",
					"\"\"\").collect()[0]\n",
					"\n",
					"expected_updates = counts['New_Versions'] + counts['Data_Changed']\n",
					"expected_inserts = counts['New_Records'] + counts['New_Versions'] + counts['Data_Changed']\n",
					"\n",
					"print(f\"expected_updates : {expected_updates}\")\n",
					"print(f\"expected_inserts : {expected_inserts}\")\n",
					"\n",
					"test_df = spark.sql(\"\"\" SELECT \n",
					"        *\n",
					"    FROM vw_merged_inspector_specialisms\n",
					"    WHERE ChangeType IN ('New_Version', 'Data_Changed')\n",
					"      AND Target_StaffNumber IS NOT NULL\"\"\")\n",
					"display(test_df)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Update and close existing Specialism records"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# MERGE for UPDATE only (close old records)\n",
					"\n",
					"# Count before update\n",
					"before_update = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as cnt \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms \n",
					"    WHERE IsActive = 'Y'\n",
					"\"\"\").collect()[0]['cnt']\n",
					"\n",
					"print(f\"Active records before: {before_update}\")\n",
					"print(f\"Expected to close: {expected_updates}\")\n",
					"\n",
					"merge_update_sql = \"\"\"\n",
					"MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS tgt\n",
					"USING (\n",
					"    SELECT \n",
					"        Target_StaffNumber,\n",
					"        QualificationName,\n",
					"        ChangeType\n",
					"    FROM vw_merged_inspector_specialisms\n",
					"    WHERE ChangeType IN ('New_Version', 'Data_Changed')\n",
					"      AND Target_StaffNumber IS NOT NULL\n",
					") AS src\n",
					"ON tgt.StaffNumber = src.Target_StaffNumber\n",
					"   AND tgt.QualificationName = src.QualificationName\n",
					"   AND tgt.IsActive = 'Y'\n",
					"WHEN MATCHED THEN\n",
					"    UPDATE SET\n",
					"        tgt.ValidTo = current_date(),\n",
					"        tgt.Current = 0,\n",
					"        tgt.IngestionDate = current_timestamp(),\n",
					"        tgt.LastUpdated = current_timestamp(),\n",
					"        tgt.IsActive = 'N'\n",
					"\"\"\"\n",
					"\n",
					"spark.sql(merge_update_sql)\n",
					"\n",
					"# Count after update\n",
					"after_update = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as cnt \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms \n",
					"    WHERE IsActive = 'Y'\n",
					"\"\"\").collect()[0]['cnt']\n",
					"\n",
					"actual_updates = before_update - after_update\n",
					"print(f\" UPDATE completed\")\n",
					"print(f\" Records closed: {actual_updates}\")\n",
					"\n",
					"if actual_updates == expected_updates:\n",
					"    print(f\" Validation passed ({actual_updates} = {expected_updates})\")\n",
					"else:\n",
					"    print(f\" Validation failed! Expected: {expected_updates}, Actual: {actual_updates}\")\n",
					"\n",
					""
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Insert New Specialism records"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# STEP 4: INSERT - Add new records\n",
					"\n",
					"# Count before insert\n",
					"before_insert = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as cnt \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"\"\"\").collect()[0]['cnt']\n",
					"\n",
					"print(f\"Total records before: {before_insert}\")\n",
					"print(f\"Expected to insert: {expected_inserts}\")\n",
					"\n",
					"insert_sql = \"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms (\n",
					"        StaffNumber, Firstname, Lastname, QualificationName, Proficien,\n",
					"        SourceSystemID, IngestionDate, ValidFrom, ValidTo, Current, RowID, IsActive\n",
					"    )\n",
					"    SELECT \n",
					"        StaffNumber, Firstname, Lastname, QualificationName, Proficien,\n",
					"        SourceSystemID, IngestionDate, ValidFrom, ValidTo, Current, RowID, IsActive\n",
					"    FROM vw_merged_inspector_specialisms\n",
					"    WHERE ChangeType IN ('New_Record', 'New_Version', 'Data_Changed')\n",
					"    \"\"\"\n",
					"\n",
					"spark.sql(insert_sql)\n",
					"\n",
					"# Count after insert\n",
					"after_insert = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as cnt \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"\"\"\").collect()[0]['cnt']\n",
					"\n",
					"actual_inserts = after_insert - before_insert\n",
					"print(f\" INSERT completed\")\n",
					"print(f\" Total records after: {after_insert}\")\n",
					"print(f\" Records inserted: {actual_inserts}\")\n",
					"\n",
					"if actual_inserts == expected_inserts:\n",
					"    print(f\" Validation passed ({actual_inserts} = {expected_inserts})\")\n",
					"else:\n",
					"    print(f\" Validation failed! Expected: {expected_inserts}, Actual: {actual_inserts}\")\n",
					"\n",
					"final_counts = spark.sql(\"\"\"\n",
					"    SELECT IsActive, COUNT(*) as Count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    GROUP BY IsActive\n",
					"    ORDER BY IsActive DESC\n",
					"\"\"\")\n",
					"final_counts.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Update Statements"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting insertion of new inspector specialisms records\")\n",
					"    \n",
					"    # First get count of potential new records\n",
					"    new_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.StaffNumber IS NULL\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {new_records_count} new inspector specialisms to insert\")\n",
					"    \n",
					"    # Insert new records that don't exist in the target table\n",
					"    logInfo(\"Inserting new records into sap_hr_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms (\n",
					"        StaffNumber, \n",
					"        Firstname, \n",
					"        Lastname, \n",
					"        QualificationName, \n",
					"        Proficien, \n",
					"        SourceSystemID, \n",
					"        IngestionDate, \n",
					"        ValidFrom, \n",
					"        ValidTo, \n",
					"        Current, \n",
					"        RowID, \n",
					"        IsActive, \n",
					"        LastUpdated\n",
					"    )\n",
					"    SELECT \n",
					"        souSpe.StaffNumber, \n",
					"        souSpe.Firstname, \n",
					"        souSpe.Lastname, \n",
					"        souSpe.QualificationName, \n",
					"        souSpe.Proficien, \n",
					"        'saphr', \n",
					"        current_timestamp(), \n",
					"        souSpe.ValidFrom, \n",
					"        souSpe.ValidTo, \n",
					"        1, \n",
					"        souSpe.RowID, \n",
					"        'Y', \n",
					"        current_timestamp()\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.StaffNumber IS NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Verify the insertion was successful\n",
					"    actual_inserted = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE IngestionDate = current_date()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Successfully inserted {actual_inserted} new inspector specialisms records\")\n",
					"    \n",
					"    if actual_inserted != new_records_count:\n",
					"        logInfo(f\"Note: Expected to insert {new_records_count} records but actually inserted {actual_inserted}\")\n",
					"    \n",
					"    logInfo(\"Inspector specialisms insertion completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting inspector specialisms records: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting update of obsolete inspector specialisms records\")\n",
					"    \n",
					"    # First, let's check for potential duplicates in the source data\n",
					"    logInfo(\"Checking for duplicate records in source data\")\n",
					"    duplicate_check = spark.sql(\"\"\"\n",
					"    SELECT \n",
					"        StaffNumber, \n",
					"        QualificationName,\n",
					"        COUNT(*) as duplicate_count\n",
					"    FROM (\n",
					"        SELECT \n",
					"            oldSpe.StaffNumber, \n",
					"            oldSpe.QualificationName\n",
					"        FROM \n",
					"            odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"        LEFT OUTER JOIN \n",
					"            odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"        ON \n",
					"            oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"            AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"        WHERE \n",
					"            souSpe.StaffNumber IS NULL\n",
					"            AND oldSpe.Current = 1\n",
					"    ) source_data\n",
					"    GROUP BY StaffNumber, QualificationName\n",
					"    HAVING COUNT(*) > 1\n",
					"    ORDER BY duplicate_count DESC\n",
					"    \"\"\")\n",
					"    \n",
					"    duplicate_count = duplicate_check.count()\n",
					"    if duplicate_count > 0:\n",
					"        logInfo(f\"Found {duplicate_count} duplicate combinations in source data - using DISTINCT to resolve\")\n",
					"        # Show some examples of duplicates for debugging\n",
					"        duplicate_examples = duplicate_check.limit(5).collect()\n",
					"        for row in duplicate_examples:\n",
					"            logInfo(f\"Duplicate: StaffNumber={row['StaffNumber']}, QualificationName={row['QualificationName']}, Count={row['duplicate_count']}\")\n",
					"    else:\n",
					"        logInfo(\"No duplicates found in source data\")\n",
					"    \n",
					"    # Get count of records to be updated (using DISTINCT to match our MERGE logic)\n",
					"    logInfo(\"Counting obsolete inspector specialisms records to update\")\n",
					"    obsolete_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM (\n",
					"        SELECT DISTINCT\n",
					"            oldSpe.StaffNumber, \n",
					"            oldSpe.QualificationName\n",
					"        FROM \n",
					"            odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"        LEFT OUTER JOIN \n",
					"            odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"        ON \n",
					"            oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"            AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"        WHERE \n",
					"            souSpe.StaffNumber IS NULL\n",
					"            AND oldSpe.Current = 1\n",
					"    ) distinct_obsolete\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {obsolete_records_count} obsolete inspector specialisms to update\")\n",
					"    \n",
					"    if obsolete_records_count == 0:\n",
					"        logInfo(\"No obsolete records found to update. Skipping MERGE operation.\")\n",
					"    else:\n",
					"        # Update records that are no longer present in the source data\n",
					"        logInfo(\"Updating obsolete records in sap_hr_inspector_Specialisms using MERGE with DISTINCT source\")\n",
					"        \n",
					"        merge_result = spark.sql(\"\"\"\n",
					"        MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS target\n",
					"        USING (\n",
					"            SELECT DISTINCT\n",
					"                oldSpe.StaffNumber, \n",
					"                oldSpe.QualificationName\n",
					"            FROM \n",
					"                odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"            LEFT OUTER JOIN \n",
					"                odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"            ON \n",
					"                oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"                AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"            WHERE \n",
					"                souSpe.StaffNumber IS NULL\n",
					"                AND oldSpe.Current = 1\n",
					"        ) AS source\n",
					"        ON \n",
					"            target.StaffNumber = source.StaffNumber\n",
					"            AND target.QualificationName = source.QualificationName\n",
					"            AND target.Current = 1\n",
					"        WHEN MATCHED THEN\n",
					"        UPDATE SET \n",
					"            target.Current = 0,\n",
					"            target.ValidTo = CURRENT_DATE()\n",
					"        \"\"\")\n",
					"        \n",
					"        logInfo(\"MERGE operation completed successfully\")\n",
					"        \n",
					"        # Verify the update was successful\n",
					"        logInfo(\"Verifying update results\")\n",
					"        updated_records = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE ValidTo = CURRENT_DATE() AND Current = 0\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        logInfo(f\"Total records now marked as obsolete today: {updated_records}\")\n",
					"        \n",
					"        # Additional verification - check if the expected records were updated\n",
					"        remaining_obsolete = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count\n",
					"        FROM (\n",
					"            SELECT DISTINCT\n",
					"                oldSpe.StaffNumber, \n",
					"                oldSpe.QualificationName\n",
					"            FROM \n",
					"                odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"            LEFT OUTER JOIN \n",
					"                odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"            ON \n",
					"                oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"                AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"            WHERE \n",
					"                souSpe.StaffNumber IS NULL\n",
					"                AND oldSpe.Current = 1\n",
					"        ) still_current_obsolete\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        if remaining_obsolete == 0:\n",
					"            logInfo(\"✓ All expected obsolete records have been successfully updated\")\n",
					"        else:\n",
					"            logError(f\"⚠ Warning: {remaining_obsolete} obsolete records are still marked as Current = 1\")\n",
					"        \n",
					"        # Log summary statistics\n",
					"        total_current_records = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 1\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        total_obsolete_records = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 0\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        logInfo(f\"Final state - Current records: {total_current_records}, Obsolete records: {total_obsolete_records}\")\n",
					"    \n",
					"    logInfo(\"Obsolete inspector specialisms update completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail with context\n",
					"    error_msg = f\"Error updating obsolete inspector specialisms records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logError(f\"Exception type: {type(e).__name__}\")\n",
					"    \n",
					"    # If it's a Spark SQL exception, try to extract more meaningful error info\n",
					"    if hasattr(e, 'java_exception') and e.java_exception is not None:\n",
					"        java_exception = str(e.java_exception)\n",
					"        if \"DeltaUnsupportedOperationException\" in java_exception:\n",
					"            logError(\"This appears to be a Delta Lake MERGE conflict error\")\n",
					"            logError(\"Possible causes: duplicate source rows, concurrent operations, or schema issues\")\n",
					"        elif \"multipleSourceRowMatchingTargetRowInMergeException\" in java_exception:\n",
					"            logError(\"Multiple source rows are matching the same target row - source data needs deduplication\")\n",
					"    \n",
					"    # Log the full stack trace for debugging\n",
					"    logException(e)\n",
					"    \n",
					"    # Provide recovery suggestions\n",
					"\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"    \n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs and cleaning up\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting insertion of changed inspector specialisms records\")\n",
					"    \n",
					"    # First get count of changed records to be inserted\n",
					"    changed_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    INNER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.RowID <> souSpe.RowID\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {changed_records_count} changed inspector specialisms to insert\")\n",
					"    \n",
					"    # Insert new versions of records that have changed (different RowID)\n",
					"    logInfo(\"Inserting changed records into sap_hr_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms (\n",
					"        StaffNumber, \n",
					"        Firstname, \n",
					"        Lastname, \n",
					"        QualificationName, \n",
					"        Proficien, \n",
					"        SourceSystemID, \n",
					"        IngestionDate, \n",
					"        ValidFrom, \n",
					"        ValidTo, \n",
					"        Current, \n",
					"        RowID, \n",
					"        IsActive, \n",
					"        LastUpdated\n",
					"    )\n",
					"    SELECT \n",
					"        souSpe.StaffNumber, \n",
					"        souSpe.Firstname, \n",
					"        souSpe.Lastname, \n",
					"        souSpe.QualificationName, \n",
					"        souSpe.Proficien, \n",
					"        'saphr', \n",
					"        current_timestamp(), \n",
					"        souSpe.ValidFrom, \n",
					"        souSpe.ValidTo, \n",
					"        1, \n",
					"        souSpe.RowID, \n",
					"        'Y', \n",
					"        current_timestamp()\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.RowID <> souSpe.RowID\n",
					"    \"\"\")\n",
					"    \n",
					"    # Verify the insertion was successful\n",
					"    inserted_records = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE IngestionDate = CURRENT_DATE() AND Current = 1\n",
					"    \"\"\").collect()[0]['count'] - spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE ValidFrom = CURRENT_DATE() AND Current = 1\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Successfully inserted {inserted_records} changed inspector specialisms records\")\n",
					"    \n",
					"    if inserted_records != changed_records_count:\n",
					"        logInfo(f\"Note: Expected to insert {changed_records_count} records but actual count differs\")\n",
					"    \n",
					"    logInfo(\"Changed inspector specialisms insertion completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting changed inspector specialisms records: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    logInfo(\"Starting enhanced MERGE operation for inspector specialisms with comprehensive record tracking\")\n",
					"    \n",
					"    # Get baseline counts before any operations\n",
					"    logInfo(\"Capturing baseline record counts\")\n",
					"    baseline_current = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 1\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    baseline_obsolete = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 0\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Baseline - Current records: {baseline_current}, Obsolete records: {baseline_obsolete}\")\n",
					"    \n",
					"    # Check for duplicates in the source data that could cause MERGE conflicts\n",
					"    logInfo(\"Checking for duplicate records in source data\")\n",
					"    duplicate_records = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM (\n",
					"        SELECT \n",
					"            StaffNumber,\n",
					"            QualificationName,\n",
					"            COUNT(*) as duplicate_count\n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 1\n",
					"        AND ValidFrom = current_date()\n",
					"        GROUP BY StaffNumber, QualificationName\n",
					"        HAVING COUNT(*) > 1\n",
					"    ) duplicates\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    if duplicate_records > 0:\n",
					"        logInfo(f\"Found {duplicate_records} duplicate combinations in source data - using ROW_NUMBER() to resolve\")\n",
					"    else:\n",
					"        logInfo(\"No duplicates found in source data\")\n",
					"    \n",
					"    # Get count of records to be updated (current records with today's date)\n",
					"    records_to_update_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 1\n",
					"    AND ValidFrom = current_date()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {records_to_update_count} current inspector specialisms records to potentially update\")\n",
					"    \n",
					"    if records_to_update_count == 0:\n",
					"        logInfo(\"No records found to update. Skipping MERGE operation.\")\n",
					"        records_updated_in_operation = 0\n",
					"    else:\n",
					"        # Get count of records that were already updated today (before this operation)\n",
					"        pre_operation_today_count = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 0 AND ValidTo = CURRENT_DATE()\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        logInfo(f\"Records already marked obsolete today before operation: {pre_operation_today_count}\")\n",
					"        \n",
					"        # Perform the MERGE operation\n",
					"        logInfo(\"Performing MERGE operation with CTE deduplication\")\n",
					"        spark.sql(\"\"\"\n",
					"        -- Step 1: Deduplicate the source table (newSpe)\n",
					"        WITH DeduplicatedSource AS (\n",
					"            SELECT \n",
					"                StaffNumber,\n",
					"                QualificationName,\n",
					"                ValidFrom,\n",
					"                Current,\n",
					"                ROW_NUMBER() OVER (\n",
					"                    PARTITION BY StaffNumber, QualificationName \n",
					"                    ORDER BY ValidFrom DESC\n",
					"                ) AS row_num\n",
					"            FROM \n",
					"                odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"            WHERE \n",
					"                Current = 1\n",
					"                AND ValidFrom = current_date()\n",
					"        )\n",
					"\n",
					"        -- Step 2: Perform the MERGE operation using the deduplicated source\n",
					"        MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS oldSpe\n",
					"        USING DeduplicatedSource AS newSpe\n",
					"        ON \n",
					"            oldSpe.StaffNumber = newSpe.StaffNumber\n",
					"            AND oldSpe.QualificationName = newSpe.QualificationName\n",
					"            AND oldSpe.Current = 1\n",
					"            AND oldSpe.ValidFrom < current_date()\n",
					"            AND newSpe.row_num = 1 -- Ensure only one row per combination\n",
					"        WHEN MATCHED THEN\n",
					"        UPDATE SET \n",
					"            oldSpe.Current = 0,\n",
					"            oldSpe.ValidTo = current_date()\n",
					"        \"\"\")\n",
					"        \n",
					"        logInfo(\"MERGE operation completed successfully\")\n",
					"        \n",
					"        # Get post-operation counts\n",
					"        post_operation_today_count = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 0 AND ValidTo = CURRENT_DATE()\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        # Calculate records updated in this specific operation\n",
					"        records_updated_in_operation = post_operation_today_count - pre_operation_today_count\n",
					"        \n",
					"        logInfo(f\"Records updated in this operation: {records_updated_in_operation}\")\n",
					"        logInfo(f\"Total records marked obsolete today: {post_operation_today_count}\")\n",
					"        \n",
					"        # Verify the update count matches expectations\n",
					"        if records_updated_in_operation != records_to_update_count:\n",
					"            # This might be expected if not all current records have matching older records\n",
					"            logInfo(f\"Note: Expected to potentially update {records_to_update_count} records but actually updated {records_updated_in_operation}\")\n",
					"            logInfo(\"This is normal if some current records don't have corresponding older records to update\")\n",
					"        else:\n",
					"            logInfo(\"✓ Update count matches expected count\")\n",
					"    \n",
					"    # Get final record counts for comprehensive tracking\n",
					"    final_current = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 1\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    final_obsolete = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 0\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    # Set record_count to total records in the table (standard metric for tracking)\n",
					"    total_record_count = final_current + final_obsolete\n",
					"    result[\"record_count\"] = total_record_count\n",
					"    \n",
					"    logInfo(f\"Total record count in table: {total_record_count}\")\n",
					"    \n",
					"    # Validate total record count consistency\n",
					"    baseline_total = baseline_current + baseline_obsolete\n",
					"    final_total = final_current + final_obsolete\n",
					"    \n",
					"    if baseline_total != final_total:\n",
					"        warning_msg = f\"Warning: Total record count changed from {baseline_total} to {final_total}\"\n",
					"        logInfo(warning_msg)\n",
					"        result[\"status\"] = \"warning\"\n",
					"    \n",
					"    logInfo(f\"Final state - Current records: {final_current}, Obsolete records: {final_obsolete}\")\n",
					"    logInfo(f\"Net change - Current: {final_current - baseline_current}, Obsolete: {final_obsolete - baseline_obsolete}\")\n",
					"    \n",
					"    logInfo(\"Enhanced MERGE operation for inspector specialisms completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information and limit to 300 characters\n",
					"    error_msg = f\"Error during MERGE operation for inspector specialisms: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    \n",
					"    # Enhanced error context for common issues\n",
					"    if \"DeltaUnsupportedOperationException\" in str(e):\n",
					"        logError(\"Delta Lake MERGE conflict detected - check for duplicate source rows\")\n",
					"        error_msg += \" [Delta MERGE conflict]\"\n",
					"    elif \"AnalysisException\" in str(e):\n",
					"        logError(\"SQL analysis error - check table structure and column references\")\n",
					"        error_msg += \" [SQL analysis error]\"\n",
					"    elif \"multipleSourceRowMatchingTargetRowInMergeException\" in str(e):\n",
					"        logError(\"Multiple source rows matching target - deduplication logic may need adjustment\")\n",
					"        error_msg += \" [Multiple source matches]\"\n",
					"    \n",
					"    logException(e)\n",
					"    \n",
					"    # Update result for error case\n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = error_msg[:300]  # Truncate to 300 characters\n",
					"    result[\"record_count\"] = -1  # Indicate failure with -1 count\n",
					"    \n",
					"    # Log recovery suggestions\n",
					"\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs and preparing final result\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Log the final result for debugging\n",
					"    \n",
					"    logInfo(f\"Final result summary:\")\n",
					"    logInfo(f\"  Status: {result['status']}\")\n",
					"    logInfo(f\"  Total record count: {result['record_count']}\")\n",
					"    logInfo(f\"  Error message: {result['error_message']}\")\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}