{
	"name": "horizon_folder",
	"properties": {
		"folder": {
			"name": "archive/odw-harmonised/DocumentTree"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0d990983-6ac7-4061-8d16-62d0be934631"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import asyncio\n",
					"import nest_asyncio\n",
					"import tracemalloc\n",
					"tracemalloc.start()\n",
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os\n",
					"import re\n",
					"from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat,max as spark_max\n",
					"from pyspark.sql.types import StringType, DateType, TimestampType, IntegerType, FloatType, StructType, StructField\n",
					"from delta import DeltaTable"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"%%configure -f\n",
					"{\n",
					"    \"conf\": {\n",
					"        \"spark.kryoserializer.buffer.max\": \"2047m\",\n",
					"        \"spark.driver.maxResultSize\": \"10g\",\n",
					"        \"spark.rpc.message.maxSize\": \"1280\"\n",
					"    }\n",
					"}"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_mount_storage"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialize Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_utils_common_logging_output"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from datetime import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"# Define variables\n",
					"target_table = \"odw_harmonised_db.horizon_folder\"\n",
					"\n",
					"# Initialize tracking variables\n",
					"start_exec_time = str(datetime.now())\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"date_folder=''\n",
					"source_folder='Horizon' #need change\n",
					"specific_file='' # if not provided, it will ingest all files in the date_folder\n",
					"delete_existing_table=False\n",
					"dataAttribute = \"\"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''\n",
					"Database = 'odw_harmonised_db'\n",
					"TableName = ''"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder.getOrCreate()\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"source_path: str = f\"abfss://odw-raw@{storage_account}{source_folder}/\"\n",
					"harmonised_container = f\"abfss://odw-harmonised@{storage_account}\"\n",
					"print(harmonised_container)"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize Application Insights logger\n",
					"app_insight_logger = ProcessingLogger()"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"def create_table_from_schema(jsonschema: str, db_name: str, table_name: str, target_container: str, target_folder: str, change_data_feed=False):\n",
					"   \n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    target_loc = f\"{target_container}/{target_folder}\".rstrip(\"/\")\n",
					"\n",
					"    # Convert array fields to string in schema definition\n",
					"    schema_dict = json.loads(jsonschema)\n",
					"    for field in schema_dict.get(\"fields\", []):\n",
					"        if field.get(\"type\") == \"array\":\n",
					"            field[\"type\"] = \"string\"\n",
					"\n",
					"    # Create empty DataFrame with adjusted schema\n",
					"    schema = StructType.fromJson(schema_dict)\n",
					"    df = spark.createDataFrame([], schema)\n",
					"\n",
					"    # Verify database existence\n",
					"    if db_name not in [db.name for db in spark.catalog.listDatabases()]:\n",
					"        raise NameError(f\"Database '{db_name}' does not exist!\")\n",
					"\n",
					"    # Verify table existence\n",
					"    if table_name in [table.name for table in spark.catalog.listTables(db_name)]:\n",
					"        table_details = spark.sql(f\"DESCRIBE DETAIL {db_name}.{table_name}\").toPandas()\n",
					"        \n",
					"        if len(table_details) > 1:\n",
					"            raise logError(\"Multiple parquet files detected. Please investigate!\")\n",
					"        \n",
					"        existing_location = table_details['location'][0].rstrip(\"/\")\n",
					"        if existing_location == target_loc:\n",
					"            logInfo(f\"Table '{db_name}.{table_name}' already exists at correct location.\")\n",
					"        else:\n",
					"            raise logError(f\"Table exists but at a different location: {existing_location}. Expected: {target_loc}\")\n",
					"        return\n",
					"\n",
					"    # Create table if not exists\n",
					"    if not DeltaTable.isDeltaTable(spark, target_loc):\n",
					"        df.write.option(\"mergeSchema\", \"false\").format(\"delta\").save(target_loc)\n",
					"    \n",
					"    logInfo(f\"Creating table {db_name}.{table_name}\")\n",
					"    spark.sql(f\"CREATE TABLE {db_name}.{table_name} USING DELTA LOCATION '{target_loc}'\")\n",
					"    \n",
					"    if change_data_feed:\n",
					"        spark.sql(f\"ALTER TABLE {db_name}.{table_name} SET TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\")\n",
					"    \n",
					"    logInfo(f\"Table '{db_name}.{table_name}' created successfully!\")"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"def ingest_adhoc(storage_account, definition, folder_path, filename, expected_from, expected_to):\n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
					"    ingestion_failure: bool = False\n",
					"    \n",
					"    harmonised_container = f\"abfss://odw-harmonised@{storage_account}\"\n",
					"    harmonised_path = f\"{definition['harmonised_Path']}\" + \"/\" #need remove text\n",
					"    harmonised_table_name = f\"{definition['harmonised_Table_Name']}\" #need remove text \n",
					"    source_filename_start = definition['Source_Filename_Start'] \n",
					"    process_name = mssparkutils.runtime.context['currentNotebookName']\n",
					"    \n",
					"    logging_container = f\"abfss://logging@{storage_account}\"\n",
					"    logging_table_name = 'tables_logs'\n",
					"    ingestion_log_table_location = logging_container + logging_table_name\n",
					"    \n",
					"    if 'harmonised_Table_Definition' in definition:\n",
					"        harmonised_table_loc = f\"abfss://odw-config@{storage_account}\" + definition['harmonised_Table_Definition']\n",
					"        harmonised_table_def_json = spark.read.text(harmonised_table_loc, wholetext=True).first().value\n",
					"    else:\n",
					"       harmonised_table_def_json = mssparkutils.notebook.run('/py_get_schema_from_url', 30, {'db_name': f'{Database}', 'entity_name': definition['Source_Frequency_Folder']})\n",
					"    \n",
					"    if not any([table.name.lower() == harmonised_table_name.lower() for table in spark.catalog.listTables(f'{Database}')]):\n",
					"        create_table_from_schema(harmonised_table_def_json, f\"{Database}\", harmonised_table_name, harmonised_container, harmonised_path + harmonised_table_name)\n",
					"    \n",
					"    table_metadata = spark.sql(f\"DESCRIBE EXTENDED {Database}.{harmonised_table_name}\")\n",
					"    data_format = table_metadata.filter(table_metadata.col_name == \"Provider\").collect()[0].data_type\n",
					"    if data_format == \"parquet\":\n",
					"        replace = spark.sql(f\"SELECT * FROM {Database}.{harmonised_table_name}\")\n",
					"        replace.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{Database}.{harmonised_table_name}_new\")\n",
					"        spark.sql(f\"DROP TABLE IF EXISTS {Database}.{harmonised_table_name}\")\n",
					"        spark.sql(f\"ALTER TABLE {Database}.{harmonised_table_name}_new RENAME TO {Database}.{harmonised_table_name}\")\n",
					"    \n",
					"    try:\n",
					"        harmonised_table_location = spark.sql(f\"DESCRIBE FORMATTED {Database}.{harmonised_table_name}\") \\\n",
					"            .filter(\"col_name = 'Location'\") \\\n",
					"            .select(\"data_type\") \\\n",
					"            .collect()[0][0]\n",
					"    except:\n",
					"        harmonised_table_location = harmonised_container + harmonised_path + harmonised_table_name\n",
					"    \n",
					"    harmonised_table_df = spark.read.format(\"delta\").load(harmonised_table_location)\n",
					"    rows =harmonised_table_df.filter((harmonised_table_df.expected_from == expected_from) & (harmonised_table_df.expected_to == expected_to)).count()\n",
					"    \n",
					"    last_load_date = harmonised_table_df.agg(spark_max(\"ingested_datetime\")).collect()[0][0]\n",
					"\n",
					"    jobId = mssparkutils.env.getJobId()\n",
					"    \n",
					"    if '.csv' in filename.lower():\n",
					"        df = spark.read.options(quote='\"', escape='\\\\', encoding='utf8', header=True, multiLine=True, columnNameOfCorruptRecord='corrupted_records', mode=\"PERMISSIVE\").csv(f\"{folder_path}/{filename}\")\n",
					"        if \"corrupted_records\" in df.columns:\n",
					"            print(f\"Corrupted Records detected from CSV ingestion in {filename}\")\n",
					"            ingestion_failure = True\n",
					"    else:\n",
					"        raise RuntimeError(f\"This file type for {filename} is unsupported\")\n",
					"    \n",
					"    sparkDF = df.select([col for col in df.columns if not col.startswith('Unnamed')])\n",
					"    rows_raw = sparkDF.count()\n",
					"    \n",
					"    sparkDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp())\n",
					"    sparkDF = sparkDF.withColumn(\"ingested_by_process_name\", lit(process_name))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_from\", lit(expected_from))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_to\", lit(expected_to))\n",
					"    sparkDF = sparkDF.withColumn(\"input_file\", input_file_name())\n",
					"    sparkDF = sparkDF.withColumn(\"modified_datetime\", current_timestamp())\n",
					"    sparkDF = sparkDF.withColumn(\"modified_by_process_name\", lit(process_name))\n",
					"    sparkDF = sparkDF.withColumn(\"entity_name\", lit(source_filename_start))\n",
					"    sparkDF = sparkDF.withColumn(\"file_ID\", sha2(concat(lit(input_file_name()), current_timestamp().cast(\"string\")), 256))\n",
					"    \n",
					"    schema = json.loads(harmonised_table_def_json)\n",
					"    for field in schema['fields']:\n",
					"        if field['type'] == 'array':\n",
					"            field['type'] = 'string'\n",
					"    schema = StructType.fromJson(schema)\n",
					"    \n",
					"    cols_orig = sparkDF.schema.names\n",
					"    cols = [re.sub('[^0-9a-zA-Z]+', '_', i).lower() for i in cols_orig]\n",
					"    cols = [colm.rstrip('_') for colm in cols]\n",
					"    newlist = []\n",
					"    for i, v in enumerate(cols):\n",
					"        totalcount = cols.count(v)\n",
					"        count = cols[:i].count(v)\n",
					"        newlist.append(v + str(count + 1) if totalcount > 1 else v)\n",
					"    for colix in range(len(cols_orig)):\n",
					"        sparkDF = sparkDF.toDF(*newlist)\n",
					"    \n",
					"    for field in sparkDF.schema:\n",
					"        table_field = next((f for f in schema if f.name.lower() == field.name.lower()), None)\n",
					"        if table_field is not None and field.dataType != table_field.dataType:\n",
					"            sparkDF = sparkDF.withColumn(field.name, col(field.name).cast(table_field.dataType))\n",
					"    \n",
					"    logInfo(f\"Writing data to {Database}.{harmonised_table_name}\")\n",
					"    sparkDF.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").saveAsTable(f\"{Database}.{harmonised_table_name}\")\n",
					"    logInfo(f\"Written data to {Database}.{harmonised_table_name}\")\n",
					"    end_exec_time = datetime.now()\n",
					"    \n",
					"    harmonised_table_df_new = spark.read.format(\"delta\").load(harmonised_table_location)\n",
					"    rows_new = harmonised_table_df.filter((harmonised_table_df.expected_from == expected_from) & (harmonised_table_df.expected_to == expected_to)).count()\n",
					"    \n",
					"    #logging and Tracking\n",
					"    table_name = f\"{Database}.{definition['harmonised_Table_Name']}\"\n",
					"    insert_count = rows_new\n",
					"\n",
					"    try:\n",
					"        ingestion_log_schema_loc = f\"abfss://odw-config@{storage_account}tables_logs.json\"\n",
					"        ingestion_log_schema = spark.read.text(ingestion_log_schema_loc, wholetext=True).first().value\n",
					"        \n",
					"        try:\n",
					"            ingestion_log_df = spark.read.format(\"delta\").load(ingestion_log_table_location)\n",
					"            table_exists = True\n",
					"        except Exception as e:\n",
					"            logInfo(f\"Ingestion log table not found at {ingestion_log_table_location}. Creating a new one.\")\n",
					"            table_exists = False\n",
					"        \n",
					"        new_log_entry = sparkDF.select(\n",
					"            \"file_ID\",\n",
					"            \"ingested_datetime\",\n",
					"            \"ingested_by_process_name\",\n",
					"            \"input_file\",\n",
					"            \"modified_datetime\",\n",
					"            \"modified_by_process_name\",\n",
					"            \"entity_name\"\n",
					"        ).limit(1)\n",
					"        # new_log_entry = new_log_entry.withColumn(\"rows_raw\", lit(sparkDF.count()))\n",
					"        # new_log_entry = new_log_entry.withColumn(\"rows_new\", lit(harmonised_table_df.filter((harmonised_table_df.expected_from == expected_from) & (harmonised_table_df.expected_to == expected_to)).count()))\n",
					"        rows_raw_count = sparkDF.count()\n",
					"        rows_new_count = harmonised_table_df.filter((col(\"expected_from\") == expected_from) &(col(\"expected_to\") == expected_to) & (col(\"ingested_datetime\") > last_load_date)).count()\n",
					"        new_log_entry = new_log_entry.withColumn(\"rows_raw\", lit(rows_raw_count))\n",
					"        new_log_entry = new_log_entry.withColumn(\"rows_new\", lit(rows_new_count))\n",
					"\n",
					"        if not table_exists:\n",
					"            new_log_entry.write.format(\"delta\").option(\"path\", ingestion_log_table_location).saveAsTable(f\"logging.{logging_table_name}\")\n",
					"            logInfo(f\"Updating ingestion logging table {logging_table_name} with first entry.\")\n",
					"        else:\n",
					"            new_log_entry.write.format(\"delta\").mode(\"append\").saveAsTable(f\"logging.{logging_table_name}\")\n",
					"            logInfo(\"Appended to existing ingestion logging table with new entry\")\n",
					"    \n",
					"    except Exception as e:\n",
					"        logInfo('Logging to tables_logs failed')\n",
					"    \n",
					"    if rows_raw <= rows_new:\n",
					"        logInfo('All rows have successfully been written')\n",
					"    else:\n",
					"        logError(f\"All rows have NOT been successfully written. Expected {rows_raw} but {rows_new} written\")\n",
					"        ingestion_failure = True\n",
					"    \n",
					"    return ingestion_failure, rows_raw,harmonised_table_name,end_exec_time"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"async def ingest_horizon(date_folder: str, file: str):\n",
					"    row_count = 0\n",
					"    error_message = \"\"\n",
					"    start_exec_time = datetime.now()\n",
					"    try:\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"        if date_folder == '':\n",
					"            date_folder = datetime.now().date()\n",
					"        else:\n",
					"            date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\")\n",
					"\n",
					"        date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"\n",
					"        # READ ORCHESTRATION DATA\n",
					"        path_to_orchestration_file = f\"abfss://odw-config@{storage_account}/orchestration/orchestration.json\"\n",
					"        df = spark.read.option(\"multiline\", \"true\").json(path_to_orchestration_file)\n",
					"        definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"        definition = next((d for d in definitions \n",
					"                          if (specific_file == '' or d['Source_Filename_Start'] == specific_file) \n",
					"                          and file.startswith(d['Source_Filename_Start']) \n",
					"                          and d['Load_Enable_status'] == 'True'), None)\n",
					"\n",
					"        if definition:\n",
					"            expected_from = date_folder - timedelta(days=1)\n",
					"            expected_from = datetime.combine(expected_from, datetime.min.time())\n",
					"            expected_to = expected_from + timedelta(days=definition['Expected_Within_Weekdays'])\n",
					"\n",
					"            if delete_existing_table:\n",
					"                print(f\"Deleting existing table if exists odw_harmonised_db.{definition['harmonised_Table_Name']}\")\n",
					"                mssparkutils.notebook.run('/utils/py_delete_table', 300, arguments={'db_name': 'odw_harmonised_db', 'table_name': definition['harmonised_Table_Name']})\n",
					"\n",
					"            ingestion_failure, row_count,harmonised_table_name,end_exec_time = ingest_adhoc(storage_account, definition, source_path, file, expected_from, expected_to)\n",
					"\n",
					"            if ingestion_failure:\n",
					"                print(\"Errors reported during Ingestion!!\")\n",
					"                error_message = \"Errors reported during Ingestion!!\"\n",
					"            else:\n",
					"                print(\"No Errors reported during Ingestion\")\n",
					"        else:\n",
					"            if specific_file != '':\n",
					"                raise logError(f\"No definition found for {file}\")\n",
					"            else:\n",
					"                print(f\"Condition Not Satisfied for Load {file} File\")\n",
					"    except Exception as e:\n",
					"        print(f\"Failed to process {file}: {e}\")\n",
					"        error_message = f\"Failed to process {file}: {str(e)}\"\n",
					"\n",
					"    return row_count, error_message,harmonised_table_name,start_exec_time,end_exec_time"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"source": [
					"import asyncio\n",
					"import nest_asyncio\n",
					"from datetime import datetime\n",
					"\n",
					"nest_asyncio.apply()\n",
					"\n",
					"async def process_file(file):\n",
					"    # Ingest file, returning insert count, error message, and table name\n",
					"    insert_count, error_message, harmonised_table_name, start_exec_time, end_exec_time = await ingest_horizon(date_folder=latest_folder, file=file)\n",
					"\n",
					"    # For testing error, uncomment below line or inject error_message dynamically\n",
					"    # error_message = \"Errors reported during Ingestion!!\"\n",
					"\n",
					"    DurationSeconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"    ActivityType = mssparkutils.runtime.context['currentNotebookName'] + \" NoteBook\"\n",
					"    StatusMessage = (\n",
					"    f\"Successfully loaded data from {file} into {harmonised_table_name} table\"\n",
					"    if not error_message\n",
					"    else f\"Failed to load data from {file} into {harmonised_table_name} table\"\n",
					")\n",
					"    Stage = \"Success\" if not error_message else \"Failed\"\n",
					"\n",
					"    params = {\n",
					"        \"Stage\": Stage,\n",
					"        \"PipelineName\": PipelineName,\n",
					"        \"PipelineRunID\": PipelineRunID,\n",
					"        \"StartTime\": start_exec_time.isoformat(),\n",
					"        \"EndTime\": end_exec_time.isoformat(),\n",
					"        \"Inserts\": insert_count,\n",
					"        \"Updates\": update_count,\n",
					"        \"Deletes\": delete_count,\n",
					"        \"ErrorMessage\": error_message,\n",
					"        \"StatusMessage\": StatusMessage,\n",
					"        \"PipelineTriggerID\": PipelineTriggerID,\n",
					"        \"PipelineTriggerName\": PipelineTriggerName,\n",
					"        \"PipelineTriggerType\": PipelineTriggerType,\n",
					"        \"PipelineTriggeredbyPipelineName\": PipelineTriggeredbyPipelineName,\n",
					"        \"PipelineTriggeredbyPipelineRunID\": PipelineTriggeredbyPipelineRunID,\n",
					"        \"PipelineExecutionTimeInSec\": DurationSeconds,\n",
					"        \"ActivityType\": ActivityType,\n",
					"        \"DurationSeconds\": DurationSeconds,\n",
					"        \"StatusCode\": \"200\" if Stage == \"Success\" else \"500\",\n",
					"        \"AppInsCustomEventName\": \"ODW_Master_Pipeline_Logs\"\n",
					"    }\n",
					"\n",
					"    # Send telemetry in a separate thread\n",
					"    await asyncio.to_thread(send_telemetry_to_app_insights, params, instrumentation_key)\n",
					"\n",
					"    # If error found, raise exception to stop overall processing and quit notebook\n",
					"    if error_message:\n",
					"        print(f\"Notebook failed for file {file}: {error_message}\")\n",
					"        raise RuntimeError(f\"Notebook failed due to error in file {file}: {error_message}\")\n",
					"\n",
					"async def load_horizon_async():\n",
					"    # Process files sequentially to stop on first error,\n",
					"    # or use gather with return_exceptions=False to stop on exceptions\n",
					"    for file in horizon_files:\n",
					"        await process_file(file)\n",
					"\n",
					"# Run and handle exception to quit notebook cleanly\n",
					"try:\n",
					"    loop = asyncio.get_event_loop()\n",
					"    loop.run_until_complete(load_horizon_async())\n",
					"except RuntimeError as e:\n",
					"    print(str(e))\n",
					"    import sys\n",
					"    sys.exit(1)  # Quits the notebook with error code 1"
				],
				"execution_count": 42
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check for new, updated or deleted data\n",
					"- This script checks for new, updated or deleted data by checking the source data (horizon tables) against the target (odw_harmonised_db.casework tables)\n",
					"- **New Data:** where an main Reference in the source does not exist in the target, then NewData flag is set to 'Y'\n",
					"- **Updated data:** Comparison occurs on Reference Fields in source and in target where the row hash is different i.e. there is a change in one of the columns. NewData flag is set to 'Y'\n",
					"- **Deleted data:** where an Reference info in the target exists but the same identifyers don't exist in the source. DeletedData flag is set to 'Y'\n",
					"\n",
					"## View horizon_folder is created"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(f\"Starting horizon_folder data processing for {target_table}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Build horizon_folder table\n",
					"-- Gets modified or deleted from source rows\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW horizon_folder_new\n",
					"\n",
					"     AS\n",
					"\n",
					"-- gets data that matches of SourceID and flags that it is modified based on a row (md5) hash. Flags as \"NewData\"\n",
					"-- gets data that is in the target but not in source. Flags as \"DeletedData\"\n",
					"\n",
					"SELECT DISTINCT\n",
					"    CASE\n",
					"        WHEN T1.id IS NULL\n",
					"        THEN T3.HorizonFolderID\n",
					"        ELSE NULL\n",
					"    END                             AS HorizonFolderID,\n",
					"    T1.id                           AS ID,\n",
					"    T1.casereference\t            AS CaseReference,\n",
					"    T1.displaynameenglish\t        AS DisplayNameEnglish,\n",
					"    T1.displaynamewelsh\t            AS DisplayNameWelsh,\n",
					"    T1.parentfolderid\t            AS ParentFolderID,\n",
					"    T1.casenodeid\t                AS CaseNodeId,\n",
					"    T1.casestage\t                AS CaseStage,\n",
					"    T2.SourceSystemID               AS SourceSystemID,\n",
					"    to_timestamp(T1.expected_from)  AS IngestionDate,\n",
					"    NULL                            AS ValidTo,\n",
					"    md5(\n",
					"        concat(\n",
					"            IFNULL(T1.id,'.'),\n",
					"            IFNULL(T1.casereference,'.'),\n",
					"            IFNULL(T1.displaynameenglish,'.'),\n",
					"            IFNULL(T1.displaynamewelsh,'.'),\n",
					"            IFNULL(T1.parentfolderid,'.'),\n",
					"            IFNULL(T1.casenodeid,'.'),\n",
					"            IFNULL(T1.casestage,'.')\n",
					"        ))                          AS RowID, -- this hash should contain all the defining fields\n",
					"    'Y'                             AS IsActive,\n",
					"    T3.IsActive                     AS HistoricIsActive\n",
					"\n",
					"FROM odw_harmonised_db.horizon_folder T1\n",
					"LEFT JOIN odw_harmonised_db.main_sourcesystem_fact T2 \n",
					"    ON \"DocumentTree\" = T2.Description AND \n",
					"        T2.IsActive = 'Y'\n",
					"FULL JOIN odw_harmonised_db.horizon_folder T3 \n",
					"    ON T1.id = T3.ID AND \n",
					"        T3.IsActive = 'Y'\n",
					"WHERE\n",
					"    -- flags new data        \n",
					"    (CASE\n",
					"        WHEN T1.casereference = T3.CaseReference AND md5(\n",
					"            concat(\n",
					"                IFNULL(T1.id,'.'),\n",
					"                IFNULL(T1.casereference,'.'),\n",
					"                IFNULL(T1.displaynameenglish,'.'),\n",
					"                IFNULL(T1.displaynamewelsh,'.'),\n",
					"                IFNULL(T1.parentfolderid,'.'),\n",
					"                IFNULL(T1.casenodeid,'.'),\n",
					"                IFNULL(T1.casestage,'.')\n",
					"            )) <> T3.RowID  -- same record, changed data\n",
					"        THEN 'Y'\n",
					"        WHEN T3.ID IS NULL -- new record\n",
					"        THEN 'Y'\n",
					"    ELSE 'N'\n",
					"    END  = 'Y' )\n",
					"    AND T1.id IS NOT NULL\n",
					"    AND NOT(T1.id = '29309932' AND T1.casestage = 'Initial Documents') --- Hardcoded exception as per Gareth request for data consistency\n",
					"    AND T1.expected_from = (SELECT MAX(expected_from) FROM odw_harmonised_db.horizon_folder)\n",
					";"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Dataset is created that contains changed data and corresponding target data\n",
					"- This script combines data that has been updated, Deleted or is new, with corresponding target data\n",
					"- View **casework_all_appeals_new** is unioned to the target data filter to only those rows where changes have been detected\n",
					"## View horizon_folder_changed_rows is created"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Create new and updated dataset\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW horizon_folder_changed_rows\n",
					"\n",
					"    AS\n",
					"\n",
					"-- gets updated, deleted and new rows \n",
					"SELECT \n",
					"    HorizonFolderID,\n",
					"    ID,\n",
					"    CaseReference,\n",
					"    DisplayNameEnglish,\n",
					"    DisplayNameWelsh,\n",
					"    ParentFolderID,\n",
					"    CaseNodeId,\n",
					"    CaseStage,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"    \n",
					"\n",
					"From horizon_folder_new WHERE HistoricIsActive = 'Y' or HistoricIsActive IS NULL\n",
					"\n",
					"    UNION ALL\n",
					"\n",
					"-- gets original versions of updated rows so we can update EndDate and set IsActive flag to 'N'\n",
					"SELECT\n",
					"    \n",
					"    HorizonFolderID,\n",
					"    ID,\n",
					"    CaseReference,\n",
					"    DisplayNameEnglish,\n",
					"    DisplayNameWelsh,\n",
					"    ParentFolderID,\n",
					"    CaseNodeId,\n",
					"    CaseStage,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"    \n",
					"FROM odw_harmonised_db.horizon_folder\n",
					"WHERE ID IN (SELECT ID FROM horizon_folder_new WHERE HorizonFolderID IS NULL) AND IsActive = 'Y'; "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW Loading_month\n",
					"\n",
					"    AS\n",
					"\n",
					"SELECT DISTINCT\n",
					"    IngestionDate AS IngestionDate,\n",
					"    to_timestamp(date_sub(IngestionDate,1)) AS ClosingDate,\n",
					"    'Y' AS IsActive\n",
					"\n",
					"FROM horizon_folder_new;\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW horizon_folder_changed_rows_final\n",
					"\n",
					"    AS\n",
					"\n",
					"SELECT \n",
					"    HorizonFolderID,\n",
					"    ID,\n",
					"    CaseReference,\n",
					"    DisplayNameEnglish,\n",
					"    DisplayNameWelsh,\n",
					"    ParentFolderID,\n",
					"    CaseNodeId,\n",
					"    CaseStage,\n",
					"    T1.SourceSystemID,\n",
					"    T1.IngestionDate,\n",
					"    T1.ValidTo,\n",
					"    T1.RowID,\n",
					"    T1.IsActive,\n",
					"    T2.ClosingDate\n",
					"FROM horizon_folder_changed_rows T1\n",
					"FULL JOIN Loading_month T2 ON T1.IsActive = T2.IsActive"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# View horizon_folder_changed_rows is used in a merge (Upsert) statement into the target table\n",
					"- **WHEN MATCHED** ON the surrogate Key (i.e. AllAppealsID), EndDate is set to today -1 day and the IsActive flag is set to 'N'\n",
					"- **WHEN NOT MATCHED** ON the surrogate Key, insert rows\n",
					"## Table odw_harmonised_db.horizon_folder is updated"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- merge into dim table\n",
					"\n",
					"MERGE INTO odw_harmonised_db.horizon_folder AS Target\n",
					"USING horizon_folder_changed_rows_final AS Source\n",
					"\n",
					"ON Source.HorizonFolderID = Target.HorizonFolderID AND Target.IsActive = 'Y'\n",
					"\n",
					"-- For Updates existing rows\n",
					"\n",
					"WHEN MATCHED\n",
					"    THEN \n",
					"    UPDATE SET\n",
					"    Target.ValidTo = to_timestamp(ClosingDate),\n",
					"    Target.IsActive = 'N'\n",
					"\n",
					"-- Insert completely new rows\n",
					"\n",
					"WHEN NOT MATCHED \n",
					"    THEN INSERT (\n",
					"        HorizonFolderID,\n",
					"        ID,\n",
					"        CaseReference,\n",
					"        DisplayNameEnglish,\n",
					"        DisplayNameWelsh,\n",
					"        ParentFolderID,\n",
					"        CaseNodeId,\n",
					"        CaseStage,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive    \n",
					"        )\n",
					"    VALUES (\n",
					"        Source.HorizonFolderID,\n",
					"        Source.ID,\n",
					"        Source.CaseReference,\n",
					"        Source.DisplayNameEnglish,\n",
					"        Source.DisplayNameWelsh,\n",
					"        Source.ParentFolderID,\n",
					"        Source.CaseNodeId,\n",
					"        Source.CaseStage,\n",
					"        Source.SourceSystemID,\n",
					"        Source.IngestionDate,\n",
					"        Source.ValidTo,\n",
					"        Source.RowID,\n",
					"        Source.IsActive)\n",
					"     ;   "
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Fix the IDs\n",
					"- No auto-increment feature is available in delta tables, therefore we need to create new IDs for the inserted rows\n",
					"- This is done by select the target data and using INSERT OVERWRITE to re-insert the data is a new Row Number\n",
					"## Table odw_harmonised_db.horizon_folder is updated"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(\"Writing odw_harmonised_db.horizon_folder\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Insert new horizon_folder\n",
					"\n",
					"INSERT OVERWRITE odw_harmonised_db.horizon_folder\n",
					"\n",
					"SELECT \n",
					"    ROW_NUMBER() OVER (ORDER BY CaseReference NULLS LAST) AS  HorizonFolderID\t,\n",
					"    ID,\n",
					"    CaseReference,\n",
					"    DisplayNameEnglish,\n",
					"    DisplayNameWelsh,\n",
					"    ParentFolderID,\n",
					"    CaseNodeId,\n",
					"    CaseStage,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"FROM odw_harmonised_db.horizon_folder;\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Processing summary cell\n",
					"try:\n",
					"    # Get record counts for logging\n",
					"    final_count_df = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.horizon_folder\")\n",
					"    insert_count = final_count_df.collect()[0]['count']\n",
					"    \n",
					"    end_exec_time = str(datetime.now())\n",
					"    logInfo(f\"Successfully processed {target_table} with {insert_count} total records\")\n",
					"    \n",
					"    # Calculate execution duration\n",
					"    execution_duration = (datetime.strptime(end_exec_time[:19], '%Y-%m-%d %H:%M:%S') - \n",
					"                         datetime.strptime(start_exec_time[:19], '%Y-%m-%d %H:%M:%S')).total_seconds()\n",
					"    \n",
					"    # Add successful result to logger\n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=insert_count,\n",
					"        update_count=update_count,\n",
					"        delete_count=delete_count,\n",
					"        table_result=\"success\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        total_exec_time=execution_duration,\n",
					"        error_message=\"\"\n",
					"    )\n",
					"    \n",
					"except Exception as e:\n",
					"    # Handle errors with proper logging\n",
					"    error_message = f\"Error processing {target_table}: {str(e)}\"\n",
					"    logError(error_message)\n",
					"    \n",
					"    end_exec_time = str(datetime.now())\n",
					"    execution_duration = (datetime.strptime(end_exec_time[:19], '%Y-%m-%d %H:%M:%S') - \n",
					"                         datetime.strptime(start_exec_time[:19], '%Y-%m-%d %H:%M:%S')).total_seconds()\n",
					"    \n",
					"    app_insight_logger.add_table_result(\n",
					"        delta_table_name=target_table,\n",
					"        insert_count=0,\n",
					"        update_count=0,\n",
					"        delete_count=0,\n",
					"        table_result=\"failed\",\n",
					"        start_exec_time=start_exec_time,\n",
					"        end_exec_time=end_exec_time,\n",
					"        total_exec_time=execution_duration,\n",
					"        error_message=error_message\n",
					"    )\n",
					"    raise"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Generate and exit with final logging results\n",
					"mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			}
		]
	}
}