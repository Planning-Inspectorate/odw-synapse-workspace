{
	"name": "py_delta_maintenance_service_bus",
	"properties": {
		"folder": {
			"name": "odw-maintenance"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e7d8f5d2-3a36-4b4e-a534-3bd64a2a9f3a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"Delta maintenance: OPTIMIZE (with dynamic ZORDER where possible) and VACUUM for service bus harmonised tables."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"# Read optional pipeline parameters\n",
					"params = mssparkutils.notebook.getContext().notebookParameters\n",
					"tables_param = params.get('tables', '')\n",
					"retention_hours = int(params.get('vacuum_retain_hours', '168'))\n",
					"\n",
					"default_tables = [\n",
					"    'odw_harmonised_db.appeal_document',\n",
					"    'odw_harmonised_db.appeal_event',\n",
					"    'odw_harmonised_db.appeal_s78',\n",
					"    'odw_harmonised_db.nsip_document',\n",
					"    'odw_harmonised_db.nsip_exam_timetable',\n",
					"    'odw_harmonised_db.nsip_project',\n",
					"    'odw_harmonised_db.nsip_representation',\n",
					"    'odw_harmonised_db.nsip_s51_advice',\n",
					"    'odw_harmonised_db.service_user'\n",
					"]\n",
					"\n",
					"tables = [t.strip() for t in (tables_param.split(',') if tables_param else default_tables) if t.strip()]\n",
					"print(f'Running maintenance for tables: {tables}')\n",
					"\n",
					"def maintain_table(table: str):\n",
					"    try:\n",
					"        cols = spark.table(table).columns\n",
					"        candidates = ['caseReference','documentId','eventId','horizonFolderId','caseId','IngestionDate']\n",
					"        zcols = [c for c in candidates if c in cols][:3]\n",
					"        if zcols:\n",
					"            spark.sql(f\"OPTIMIZE {table} ZORDER BY ({', '.join(zcols)})\")\n",
					"        else:\n",
					"            spark.sql(f\"OPTIMIZE {table}\")\n",
					"        print(f'OPTIMIZE complete for {table} (ZORDER: {zcols})')\n",
					"    except Exception as e:\n",
					"        print(f'OPTIMIZE skipped/failed for {table}: {e}')\n",
					"    try:\n",
					"        spark.sql(f\"VACUUM {table} RETAIN {retention_hours} HOURS\")\n",
					"        print(f'VACUUM complete for {table} (retain {retention_hours} hours)')\n",
					"    except Exception as e:\n",
					"        print(f'VACUUM failed for {table}: {e}')\n",
					"\n",
					"for t in tables:\n",
					"    print(f'--- Maintaining {t} ---')\n",
					"    maintain_table(t)\n",
					"\n",
					"print('Delta maintenance completed.')\n"
				],
				"execution_count": null
			}
		]
	}
}