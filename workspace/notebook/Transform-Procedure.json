{
	"name": "Transform-Procedure",
	"properties": {
		"folder": {
			"name": "Horizon-Migration/Horizon-Transform"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "hbtPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "229d71c7-988c-45ea-b06d-011d8522ba4c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/hbtPool",
				"name": "hbtPool",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/hbtPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"BASE = \"abfss://horizon-migration-poc@pinsstodwdevuks9h80mb.dfs.core.windows.net\"\n",
					"EVENT_DIR = f\"{BASE}/MPESC-EXTRACT/LLAttr/Event_95506\"\n",
					"\n",
					"events_raw = (\n",
					"    spark.read.format(\"csv\")\n",
					"    .option(\"header\", \"true\")\n",
					"    .option(\"multiLine\", \"true\")\n",
					"    .option(\"escape\", \"\\\"\")\n",
					"    .load(EVENT_DIR)  # loads the whole folder of part-*.csv files\n",
					")\n",
					"\n",
					"print(\"Row count:\", events_raw.count())\n",
					"print(\"Columns:\", events_raw.columns)\n",
					"display(events_raw.limit(20))\n",
					""
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"cases_path = f\"{BASE}/config/Cases/Cases.csv\"\n",
					"\n",
					"cases_raw = (\n",
					"    spark.read.format(\"csv\")\n",
					"    .option(\"header\", \"true\")\n",
					"    .option(\"multiLine\", \"true\")\n",
					"    .option(\"escape\", \"\\\"\")\n",
					"    .load(cases_path)\n",
					")\n",
					"\n",
					"print(\"cases_raw rows:\", cases_raw.count())\n",
					"print(\"cases_raw columns:\", cases_raw.columns)\n",
					"display(cases_raw.limit(10))\n",
					"\n",
					"# Handle either naming convention:\n",
					"# - ('Case ID','GUID')  OR  ('legacyCaseId','caseId')\n",
					"cols = [c.strip() for c in cases_raw.columns]\n",
					"cases_raw2 = cases_raw.toDF(*cols)\n",
					"\n",
					"if \"legacyCaseId\" in cases_raw2.columns and \"caseId\" in cases_raw2.columns:\n",
					"    cases_map = cases_raw2.select(\n",
					"        F.col(\"legacyCaseId\").cast(\"string\").alias(\"legacyCaseId\"),\n",
					"        F.col(\"caseId\").cast(\"string\").alias(\"caseId\")\n",
					"    )\n",
					"else:\n",
					"    # your test file earlier was: 'Case ID', 'GUID'\n",
					"    cases_map = cases_raw2.select(\n",
					"        F.col(\"Case ID\").cast(\"string\").alias(\"legacyCaseId\"),\n",
					"        F.col(\"GUID\").cast(\"string\").alias(\"caseId\")\n",
					"    )\n",
					"\n",
					"cases_map = (\n",
					"    cases_map\n",
					"    .filter(F.col(\"legacyCaseId\").isNotNull() & (F.trim(\"legacyCaseId\") != \"\"))\n",
					"    .filter(F.col(\"caseId\").isNotNull() & (F.trim(\"caseId\") != \"\"))\n",
					")\n",
					"\n",
					"print(\"cases_map rows (non-null):\", cases_map.count())\n",
					"display(cases_map.limit(10))\n",
					"\n",
					"# Join rule for MPESC Event_95506:\n",
					"# - Events column 'ID' is the legacy key we expect to match Cases.legacyCaseId (CaseNodeId)\n",
					"events_keyed = (\n",
					"    events_raw\n",
					"    .withColumn(\"legacyCaseId\", F.col(\"ID\").cast(\"string\"))\n",
					"    .withColumn(\"eventRowId\", F.col(\"ID\").cast(\"string\"))  # keep for clarity\n",
					")\n",
					"\n",
					"joined = events_keyed.join(cases_map, on=\"legacyCaseId\", how=\"left\")\n",
					"\n",
					"total = joined.count()\n",
					"mapped = joined.filter(F.col(\"caseId\").isNotNull()).count()\n",
					"unmapped = joined.filter(F.col(\"caseId\").isNull()).count()\n",
					"\n",
					"print(\"events total:\", total)\n",
					"print(\"mapped (have case GUID):\", mapped)\n",
					"print(\"unmapped:\", unmapped)\n",
					"\n",
					"display(\n",
					"    joined.select(\"legacyCaseId\", \"caseId\", F.col(\"25\").alias(\"typeOfEvent\"), F.col(\"26\").alias(\"status\"))\n",
					"          .limit(30)\n",
					")"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"def clean_str(col):\n",
					"    # turns \"\", \"undefined\", \"UNDEFINED\" into null\n",
					"    return (\n",
					"        F.when(col.isNull(), F.lit(None))\n",
					"         .when(F.trim(col) == \"\", F.lit(None))\n",
					"         .when(F.lower(F.trim(col)) == \"undefined\", F.lit(None))\n",
					"         .otherwise(col)\n",
					"    )\n",
					"\n",
					"# keep only rows that can load (have case GUID)\n",
					"mapped_only = joined.filter(F.col(\"caseId\").isNotNull())\n",
					"\n",
					"print(\"mapped_only rows:\", mapped_only.count())\n",
					"display(mapped_only.select(\"legacyCaseId\", \"caseId\", F.col(\"25\").alias(\"typeOfEvent\"), F.col(\"26\").alias(\"status\")).limit(20))\n",
					"\n",
					"events_clean = (\n",
					"    mapped_only\n",
					"    .select(\n",
					"        F.col(\"legacyCaseId\").cast(\"string\"),\n",
					"        F.col(\"caseId\").cast(\"string\"),\n",
					"\n",
					"        F.col(\"ID\").cast(\"string\").alias(\"eventId\"),\n",
					"\n",
					"        clean_str(F.col(\"24\")).alias(\"eventName\"),\n",
					"        clean_str(F.col(\"25\")).alias(\"typeOfEvent_raw\"),\n",
					"        clean_str(F.col(\"26\")).alias(\"chartStatus_raw\"),\n",
					"\n",
					"        clean_str(F.col(\"30\")).alias(\"venueLine1\"),\n",
					"        clean_str(F.col(\"31\")).alias(\"venueLine2\"),\n",
					"        clean_str(F.col(\"32\")).alias(\"venueTown\"),\n",
					"        clean_str(F.col(\"33\")).alias(\"venuePostcode\"),\n",
					"        clean_str(F.col(\"42\")).alias(\"venueCounty\"),\n",
					"\n",
					"        clean_str(F.col(\"35\")).alias(\"startDate_raw\"),\n",
					"        clean_str(F.col(\"36\")).alias(\"startTime_raw\"),\n",
					"        clean_str(F.col(\"37\")).alias(\"endDate_raw\"),\n",
					"\n",
					"        clean_str(F.col(\"43\")).alias(\"estPrepTime_raw\"),\n",
					"        clean_str(F.col(\"44\")).alias(\"estSitTime_raw\"),\n",
					"        clean_str(F.col(\"45\")).alias(\"estRepTime_raw\"),\n",
					"        clean_str(F.col(\"47\")).alias(\"dateEventRequested_raw\"),\n",
					"        clean_str(F.col(\"48\")).alias(\"actualDuration_raw\"),\n",
					"    )\n",
					")\n",
					"\n",
					"display(events_clean.limit(20))\n",
					"print(\"events_clean columns:\", events_clean.columns)"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"def combine_date_time(date_ts_col, time_str_col):\n",
					"    \"\"\"\n",
					"    If time is usable (HH:mm or HH.mm), combine it with the date.\n",
					"    Otherwise return the date timestamp as-is (midnight).\n",
					"    \"\"\"\n",
					"    t = F.lower(F.trim(time_str_col))\n",
					"\n",
					"    # normalize \"10.00\" -> \"10:00\"\n",
					"    t = F.regexp_replace(t, r\"\\.\", \":\")\n",
					"\n",
					"    # keep only proper HH:mm (e.g. 10:00, 11:30, 00:00)\n",
					"    is_hhmm = t.rlike(r\"^\\d{1,2}:\\d{2}$\")\n",
					"\n",
					"    hh = F.lpad(F.split(t, \":\").getItem(0), 2, \"0\")\n",
					"    mm = F.split(t, \":\").getItem(1)\n",
					"\n",
					"    combined = F.to_timestamp(\n",
					"        F.concat(F.date_format(date_ts_col, \"yyyy-MM-dd\"), F.lit(\" \"), hh, F.lit(\":\"), mm, F.lit(\":00\"))\n",
					"    )\n",
					"\n",
					"    return F.when(date_ts_col.isNull(), F.lit(None).cast(\"timestamp\")) \\\n",
					"            .when(is_hhmm, combined) \\\n",
					"            .otherwise(date_ts_col)\n",
					"\n",
					"events_dt = (\n",
					"    events_clean\n",
					"    .withColumn(\"startDate_ts\", F.to_timestamp(\"startDate_raw\"))\n",
					"    .withColumn(\"endDate_ts\", F.to_timestamp(\"endDate_raw\"))\n",
					"    .withColumn(\"dateEventRequested_ts\", F.to_timestamp(\"dateEventRequested_raw\"))\n",
					"    .withColumn(\"confirmedStartDateTime\", combine_date_time(F.col(\"startDate_ts\"), F.col(\"startTime_raw\")))\n",
					")\n",
					"\n",
					"display(\n",
					"    events_dt.select(\n",
					"        \"legacyCaseId\",\n",
					"        \"typeOfEvent_raw\",\n",
					"        \"chartStatus_raw\",\n",
					"        \"startDate_raw\",\n",
					"        \"startTime_raw\",\n",
					"        \"confirmedStartDateTime\",\n",
					"        \"endDate_raw\",\n",
					"        \"endDate_ts\"\n",
					"    ).limit(20)\n",
					")"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					" from pyspark.sql import functions as F, types as T\n",
					"import uuid\n",
					"\n",
					"# deterministic UUID namespace (same one you use elsewhere)\n",
					"NAMESPACE_UUID = uuid.UUID(\"6ba7b810-9dad-11d1-80b4-00c04fd430c8\")\n",
					"\n",
					"@F.udf(returnType=T.StringType())\n",
					"def uuid5_from_text(txt: str):\n",
					"    if txt is None:\n",
					"        return None\n",
					"    s = txt.strip().lower()\n",
					"    if s == \"\":\n",
					"        return None\n",
					"    return str(uuid.uuid5(NAMESPACE_UUID, s))\n",
					"\n",
					"# Build a stable key for the venue address\n",
					"events_with_addrkey = (\n",
					"    events_dt\n",
					"    .withColumn(\n",
					"        \"addrKey\",\n",
					"        F.concat_ws(\n",
					"            \"|\",\n",
					"            F.coalesce(F.lower(F.trim(F.col(\"venueLine1\"))), F.lit(\"\")),\n",
					"            F.coalesce(F.lower(F.trim(F.col(\"venueLine2\"))), F.lit(\"\")),\n",
					"            F.coalesce(F.lower(F.trim(F.col(\"venueTown\"))),  F.lit(\"\")),\n",
					"            F.coalesce(F.lower(F.trim(F.col(\"venueCounty\"))),F.lit(\"\")),\n",
					"            F.coalesce(F.lower(F.trim(F.col(\"venuePostcode\"))), F.lit(\"\")),\n",
					"        )\n",
					"    )\n",
					"    .withColumn(\n",
					"        \"hasVenue\",\n",
					"        (F.col(\"venueLine1\").isNotNull()) | (F.col(\"venueTown\").isNotNull()) | (F.col(\"venuePostcode\").isNotNull())\n",
					"    )\n",
					"    .withColumn(\"venueId\", F.when(F.col(\"hasVenue\"), uuid5_from_text(F.col(\"addrKey\"))))\n",
					")\n",
					"\n",
					"# Address rows to load (deduped)\n",
					"venue_address_ready = (\n",
					"    events_with_addrkey\n",
					"    .filter(F.col(\"venueId\").isNotNull())\n",
					"    .select(\n",
					"        F.col(\"venueId\").alias(\"id\"),\n",
					"        F.col(\"venueLine1\").alias(\"line1\"),\n",
					"        F.col(\"venueLine2\").alias(\"line2\"),\n",
					"        F.col(\"venueTown\").alias(\"townCity\"),\n",
					"        F.col(\"venueCounty\").alias(\"county\"),\n",
					"        F.col(\"venuePostcode\").alias(\"postcode\"),\n",
					"        F.lit(None).cast(\"string\").alias(\"legacyCaseId\")  # optional; keep null for now\n",
					"    )\n",
					"    .dropDuplicates([\"id\"])\n",
					")\n",
					"\n",
					"print(\"Unique venue addresses:\", venue_address_ready.count())\n",
					"display(venue_address_ready.limit(20))\n",
					"\n",
					"# Events with venueId attached (used later for Procedure hearingVenueId/inquiryVenueId)\n",
					"events_with_venue = events_with_addrkey.drop(\"addrKey\")\n",
					"\n",
					"display(\n",
					"    events_with_venue.select(\n",
					"        \"legacyCaseId\", \"caseId\", \"typeOfEvent_raw\", \"venueId\", \"venueLine1\", \"venueTown\", \"venuePostcode\"\n",
					"    ).limit(20)\n",
					")"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"type_lc = F.lower(F.col(\"typeOfEvent_raw\"))\n",
					"status_lc = F.lower(F.col(\"chartStatus_raw\"))\n",
					"\n",
					"events_mapped = (\n",
					"    events_with_venue\n",
					"    .withColumn(\n",
					"        \"procedureTypeId\",\n",
					"        F.when(type_lc.contains(\"hearing\"), F.lit(\"hearing\"))\n",
					"         .when(type_lc.contains(\"inquiry\"), F.lit(\"inquiry\"))\n",
					"         .when(type_lc.contains(\"site visit\"), F.lit(\"site-visit\"))\n",
					"         .when(type_lc.contains(\"written\"), F.lit(\"written-reps\"))\n",
					"         .when(type_lc.contains(\"in-house\"), F.lit(\"admin-in-house\"))\n",
					"         .otherwise(F.lit(None))\n",
					"    )\n",
					"    .withColumn(\n",
					"        \"procedureStatusId\",\n",
					"        F.when(status_lc.contains(\"confirm\"), F.lit(\"active\"))\n",
					"         .when(status_lc.contains(\"new\"), F.lit(\"active\"))  # New / Re-scheduled\n",
					"         .when(status_lc.contains(\"resched\"), F.lit(\"active\"))\n",
					"         .when(status_lc.contains(\"postpon\"), F.lit(\"active\"))  # or 'changed-procedure-type' depending on policy\n",
					"         .when(status_lc.contains(\"change of procedure\") | status_lc.contains(\"change of procedure type\"), F.lit(\"changed-procedure-type\"))\n",
					"         .when(status_lc.contains(\"cancel\"), F.lit(\"cancelled\"))\n",
					"         .when(status_lc.contains(\"withdraw\"), F.lit(\"withdrawn\"))\n",
					"         .when(status_lc.contains(\"complete\"), F.lit(\"completed\"))\n",
					"         .when(status_lc.contains(\"fallen\"), F.lit(\"fallen-away\"))\n",
					"         .otherwise(F.lit(None))\n",
					"    )\n",
					")\n",
					"\n",
					"display(\n",
					"    events_mapped.select(\n",
					"        \"legacyCaseId\", \"caseId\",\n",
					"        \"typeOfEvent_raw\", \"procedureTypeId\",\n",
					"        \"chartStatus_raw\", \"procedureStatusId\",\n",
					"        \"confirmedStartDateTime\", \"endDate_ts\",\n",
					"        \"venueId\"\n",
					"    ).limit(50)\n",
					")"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F, types as T\n",
					"from pyspark.sql.window import Window\n",
					"import uuid\n",
					"\n",
					"# deterministic UUID namespace\n",
					"NAMESPACE_UUID = uuid.UUID(\"6ba7b810-9dad-11d1-80b4-00c04fd430c8\")\n",
					"\n",
					"@F.udf(returnType=T.StringType())\n",
					"def uuid5_from_text(txt: str):\n",
					"    if txt is None:\n",
					"        return None\n",
					"    s = txt.strip().lower()\n",
					"    if s == \"\":\n",
					"        return None\n",
					"    return str(uuid.uuid5(NAMESPACE_UUID, s))\n",
					"\n",
					"@F.udf(returnType=T.StringType())\n",
					"def step_name(n: int):\n",
					"    names = {\n",
					"        1: \"ProcedureOne\", 2: \"ProcedureTwo\", 3: \"ProcedureThree\", 4: \"ProcedureFour\", 5: \"ProcedureFive\",\n",
					"        6: \"ProcedureSix\", 7: \"ProcedureSeven\", 8: \"ProcedureEight\", 9: \"ProcedureNine\", 10: \"ProcedureTen\",\n",
					"        11: \"ProcedureEleven\", 12: \"ProcedureTwelve\", 13: \"ProcedureThirteen\", 14: \"ProcedureFourteen\",\n",
					"        15: \"ProcedureFifteen\", 16: \"ProcedureSixteen\", 17: \"ProcedureSeventeen\", 18: \"ProcedureEighteen\",\n",
					"        19: \"ProcedureNineteen\", 20: \"ProcedureTwenty\",\n",
					"    }\n",
					"    if n is None:\n",
					"        return None\n",
					"    return names.get(int(n), f\"Procedure{int(n)}\")\n",
					"\n",
					"# order within case: confirmedStartDateTime (nulls last), then eventId\n",
					"order_ts = F.coalesce(F.col(\"confirmedStartDateTime\"), F.lit(\"9999-12-31 00:00:00\").cast(\"timestamp\"))\n",
					"w = Window.partitionBy(\"caseId\").orderBy(order_ts.asc(), F.col(\"eventId\").asc())\n",
					"\n",
					"events_with_steps = (\n",
					"    events_mapped\n",
					"    .withColumn(\"stepNum\", F.row_number().over(w))\n",
					"    .withColumn(\"step\", step_name(F.col(\"stepNum\")))\n",
					")\n",
					"\n",
					"# deterministic procedure id from caseId|step\n",
					"procedure_seed = F.concat_ws(\"|\", F.col(\"caseId\"), F.col(\"step\"))\n",
					"\n",
					"procedure_ready = (\n",
					"    events_with_steps\n",
					"    .withColumn(\"id\", uuid5_from_text(procedure_seed))\n",
					"\n",
					"    # venue foreign keys by type\n",
					"    .withColumn(\"hearingVenueId\", F.when(F.col(\"procedureTypeId\") == \"hearing\", F.col(\"venueId\")))\n",
					"    .withColumn(\"inquiryVenueId\", F.when(F.col(\"procedureTypeId\") == \"inquiry\", F.col(\"venueId\")))\n",
					"    .withColumn(\"conferenceVenueId\", F.lit(None).cast(\"string\"))\n",
					"\n",
					"    # key dates by type\n",
					"    .withColumn(\"siteVisitDate\", F.when(F.col(\"procedureTypeId\") == \"site-visit\", F.col(\"confirmedStartDateTime\")))\n",
					"    .withColumn(\"confirmedHearingDate\", F.when(F.col(\"procedureTypeId\") == \"hearing\", F.col(\"confirmedStartDateTime\")))\n",
					"    .withColumn(\"confirmedInquiryDate\", F.when(F.col(\"procedureTypeId\") == \"inquiry\", F.col(\"confirmedStartDateTime\")))\n",
					"\n",
					"    # best-fit closed dates\n",
					"    .withColumn(\"hearingClosedDate\", F.when(F.col(\"procedureTypeId\") == \"hearing\", F.col(\"endDate_ts\")))\n",
					"    .withColumn(\"inquiryClosedDate\", F.when(F.col(\"procedureTypeId\") == \"inquiry\", F.col(\"endDate_ts\")))\n",
					"\n",
					"    # select columns in the target schema order (everything else = null for now)\n",
					"    .select(\n",
					"        \"id\",\n",
					"        \"procedureTypeId\",\n",
					"        \"procedureStatusId\",\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"caseOfficerVerificationDate\"),\n",
					"\n",
					"        \"siteVisitDate\",\n",
					"        F.lit(None).cast(\"string\").alias(\"siteVisitTypeId\"),\n",
					"\n",
					"        F.lit(None).cast(\"string\").alias(\"adminProcedureType\"),\n",
					"\n",
					"        # hearing timeline\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"hearingTargetDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"earliestHearingDate\"),\n",
					"        \"confirmedHearingDate\",\n",
					"        \"hearingClosedDate\",\n",
					"\n",
					"        # hearing notifications\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"hearingDateNotificationDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"hearingVenueNotificationDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"partiesNotifiedOfHearingDate\"),\n",
					"\n",
					"        # hearing data\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"lengthOfHearingEvent\"),\n",
					"        F.lit(None).cast(\"boolean\").alias(\"hearingInTarget\"),\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"hearingPreparationTimeDays\"),\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"hearingTravelTimeDays\"),\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"hearingSittingTimeDays\"),\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"hearingReportingTimeDays\"),\n",
					"\n",
					"        # hearing format/venue\n",
					"        F.lit(None).cast(\"string\").alias(\"hearingFormatId\"),\n",
					"        \"hearingVenueId\",\n",
					"\n",
					"        # inquiry timeline\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"inquiryTargetDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"earliestInquiryDate\"),\n",
					"        \"confirmedInquiryDate\",\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"inquiryFinishedDate\"),\n",
					"        \"inquiryClosedDate\",\n",
					"\n",
					"        # inquiry notifications\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"inquiryDateNotificationDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"inquiryVenueNotificationDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"partiesNotifiedOfInquiryDate\"),\n",
					"\n",
					"        # inquiry data\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"lengthOfInquiryEvent\"),\n",
					"        F.lit(None).cast(\"boolean\").alias(\"inquiryInTarget\"),\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"inquiryPreparationTimeDays\"),\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"inquiryTravelTimeDays\"),\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"inquirySittingTimeDays\"),\n",
					"        F.lit(None).cast(\"decimal(18,4)\").alias(\"inquiryReportingTimeDays\"),\n",
					"\n",
					"        # inquiry format/venue\n",
					"        F.lit(None).cast(\"string\").alias(\"inquiryFormatId\"),\n",
					"        \"inquiryVenueId\",\n",
					"\n",
					"        # conference / pre-inquiry (not mapped from this extract)\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"conferenceDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"conferenceNoteSentDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"preInquiryMeetingDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"preInquiryNoteSentDate\"),\n",
					"        F.lit(None).cast(\"string\").alias(\"conferenceFormatId\"),\n",
					"        \"conferenceVenueId\",\n",
					"        F.lit(None).cast(\"string\").alias(\"preInquiryMeetingFormatId\"),\n",
					"\n",
					"        F.lit(None).cast(\"string\").alias(\"inquiryOrConferenceId\"),\n",
					"\n",
					"        # docs dates (not mapped)\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"proofsOfEvidenceReceivedDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"statementsOfCaseReceivedDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"inHouseDate\"),\n",
					"        F.lit(None).cast(\"timestamp\").alias(\"offerForWrittenRepresentationsDate\"),\n",
					"\n",
					"        # link + step\n",
					"        \"caseId\",\n",
					"        \"step\"\n",
					"    )\n",
					")\n",
					"\n",
					"print(\"procedure_ready rows:\", procedure_ready.count())\n",
					"display(procedure_ready.limit(50))"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"# Check which rows are missing a procedureTypeId\n",
					"print(\"Missing procedureTypeId:\", procedure_ready.filter(F.col(\"procedureTypeId\").isNull()).count())\n",
					"display(procedure_ready.filter(F.col(\"procedureTypeId\").isNull()).select(\"id\",\"caseId\",\"step\").limit(20))\n",
					"\n",
					"# Optional: drop rows with no procedureTypeId (recommended)\n",
					"procedure_ready_final = procedure_ready.filter(F.col(\"procedureTypeId\").isNotNull())\n",
					"\n",
					"print(\"procedure_ready_final rows:\", procedure_ready_final.count())\n",
					"display(procedure_ready_final.limit(20))"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					"out_base = f\"{BASE}/MPESC-TRANSFORM/Procedure\"\n",
					"out_procedure_path = f\"{out_base}/Procedure.csv\"\n",
					"out_address_path   = f\"{out_base}/ProcedureVenues_Address.csv\"\n",
					"\n",
					"# Write Address venues\n",
					"venue_address_ready.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(out_address_path)\n",
					"\n",
					"# Write Procedure\n",
					"procedure_ready_final.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(out_procedure_path)\n",
					"\n",
					"print(\"Wrote Address to:\", out_address_path)\n",
					"print(\"Wrote Procedure to:\", out_procedure_path)"
				],
				"execution_count": 26
			}
		]
	}
}