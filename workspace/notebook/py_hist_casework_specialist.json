{
	"name": "py_hist_casework_specialist",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b56943b7-557c-4614-b184-61c1139feac6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_logging_decorator"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"spark_table_final = \"odw_harmonised_db.hist_green_specialist_case\"\n",
					"source_table = \"odw_standardised_db.green_specialist_case\"\n",
					"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message = ''\n",
					"table_exists = False\n",
					"new_count = 0\n",
					"changed_count = 0\n",
					"deleted_count = 0"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Setting legacy time parser policy\")\n",
					"        spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"        \n",
					"        table_exists = spark.catalog.tableExists(spark_table_final)\n",
					"        logInfo(f\"Target table exists: {table_exists}\")\n",
					"        \n",
					"        logInfo(f\"Reading source data from {source_table}\")\n",
					"        \n",
					"        # Create staging view with deduplication and transformations\n",
					"        spark.sql(f\"\"\"\n",
					"        CREATE OR REPLACE TEMP VIEW stg_casework_specialist AS\n",
					"        SELECT\n",
					"            greenCaseType, greenCaseId, FullReference as caseReference,\n",
					"            horizonId, linkedGreenCaseId, caseOfficerName, caseOfficerEmail,\n",
					"            appealType, procedure, processingState,\n",
					"            LPACode as pinsLpaCode, LPAName as pinsLpaName,\n",
					"            appellantName, agentName, siteAddressLine1, siteAddressLine2,\n",
					"            siteTownCity, sitePostcode, otherPartyName,\n",
					"            to_date(receiptDate, 'dd/MM/yyyy') AS receiptDate,\n",
					"            to_date(validDate, 'dd/MM/yyyy') AS validDate,\n",
					"            to_date(startDate, 'dd/MM/yyyy') AS startDate,\n",
					"            to_date(QuDate, 'dd/MM/yyyy') AS lpaQuestionnaireDue,\n",
					"            to_date(QuRecDate, 'dd/MM/yyyy') AS lpaQuestionnaireReceived,\n",
					"            to_date(`6Weeks`, 'dd/MM/yyyy') AS week6Date,\n",
					"            to_date(`8Weeks`, 'dd/MM/yyyy') AS week8Date,\n",
					"            to_date(`9Weeks`, 'dd/MM/yyyy') AS week9Date,\n",
					"            to_date(eventDate, 'dd/MM/yyyy') AS eventDate,\n",
					"            eventTime, inspectorName, inspectorEmail, decision,\n",
					"            to_date(decisionDate, 'dd/MM/yyyy') AS decisionDate,\n",
					"            withdrawnOrTurnedAway,\n",
					"            to_date(DateWithdrawnorTurnedAway, 'dd/MM/yyyy') AS withdrawnOrTurnedAwayDate,\n",
					"            comments, 'N' as Migrated\n",
					"        FROM (\n",
					"            SELECT *, ROW_NUMBER() OVER (\n",
					"                PARTITION BY greenCaseId \n",
					"                ORDER BY \n",
					"                    to_date(receiptDate, 'dd/MM/yyyy') DESC NULLS LAST,\n",
					"                    to_date(startDate, 'dd/MM/yyyy') DESC NULLS LAST\n",
					"            ) as rn\n",
					"            FROM {source_table}\n",
					"        )\n",
					"        WHERE rn = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        source_count = spark.sql(\"SELECT COUNT(*) as cnt FROM stg_casework_specialist\").collect()[0]['cnt']\n",
					"        logInfo(f\"Source data: {source_count} records (deduplicated)\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in setup/source prep: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message and not table_exists:\n",
					"    try:\n",
					"        logInfo(\"Performing initial load\")\n",
					"        \n",
					"        spark.sql(f\"\"\"\n",
					"        CREATE TABLE {spark_table_final} USING DELTA AS\n",
					"        SELECT\n",
					"            ROW_NUMBER() OVER (ORDER BY greenCaseId) AS casework_specialist_id,\n",
					"            greenCaseType, greenCaseId, caseReference, horizonId, linkedGreenCaseId,\n",
					"            caseOfficerName, caseOfficerEmail, appealType, procedure, processingState,\n",
					"            pinsLpaCode, pinsLpaName, appellantName, agentName,\n",
					"            siteAddressLine1, siteAddressLine2, siteTownCity, sitePostcode, otherPartyName,\n",
					"            receiptDate, validDate, startDate, lpaQuestionnaireDue, lpaQuestionnaireReceived,\n",
					"            week6Date, week8Date, week9Date, eventDate, eventTime,\n",
					"            inspectorName, inspectorEmail, decision, decisionDate,\n",
					"            withdrawnOrTurnedAway, withdrawnOrTurnedAwayDate, comments, Migrated,\n",
					"            CURRENT_TIMESTAMP() AS IngestionDate,\n",
					"            1 AS is_current,\n",
					"            CURRENT_TIMESTAMP() AS record_start_date,\n",
					"            CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS record_end_date,\n",
					"            '' AS RowID\n",
					"        FROM stg_casework_specialist\n",
					"        \"\"\")\n",
					"        \n",
					"        insert_count = source_count\n",
					"        logInfo(f\"Initial load: {insert_count} records\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in initial load: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message and table_exists:\n",
					"    try:\n",
					"        logInfo(\"Analyzing changes for SCD Type 2\")\n",
					"        \n",
					"        #  hash generation\n",
					"        hash_cols = \"\"\"md5(concat_ws('|',\n",
					"            COALESCE(greenCaseType, 'N'), COALESCE(CAST(greenCaseId AS STRING), 'N'),\n",
					"            COALESCE(caseReference, 'N'), COALESCE(processingState, 'N'),\n",
					"            COALESCE(caseOfficerName, 'N'), COALESCE(appellantName, 'N'),\n",
					"            COALESCE(decision, 'N'), COALESCE(CAST(decisionDate AS STRING), 'N')\n",
					"        ))\"\"\"\n",
					"        \n",
					"        spark.sql(f\"\"\"\n",
					"        CREATE OR REPLACE TEMP VIEW stg_with_hash AS\n",
					"        SELECT *, {hash_cols} as data_hash\n",
					"        FROM stg_casework_specialist\n",
					"        \"\"\")\n",
					"        \n",
					"        spark.sql(f\"\"\"\n",
					"        CREATE OR REPLACE TEMP VIEW current_active AS\n",
					"        SELECT casework_specialist_id, greenCaseId, {hash_cols} as data_hash\n",
					"        FROM {spark_table_final}\n",
					"        WHERE is_current = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMP VIEW change_analysis AS\n",
					"        SELECT \n",
					"            src.greenCaseId, tgt.casework_specialist_id,\n",
					"            CASE \n",
					"                WHEN tgt.greenCaseId IS NULL THEN 'NEW'\n",
					"                WHEN src.data_hash != tgt.data_hash THEN 'CHANGED'\n",
					"                ELSE 'UNCHANGED'\n",
					"            END as change_type\n",
					"        FROM stg_with_hash src\n",
					"        LEFT JOIN current_active tgt ON src.greenCaseId = tgt.greenCaseId\n",
					"        \"\"\")\n",
					"        \n",
					"        counts = spark.sql(\"\"\"\n",
					"        SELECT change_type, COUNT(*) as cnt FROM change_analysis GROUP BY change_type\n",
					"        \"\"\").collect()\n",
					"        \n",
					"        for row in counts:\n",
					"            if row['change_type'] == 'NEW': new_count = row['cnt']\n",
					"            elif row['change_type'] == 'CHANGED': changed_count = row['cnt']\n",
					"        \n",
					"        deleted_count = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as cnt FROM current_active tgt\n",
					"        LEFT JOIN stg_with_hash src ON tgt.greenCaseId = src.greenCaseId\n",
					"        WHERE src.greenCaseId IS NULL\n",
					"        \"\"\").collect()[0]['cnt']\n",
					"        \n",
					"        logInfo(f\"Changes - New: {new_count}, Changed: {changed_count}, Deleted: {deleted_count}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error analyzing changes: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message and table_exists and (changed_count > 0 or deleted_count > 0):\n",
					"    try:\n",
					"        records_to_close = changed_count + deleted_count\n",
					"        logInfo(f\"Closing {records_to_close} records\")\n",
					"        \n",
					"        deltaTable = DeltaTable.forName(spark, spark_table_final)\n",
					"        \n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMP VIEW records_to_close AS\n",
					"        SELECT DISTINCT greenCaseId FROM change_analysis WHERE change_type = 'CHANGED'\n",
					"        UNION\n",
					"        SELECT DISTINCT tgt.greenCaseId\n",
					"        FROM current_active tgt\n",
					"        LEFT JOIN stg_with_hash src ON tgt.greenCaseId = src.greenCaseId\n",
					"        WHERE src.greenCaseId IS NULL\n",
					"        \"\"\")\n",
					"        \n",
					"        deltaTable.alias(\"target\").merge(\n",
					"            spark.table(\"records_to_close\").alias(\"source\"),\n",
					"            \"target.greenCaseId = source.greenCaseId AND target.is_current = 1\"\n",
					"        ).whenMatchedUpdate(\n",
					"            set = {\n",
					"                \"record_end_date\": \"current_timestamp()\",\n",
					"                \"is_current\": \"0\"\n",
					"            }\n",
					"        ).execute()\n",
					"        \n",
					"        update_count = records_to_close\n",
					"        delete_count = deleted_count\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error closing records: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message and table_exists and (changed_count > 0 or new_count > 0):\n",
					"    try:\n",
					"        if changed_count > 0:\n",
					"            logInfo(f\"Inserting {changed_count} changed versions\")\n",
					"            spark.sql(f\"\"\"\n",
					"            INSERT INTO {spark_table_final}\n",
					"            SELECT ca.casework_specialist_id, stg.*, \n",
					"                CURRENT_TIMESTAMP() AS IngestionDate, 1 AS is_current,\n",
					"                CURRENT_TIMESTAMP() AS record_start_date,\n",
					"                CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS record_end_date,\n",
					"                '' AS RowID\n",
					"            FROM stg_casework_specialist stg\n",
					"            INNER JOIN change_analysis ca ON stg.greenCaseId = ca.greenCaseId \n",
					"            WHERE ca.change_type = 'CHANGED'\n",
					"            \"\"\")\n",
					"        \n",
					"        if new_count > 0:\n",
					"            logInfo(f\"Inserting {new_count} new records\")\n",
					"            max_id = spark.sql(f\"SELECT COALESCE(MAX(casework_specialist_id), 0) as max_id FROM {spark_table_final}\").collect()[0]['max_id']\n",
					"            \n",
					"            spark.sql(f\"\"\"\n",
					"            INSERT INTO {spark_table_final}\n",
					"            SELECT ROW_NUMBER() OVER (ORDER BY greenCaseId) + {max_id} AS casework_specialist_id,\n",
					"                stg.*, CURRENT_TIMESTAMP() AS IngestionDate, 1 AS is_current,\n",
					"                CURRENT_TIMESTAMP() AS record_start_date,\n",
					"                CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS record_end_date,\n",
					"                '' AS RowID\n",
					"            FROM stg_casework_specialist stg\n",
					"            INNER JOIN change_analysis ca ON stg.greenCaseId = ca.greenCaseId \n",
					"            WHERE ca.change_type = 'NEW'\n",
					"            \"\"\")\n",
					"        \n",
					"        insert_count = new_count + changed_count\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error inserting records: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Updating RowID\")\n",
					"        spark.sql(f\"\"\"\n",
					"        UPDATE {spark_table_final}\n",
					"        SET RowID = md5(concat_ws('|',\n",
					"            IFNULL(CAST(casework_specialist_id AS STRING), '.'),\n",
					"            IFNULL(greenCaseType, '.'), IFNULL(CAST(greenCaseId AS STRING), '.'),\n",
					"            IFNULL(CAST(is_current AS STRING), '.'),\n",
					"            IFNULL(CAST(record_start_date AS STRING), '.')\n",
					"        ))\n",
					"        WHERE RowID = ''\n",
					"        \"\"\")\n",
					"        logInfo(\"RowID updated\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error updating RowID: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"if not error_message:\n",
					"    try:\n",
					"        logInfo(\"Data quality checks\")\n",
					"        \n",
					"        null_rowid = spark.sql(f\"SELECT COUNT(*) as cnt FROM {spark_table_final} WHERE RowID = ''\").collect()[0]['cnt']\n",
					"        if null_rowid > 0: logError(f\"{null_rowid} rows have empty RowID\")\n",
					"        \n",
					"        duplicates = spark.sql(f\"\"\"\n",
					"        SELECT COUNT(*) as cnt FROM (\n",
					"            SELECT greenCaseId FROM {spark_table_final} \n",
					"            WHERE is_current = 1 GROUP BY greenCaseId HAVING COUNT(*) > 1\n",
					"        )\n",
					"        \"\"\").collect()[0]['cnt']\n",
					"        \n",
					"        if duplicates > 0:\n",
					"            logError(f\"{duplicates} duplicate active records\")\n",
					"        else:\n",
					"            logInfo(\"No duplicate active records\")\n",
					"        \n",
					"        total = spark.sql(f\"SELECT COUNT(*) as cnt FROM {spark_table_final}\").collect()[0]['cnt']\n",
					"        active = spark.sql(f\"SELECT COUNT(*) as cnt FROM {spark_table_final} WHERE is_current = 1\").collect()[0]['cnt']\n",
					"        \n",
					"        logInfo(f\"Total: {total}, Active: {active}, New: {insert_count}, Updated: {update_count}, Deleted: {delete_count}\")\n",
					"        end_exec_time = datetime.now()\n",
					"        \n",
					"    except Exception as e:\n",
					"        error_message = f\"Error in quality checks: {str(e)[:800]}\"\n",
					"        end_exec_time = datetime.now()"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\" if not error_message else \"Failed\"\n",
					"status_message = (\n",
					"    f\"Successfully processed SCD Type 2 for {spark_table_final}\"\n",
					"    if not error_message\n",
					"    else f\"Failed to process SCD Type 2 for {spark_table_final}\"\n",
					")\n",
					"status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"log_telemetry_and_exit(\n",
					"    stage, start_exec_time, end_exec_time, error_message,\n",
					"    spark_table_final, insert_count, update_count, delete_count,\n",
					"    PipelineName, PipelineRunID, PipelineTriggerID, PipelineTriggerName,\n",
					"    PipelineTriggerType, PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID, activity_type,\n",
					"    duration_seconds, status_message, status_code\n",
					")"
				],
				"execution_count": 12
			}
		]
	}
}