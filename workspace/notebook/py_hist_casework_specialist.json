{
	"name": "py_hist_casework_specialist",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "17572e91-e619-443b-ad5f-5be83e46c18a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.window import Window\n",
					"from delta.tables import DeltaTable\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"new_records\": 0,\n",
					"    \"updated_records\": 0,\n",
					"    \"deleted_records\": 0,\n",
					"    \"unchanged_records\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Set time parser policy\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # ==========================================\n",
					"    # STEP 1: Prepare source data with staging\n",
					"    # ==========================================\n",
					"    logInfo(\"Reading and preparing source data from odw_standardised_db.casework_specialist\")\n",
					"    \n",
					"    # Create staging view with transformed data\n",
					"    spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMP VIEW stg_casework_specialist AS\n",
					"        SELECT\n",
					"            greenCaseType,\n",
					"            greenCaseId,\n",
					"            FullReference as caseReference,\n",
					"            horizonId,\n",
					"            linkedGreenCaseId,\n",
					"            caseOfficerName,\n",
					"            caseOfficerEmail,\n",
					"            appealType,\n",
					"            procedure,\n",
					"            processingState,\n",
					"            LPACode as pinsLpaCode,\n",
					"            LPAName as pinsLpaName,\n",
					"            appellantName,\n",
					"            agentName,\n",
					"            siteAddressLine1,\n",
					"            siteAddressLine2,\n",
					"            siteTownCity,\n",
					"            sitePostcode,\n",
					"            otherPartyName,\n",
					"            to_date(receiptDate, 'dd/MM/yyyy') AS receiptDate,\n",
					"            to_date(validDate, 'dd/MM/yyyy') AS validDate,\n",
					"            to_date(startDate, 'dd/MM/yyyy') AS startDate,\n",
					"            to_date(QuDate, 'dd/MM/yyyy') AS lpaQuestionnaireDue,\n",
					"            to_date(QuRecDate, 'dd/MM/yyyy') AS lpaQuestionnaireReceived,\n",
					"            to_date(`6Weeks`, 'dd/MM/yyyy') AS week6Date,\n",
					"            to_date(`8Weeks`, 'dd/MM/yyyy') AS week8Date,\n",
					"            to_date(`9Weeks`, 'dd/MM/yyyy') AS week9Date,\n",
					"            to_date(eventDate, 'dd/MM/yyyy') AS eventDate,\n",
					"            eventTime,\n",
					"            inspectorName,\n",
					"            inspectorEmail,\n",
					"            decision,\n",
					"            to_date(decisionDate, 'dd/MM/yyyy') AS decisionDate,\n",
					"            withdrawnOrTurnedAway,\n",
					"            to_date(DateWithdrawnorTurnedAway, 'dd/MM/yyyy') AS withdrawnOrTurnedAwayDate,\n",
					"            comments,\n",
					"            'N' as Migrated,\n",
					"            -- Generate data hash for change detection\n",
					"            md5(concat_ws('|',\n",
					"                COALESCE(greenCaseType, 'NULL'),\n",
					"                COALESCE(CAST(greenCaseId AS STRING), 'NULL'),\n",
					"                COALESCE(FullReference, 'NULL'),\n",
					"                COALESCE(horizonId, 'NULL'),\n",
					"                COALESCE(CAST(linkedGreenCaseId AS STRING), 'NULL'),\n",
					"                COALESCE(caseOfficerName, 'NULL'),\n",
					"                COALESCE(caseOfficerEmail, 'NULL'),\n",
					"                COALESCE(appealType, 'NULL'),\n",
					"                COALESCE(procedure, 'NULL'),\n",
					"                COALESCE(processingState, 'NULL'),\n",
					"                COALESCE(LPACode, 'NULL'),\n",
					"                COALESCE(LPAName, 'NULL'),\n",
					"                COALESCE(appellantName, 'NULL'),\n",
					"                COALESCE(agentName, 'NULL'),\n",
					"                COALESCE(siteAddressLine1, 'NULL'),\n",
					"                COALESCE(siteAddressLine2, 'NULL'),\n",
					"                COALESCE(siteTownCity, 'NULL'),\n",
					"                COALESCE(sitePostcode, 'NULL'),\n",
					"                COALESCE(otherPartyName, 'NULL'),\n",
					"                COALESCE(CAST(to_date(receiptDate, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(CAST(to_date(validDate, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(CAST(to_date(startDate, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(CAST(to_date(QuDate, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(CAST(to_date(QuRecDate, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(CAST(to_date(`6Weeks`, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(CAST(to_date(`8Weeks`, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(CAST(to_date(`9Weeks`, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(CAST(to_date(eventDate, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(eventTime, 'NULL'),\n",
					"                COALESCE(inspectorName, 'NULL'),\n",
					"                COALESCE(inspectorEmail, 'NULL'),\n",
					"                COALESCE(decision, 'NULL'),\n",
					"                COALESCE(CAST(to_date(decisionDate, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(withdrawnOrTurnedAway, 'NULL'),\n",
					"                COALESCE(CAST(to_date(DateWithdrawnorTurnedAway, 'dd/MM/yyyy') AS STRING), 'NULL'),\n",
					"                COALESCE(comments, 'NULL')\n",
					"            )) as data_hash\n",
					"        FROM odw_standardised_db.casework_specialist\n",
					"    \"\"\")\n",
					"    \n",
					"    source_count = spark.sql(\"SELECT COUNT(*) as cnt FROM stg_casework_specialist\").collect()[0]['cnt']\n",
					"    logInfo(f\"Source data loaded: {source_count} records\")\n",
					"    \n",
					"    # ==========================================\n",
					"    # STEP 2: Check if target table exists\n",
					"    # ==========================================\n",
					"    target_table_path = \"odw_harmonised_db.hist_casework_specialist\"\n",
					"    table_exists = spark.catalog.tableExists(target_table_path)\n",
					"    \n",
					"    if not table_exists:\n",
					"        # ==========================================\n",
					"        # INITIAL LOAD - Table doesn't exist\n",
					"        # ==========================================\n",
					"        logInfo(\"Target table does not exist - performing initial load\")\n",
					"        \n",
					"        spark.sql(f\"\"\"\n",
					"            CREATE TABLE IF NOT EXISTS {target_table_path}\n",
					"            USING DELTA\n",
					"            AS\n",
					"            SELECT\n",
					"                ROW_NUMBER() OVER (ORDER BY greenCaseId, receiptDate) AS casework_specialist_id,\n",
					"                greenCaseType,\n",
					"                greenCaseId,\n",
					"                caseReference,\n",
					"                horizonId,\n",
					"                linkedGreenCaseId,\n",
					"                caseOfficerName,\n",
					"                caseOfficerEmail,\n",
					"                appealType,\n",
					"                procedure,\n",
					"                processingState,\n",
					"                pinsLpaCode,\n",
					"                pinsLpaName,\n",
					"                appellantName,\n",
					"                agentName,\n",
					"                siteAddressLine1,\n",
					"                siteAddressLine2,\n",
					"                siteTownCity,\n",
					"                sitePostcode,\n",
					"                otherPartyName,\n",
					"                receiptDate,\n",
					"                validDate,\n",
					"                startDate,\n",
					"                lpaQuestionnaireDue,\n",
					"                lpaQuestionnaireReceived,\n",
					"                week6Date,\n",
					"                week8Date,\n",
					"                week9Date,\n",
					"                eventDate,\n",
					"                eventTime,\n",
					"                inspectorName,\n",
					"                inspectorEmail,\n",
					"                decision,\n",
					"                decisionDate,\n",
					"                withdrawnOrTurnedAway,\n",
					"                withdrawnOrTurnedAwayDate,\n",
					"                comments,\n",
					"                Migrated,\n",
					"                CURRENT_TIMESTAMP() AS IngestionDate,\n",
					"                CURRENT_TIMESTAMP() AS record_start_date,\n",
					"                CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS record_end_date,\n",
					"                1 AS is_current,\n",
					"                md5(concat_ws('|',\n",
					"                    COALESCE(greenCaseType, 'NULL'), COALESCE(CAST(greenCaseId AS STRING), 'NULL'),\n",
					"                    COALESCE(caseReference, 'NULL'), COALESCE(horizonId, 'NULL'),\n",
					"                    COALESCE(CAST(linkedGreenCaseId AS STRING), 'NULL'), COALESCE(caseOfficerName, 'NULL'),\n",
					"                    COALESCE(caseOfficerEmail, 'NULL'), COALESCE(appealType, 'NULL'),\n",
					"                    COALESCE(procedure, 'NULL'), COALESCE(processingState, 'NULL'),\n",
					"                    COALESCE(pinsLpaCode, 'NULL'), COALESCE(pinsLpaName, 'NULL'),\n",
					"                    COALESCE(appellantName, 'NULL'), COALESCE(agentName, 'NULL'),\n",
					"                    COALESCE(siteAddressLine1, 'NULL'), COALESCE(siteAddressLine2, 'NULL'),\n",
					"                    COALESCE(siteTownCity, 'NULL'), COALESCE(sitePostcode, 'NULL'),\n",
					"                    COALESCE(otherPartyName, 'NULL'), COALESCE(CAST(receiptDate AS STRING), 'NULL'),\n",
					"                    COALESCE(CAST(validDate AS STRING), 'NULL'), COALESCE(CAST(startDate AS STRING), 'NULL'),\n",
					"                    COALESCE(CAST(lpaQuestionnaireDue AS STRING), 'NULL'), COALESCE(CAST(lpaQuestionnaireReceived AS STRING), 'NULL'),\n",
					"                    COALESCE(CAST(week6Date AS STRING), 'NULL'), COALESCE(CAST(week8Date AS STRING), 'NULL'),\n",
					"                    COALESCE(CAST(week9Date AS STRING), 'NULL'), COALESCE(CAST(eventDate AS STRING), 'NULL'),\n",
					"                    COALESCE(eventTime, 'NULL'), COALESCE(inspectorName, 'NULL'),\n",
					"                    COALESCE(inspectorEmail, 'NULL'), COALESCE(decision, 'NULL'),\n",
					"                    COALESCE(CAST(decisionDate AS STRING), 'NULL'), COALESCE(withdrawnOrTurnedAway, 'NULL'),\n",
					"                    COALESCE(CAST(withdrawnOrTurnedAwayDate AS STRING), 'NULL'), COALESCE(comments, 'NULL'),\n",
					"                    COALESCE(Migrated, 'NULL'), CAST(CURRENT_TIMESTAMP() AS STRING)\n",
					"                )) AS RowID\n",
					"            FROM stg_casework_specialist\n",
					"        \"\"\")\n",
					"        \n",
					"        result[\"new_records\"] = source_count\n",
					"        logInfo(f\"Initial load completed: {source_count} records inserted\")\n",
					"        \n",
					"    else:\n",
					"        # ==========================================\n",
					"        # INCREMENTAL LOAD with SCD Type 2\n",
					"        # ==========================================\n",
					"        logInfo(\"Performing incremental SCD Type 2 load\")\n",
					"        \n",
					"        # Create temp view with current active records and their hashes\n",
					"        spark.sql(f\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW current_active AS\n",
					"            SELECT \n",
					"                casework_specialist_id,\n",
					"                greenCaseId,\n",
					"                processingState,\n",
					"                md5(concat_ws('|',\n",
					"                    COALESCE(greenCaseType, 'NULL'), COALESCE(CAST(greenCaseId AS STRING), 'NULL'),\n",
					"                    COALESCE(caseReference, 'NULL'), COALESCE(horizonId, 'NULL'),\n",
					"                    COALESCE(CAST(linkedGreenCaseId AS STRING), 'NULL'), COALESCE(caseOfficerName, 'NULL'),\n",
					"                    COALESCE(caseOfficerEmail, 'NULL'), COALESCE(appealType, 'NULL'),\n",
					"                    COALESCE(procedure, 'NULL'), COALESCE(processingState, 'NULL'),\n",
					"                    COALESCE(pinsLpaCode, 'NULL'), COALESCE(pinsLpaName, 'NULL'),\n",
					"                    COALESCE(appellantName, 'NULL'), COALESCE(agentName, 'NULL'),\n",
					"                    COALESCE(siteAddressLine1, 'NULL'), COALESCE(siteAddressLine2, 'NULL'),\n",
					"                    COALESCE(siteTownCity, 'NULL'), COALESCE(sitePostcode, 'NULL'),\n",
					"                    COALESCE(otherPartyName, 'NULL'), COALESCE(CAST(receiptDate AS STRING), 'NULL'),\n",
					"                    COALESCE(CAST(validDate AS STRING), 'NULL'), COALESCE(CAST(startDate AS STRING), 'NULL'),\n",
					"                    COALESCE(CAST(lpaQuestionnaireDue AS STRING), 'NULL'), COALESCE(CAST(lpaQuestionnaireReceived AS STRING), 'NULL'),\n",
					"                    COALESCE(CAST(week6Date AS STRING), 'NULL'), COALESCE(CAST(week8Date AS STRING), 'NULL'),\n",
					"                    COALESCE(CAST(week9Date AS STRING), 'NULL'), COALESCE(CAST(eventDate AS STRING), 'NULL'),\n",
					"                    COALESCE(eventTime, 'NULL'), COALESCE(inspectorName, 'NULL'),\n",
					"                    COALESCE(inspectorEmail, 'NULL'), COALESCE(decision, 'NULL'),\n",
					"                    COALESCE(CAST(decisionDate AS STRING), 'NULL'), COALESCE(withdrawnOrTurnedAway, 'NULL'),\n",
					"                    COALESCE(CAST(withdrawnOrTurnedAwayDate AS STRING), 'NULL'), COALESCE(comments, 'NULL'),\n",
					"                    COALESCE(Migrated, 'NULL')\n",
					"                )) as data_hash\n",
					"            FROM {target_table_path}\n",
					"            WHERE is_current = 1\n",
					"        \"\"\")\n",
					"        \n",
					"        # Identify changes using comparison logic\n",
					"        spark.sql(\"\"\"\n",
					"            CREATE OR REPLACE TEMP VIEW change_analysis AS\n",
					"            SELECT \n",
					"                src.greenCaseId,\n",
					"                src.processingState as src_processing_state,\n",
					"                tgt.processingState as tgt_processing_state,\n",
					"                src.data_hash as src_hash,\n",
					"                tgt.data_hash as tgt_hash,\n",
					"                tgt.casework_specialist_id,\n",
					"                CASE \n",
					"                    WHEN tgt.greenCaseId IS NULL THEN 'NEW'\n",
					"                    WHEN src.data_hash != tgt.data_hash THEN 'CHANGED'\n",
					"                    ELSE 'UNCHANGED'\n",
					"                END as change_type\n",
					"            FROM stg_casework_specialist src\n",
					"            LEFT JOIN current_active tgt ON src.greenCaseId = tgt.greenCaseId\n",
					"        \"\"\")\n",
					"        \n",
					"        # Count changes\n",
					"        change_counts = spark.sql(\"\"\"\n",
					"            SELECT \n",
					"                change_type,\n",
					"                COUNT(*) as cnt\n",
					"            FROM change_analysis\n",
					"            GROUP BY change_type\n",
					"        \"\"\").collect()\n",
					"        \n",
					"        for row in change_counts:\n",
					"            if row['change_type'] == 'NEW':\n",
					"                result[\"new_records\"] = row['cnt']\n",
					"            elif row['change_type'] == 'CHANGED':\n",
					"                result[\"updated_records\"] = row['cnt']\n",
					"            elif row['change_type'] == 'UNCHANGED':\n",
					"                result[\"unchanged_records\"] = row['cnt']\n",
					"        \n",
					"        # Count deletions (records in target but not in source)\n",
					"        deleted_count = spark.sql(\"\"\"\n",
					"            SELECT COUNT(*) as cnt\n",
					"            FROM current_active tgt\n",
					"            LEFT JOIN stg_casework_specialist src ON tgt.greenCaseId = src.greenCaseId\n",
					"            WHERE src.greenCaseId IS NULL\n",
					"        \"\"\").collect()[0]['cnt']\n",
					"        \n",
					"        result[\"deleted_records\"] = deleted_count\n",
					"        \n",
					"        logInfo(f\"Change analysis - New: {result['new_records']}, Changed: {result['updated_records']}, Unchanged: {result['unchanged_records']}, Deleted: {result['deleted_records']}\")\n",
					"        \n",
					"        # Log sample of changes for debugging\n",
					"        if result[\"updated_records\"] > 0:\n",
					"            logInfo(\"Sample of changed records:\")\n",
					"            changed_sample = spark.sql(\"\"\"\n",
					"                SELECT \n",
					"                    greenCaseId,\n",
					"                    src_processing_state,\n",
					"                    tgt_processing_state\n",
					"                FROM change_analysis \n",
					"                WHERE change_type = 'CHANGED'\n",
					"                LIMIT 5\n",
					"            \"\"\")\n",
					"            for row in changed_sample.collect():\n",
					"                logInfo(f\"  Case {row['greenCaseId']}: {row['tgt_processing_state']} -> {row['src_processing_state']}\")\n",
					"        \n",
					"        # ==========================================\n",
					"        # STEP 3A: Close changed records\n",
					"        # ==========================================\n",
					"        if result[\"updated_records\"] > 0:\n",
					"            logInfo(f\"Closing {result['updated_records']} changed records\")\n",
					"            \n",
					"            spark.sql(f\"\"\"\n",
					"                UPDATE {target_table_path}\n",
					"                SET record_end_date = CURRENT_TIMESTAMP(),\n",
					"                    is_current = 0\n",
					"                WHERE is_current = 1\n",
					"                AND greenCaseId IN (\n",
					"                    SELECT greenCaseId \n",
					"                    FROM change_analysis \n",
					"                    WHERE change_type = 'CHANGED'\n",
					"                )\n",
					"            \"\"\")\n",
					"        \n",
					"        # ==========================================\n",
					"        # STEP 3B: Close deleted records\n",
					"        # ==========================================\n",
					"        if result[\"deleted_records\"] > 0:\n",
					"            logInfo(f\"Closing {result['deleted_records']} deleted records\")\n",
					"            \n",
					"            spark.sql(f\"\"\"\n",
					"                UPDATE {target_table_path}\n",
					"                SET record_end_date = CURRENT_TIMESTAMP(),\n",
					"                    is_current = 0\n",
					"                WHERE is_current = 1\n",
					"                AND greenCaseId NOT IN (SELECT greenCaseId FROM stg_casework_specialist)\n",
					"            \"\"\")\n",
					"        \n",
					"        # ==========================================\n",
					"        # STEP 3C: Insert new versions of changed records\n",
					"        # ==========================================\n",
					"        if result[\"updated_records\"] > 0:\n",
					"            logInfo(f\"Inserting new versions of {result['updated_records']} changed records\")\n",
					"            \n",
					"            spark.sql(f\"\"\"\n",
					"                INSERT INTO {target_table_path}\n",
					"                SELECT\n",
					"                    ca.casework_specialist_id,\n",
					"                    stg.greenCaseType,\n",
					"                    stg.greenCaseId,\n",
					"                    stg.caseReference,\n",
					"                    stg.horizonId,\n",
					"                    stg.linkedGreenCaseId,\n",
					"                    stg.caseOfficerName,\n",
					"                    stg.caseOfficerEmail,\n",
					"                    stg.appealType,\n",
					"                    stg.procedure,\n",
					"                    stg.processingState,\n",
					"                    stg.pinsLpaCode,\n",
					"                    stg.pinsLpaName,\n",
					"                    stg.appellantName,\n",
					"                    stg.agentName,\n",
					"                    stg.siteAddressLine1,\n",
					"                    stg.siteAddressLine2,\n",
					"                    stg.siteTownCity,\n",
					"                    stg.sitePostcode,\n",
					"                    stg.otherPartyName,\n",
					"                    stg.receiptDate,\n",
					"                    stg.validDate,\n",
					"                    stg.startDate,\n",
					"                    stg.lpaQuestionnaireDue,\n",
					"                    stg.lpaQuestionnaireReceived,\n",
					"                    stg.week6Date,\n",
					"                    stg.week8Date,\n",
					"                    stg.week9Date,\n",
					"                    stg.eventDate,\n",
					"                    stg.eventTime,\n",
					"                    stg.inspectorName,\n",
					"                    stg.inspectorEmail,\n",
					"                    stg.decision,\n",
					"                    stg.decisionDate,\n",
					"                    stg.withdrawnOrTurnedAway,\n",
					"                    stg.withdrawnOrTurnedAwayDate,\n",
					"                    stg.comments,\n",
					"                    stg.Migrated,\n",
					"                    CURRENT_TIMESTAMP() AS IngestionDate,\n",
					"                    CURRENT_TIMESTAMP() AS record_start_date,\n",
					"                    CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS record_end_date,\n",
					"                    1 AS is_current,\n",
					"                    md5(concat_ws('|',\n",
					"                        COALESCE(stg.greenCaseType, 'NULL'), COALESCE(CAST(stg.greenCaseId AS STRING), 'NULL'),\n",
					"                        COALESCE(stg.caseReference, 'NULL'), COALESCE(stg.horizonId, 'NULL'),\n",
					"                        COALESCE(CAST(stg.linkedGreenCaseId AS STRING), 'NULL'), COALESCE(stg.caseOfficerName, 'NULL'),\n",
					"                        COALESCE(stg.caseOfficerEmail, 'NULL'), COALESCE(stg.appealType, 'NULL'),\n",
					"                        COALESCE(stg.procedure, 'NULL'), COALESCE(stg.processingState, 'NULL'),\n",
					"                        COALESCE(stg.pinsLpaCode, 'NULL'), COALESCE(stg.pinsLpaName, 'NULL'),\n",
					"                        COALESCE(stg.appellantName, 'NULL'), COALESCE(stg.agentName, 'NULL'),\n",
					"                        COALESCE(stg.siteAddressLine1, 'NULL'), COALESCE(stg.siteAddressLine2, 'NULL'),\n",
					"                        COALESCE(stg.siteTownCity, 'NULL'), COALESCE(stg.sitePostcode, 'NULL'),\n",
					"                        COALESCE(stg.otherPartyName, 'NULL'), COALESCE(CAST(stg.receiptDate AS STRING), 'NULL'),\n",
					"                        COALESCE(CAST(stg.validDate AS STRING), 'NULL'), COALESCE(CAST(stg.startDate AS STRING), 'NULL'),\n",
					"                        COALESCE(CAST(stg.lpaQuestionnaireDue AS STRING), 'NULL'), COALESCE(CAST(stg.lpaQuestionnaireReceived AS STRING), 'NULL'),\n",
					"                        COALESCE(CAST(stg.week6Date AS STRING), 'NULL'), COALESCE(CAST(stg.week8Date AS STRING), 'NULL'),\n",
					"                        COALESCE(CAST(stg.week9Date AS STRING), 'NULL'), COALESCE(CAST(stg.eventDate AS STRING), 'NULL'),\n",
					"                        COALESCE(stg.eventTime, 'NULL'), COALESCE(stg.inspectorName, 'NULL'),\n",
					"                        COALESCE(stg.inspectorEmail, 'NULL'), COALESCE(stg.decision, 'NULL'),\n",
					"                        COALESCE(CAST(stg.decisionDate AS STRING), 'NULL'), COALESCE(stg.withdrawnOrTurnedAway, 'NULL'),\n",
					"                        COALESCE(CAST(stg.withdrawnOrTurnedAwayDate AS STRING), 'NULL'), COALESCE(stg.comments, 'NULL'),\n",
					"                        COALESCE(stg.Migrated, 'NULL'), CAST(CURRENT_TIMESTAMP() AS STRING)\n",
					"                    )) AS RowID\n",
					"                FROM stg_casework_specialist stg\n",
					"                INNER JOIN change_analysis ca \n",
					"                    ON stg.greenCaseId = ca.greenCaseId \n",
					"                    AND ca.change_type = 'CHANGED'\n",
					"            \"\"\")\n",
					"        \n",
					"        # ==========================================\n",
					"        # STEP 3D: Insert completely new records\n",
					"        # ==========================================\n",
					"        if result[\"new_records\"] > 0:\n",
					"            logInfo(f\"Inserting {result['new_records']} new records\")\n",
					"            \n",
					"            # Get max ID for new records\n",
					"            max_id = spark.sql(f\"SELECT COALESCE(MAX(casework_specialist_id), 0) as max_id FROM {target_table_path}\").collect()[0]['max_id']\n",
					"            \n",
					"            spark.sql(f\"\"\"\n",
					"                INSERT INTO {target_table_path}\n",
					"                SELECT\n",
					"                    ROW_NUMBER() OVER (ORDER BY stg.greenCaseId, stg.receiptDate) + {max_id} AS casework_specialist_id,\n",
					"                    stg.greenCaseType,\n",
					"                    stg.greenCaseId,\n",
					"                    stg.caseReference,\n",
					"                    stg.horizonId,\n",
					"                    stg.linkedGreenCaseId,\n",
					"                    stg.caseOfficerName,\n",
					"                    stg.caseOfficerEmail,\n",
					"                    stg.appealType,\n",
					"                    stg.procedure,\n",
					"                    stg.processingState,\n",
					"                    stg.pinsLpaCode,\n",
					"                    stg.pinsLpaName,\n",
					"                    stg.appellantName,\n",
					"                    stg.agentName,\n",
					"                    stg.siteAddressLine1,\n",
					"                    stg.siteAddressLine2,\n",
					"                    stg.siteTownCity,\n",
					"                    stg.sitePostcode,\n",
					"                    stg.otherPartyName,\n",
					"                    stg.receiptDate,\n",
					"                    stg.validDate,\n",
					"                    stg.startDate,\n",
					"                    stg.lpaQuestionnaireDue,\n",
					"                    stg.lpaQuestionnaireReceived,\n",
					"                    stg.week6Date,\n",
					"                    stg.week8Date,\n",
					"                    stg.week9Date,\n",
					"                    stg.eventDate,\n",
					"                    stg.eventTime,\n",
					"                    stg.inspectorName,\n",
					"                    stg.inspectorEmail,\n",
					"                    stg.decision,\n",
					"                    stg.decisionDate,\n",
					"                    stg.withdrawnOrTurnedAway,\n",
					"                    stg.withdrawnOrTurnedAwayDate,\n",
					"                    stg.comments,\n",
					"                    stg.Migrated,\n",
					"                    CURRENT_TIMESTAMP() AS IngestionDate,\n",
					"                    CURRENT_TIMESTAMP() AS record_start_date,\n",
					"                    CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS record_end_date,\n",
					"                    1 AS is_current,\n",
					"                    md5(concat_ws('|',\n",
					"                        COALESCE(stg.greenCaseType, 'NULL'), COALESCE(CAST(stg.greenCaseId AS STRING), 'NULL'),\n",
					"                        COALESCE(stg.caseReference, 'NULL'), COALESCE(stg.horizonId, 'NULL'),\n",
					"                        COALESCE(CAST(stg.linkedGreenCaseId AS STRING), 'NULL'), COALESCE(stg.caseOfficerName, 'NULL'),\n",
					"                        COALESCE(stg.caseOfficerEmail, 'NULL'), COALESCE(stg.appealType, 'NULL'),\n",
					"                        COALESCE(stg.procedure, 'NULL'), COALESCE(stg.processingState, 'NULL'),\n",
					"                        COALESCE(stg.pinsLpaCode, 'NULL'), COALESCE(stg.pinsLpaName, 'NULL'),\n",
					"                        COALESCE(stg.appellantName, 'NULL'), COALESCE(stg.agentName, 'NULL'),\n",
					"                        COALESCE(stg.siteAddressLine1, 'NULL'), COALESCE(stg.siteAddressLine2, 'NULL'),\n",
					"                        COALESCE(stg.siteTownCity, 'NULL'), COALESCE(stg.sitePostcode, 'NULL'),\n",
					"                        COALESCE(stg.otherPartyName, 'NULL'), COALESCE(CAST(stg.receiptDate AS STRING), 'NULL'),\n",
					"                        COALESCE(CAST(stg.validDate AS STRING), 'NULL'), COALESCE(CAST(stg.startDate AS STRING), 'NULL'),\n",
					"                        COALESCE(CAST(stg.lpaQuestionnaireDue AS STRING), 'NULL'), COALESCE(CAST(stg.lpaQuestionnaireReceived AS STRING), 'NULL'),\n",
					"                        COALESCE(CAST(stg.week6Date AS STRING), 'NULL'), COALESCE(CAST(stg.week8Date AS STRING), 'NULL'),\n",
					"                        COALESCE(CAST(stg.week9Date AS STRING), 'NULL'), COALESCE(CAST(stg.eventDate AS STRING), 'NULL'),\n",
					"                        COALESCE(stg.eventTime, 'NULL'), COALESCE(stg.inspectorName, 'NULL'),\n",
					"                        COALESCE(stg.inspectorEmail, 'NULL'), COALESCE(stg.decision, 'NULL'),\n",
					"                        COALESCE(CAST(stg.decisionDate AS STRING), 'NULL'), COALESCE(stg.withdrawnOrTurnedAway, 'NULL'),\n",
					"                        COALESCE(CAST(stg.withdrawnOrTurnedAwayDate AS STRING), 'NULL'), COALESCE(stg.comments, 'NULL'),\n",
					"                        COALESCE(stg.Migrated, 'NULL'), CAST(CURRENT_TIMESTAMP() AS STRING)\n",
					"                    )) AS RowID\n",
					"                FROM stg_casework_specialist stg\n",
					"                INNER JOIN change_analysis ca \n",
					"                    ON stg.greenCaseId = ca.greenCaseId \n",
					"                    AND ca.change_type = 'NEW'\n",
					"            \"\"\")\n",
					"    \n",
					"    # ==========================================\n",
					"    # STEP 4: Data quality validation\n",
					"    # ==========================================\n",
					"    logInfo(\"Performing data quality checks\")\n",
					"    \n",
					"    # Check for NULL RowIDs\n",
					"    null_rowid_count = spark.sql(f\"SELECT COUNT(*) as count FROM {target_table_path} WHERE RowID IS NULL\").collect()[0]['count']\n",
					"    if null_rowid_count > 0:\n",
					"        logError(f\"Data quality issue: {null_rowid_count} rows have NULL RowID values\")\n",
					"        result[\"status\"] = \"warning\"\n",
					"    \n",
					"    # Check for multiple active records with same business key\n",
					"    duplicate_check = spark.sql(f\"\"\"\n",
					"        SELECT greenCaseId, COUNT(*) as cnt \n",
					"        FROM {target_table_path} \n",
					"        WHERE is_current = 1\n",
					"        GROUP BY greenCaseId \n",
					"        HAVING COUNT(*) > 1\n",
					"    \"\"\")\n",
					"    \n",
					"    duplicate_count = duplicate_check.count()\n",
					"    if duplicate_count > 0:\n",
					"        logError(f\"Data quality issue: {duplicate_count} business keys have multiple active records\")\n",
					"        result[\"status\"] = \"warning\"\n",
					"        duplicate_check.show(10, truncate=False)\n",
					"    else:\n",
					"        logInfo(\"Data quality check passed: No duplicate active records\")\n",
					"    \n",
					"    # Check for records with record_end_date before record_start_date\n",
					"    invalid_dates = spark.sql(f\"\"\"\n",
					"        SELECT COUNT(*) as cnt\n",
					"        FROM {target_table_path}\n",
					"        WHERE record_end_date != CAST('9999-12-31 23:59:59' AS TIMESTAMP)\n",
					"        AND record_end_date < record_start_date\n",
					"    \"\"\").collect()[0]['cnt']\n",
					"    \n",
					"    if invalid_dates > 0:\n",
					"        logError(f\"Data quality issue: {invalid_dates} records have record_end_date before record_start_date\")\n",
					"        result[\"status\"] = \"warning\"\n",
					"    \n",
					"    # Final statistics\n",
					"    total_active = spark.sql(f\"SELECT COUNT(*) as count FROM {target_table_path} WHERE is_current = 1\").collect()[0]['count']\n",
					"    total_historical = spark.sql(f\"SELECT COUNT(*) as count FROM {target_table_path} WHERE is_current = 0\").collect()[0]['count']\n",
					"    total_records = spark.sql(f\"SELECT COUNT(*) as count FROM {target_table_path}\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"SCD Type 2 processing completed successfully\")\n",
					"    logInfo(f\"Final Statistics:\")\n",
					"    logInfo(f\"  - Total records: {total_records}\")\n",
					"    logInfo(f\"  - Active records (is_current=1): {total_active}\")\n",
					"    logInfo(f\"  - Historical records (is_current=0): {total_historical}\")\n",
					"    logInfo(f\"Change Summary:\")\n",
					"    logInfo(f\"  - New: {result['new_records']}\")\n",
					"    logInfo(f\"  - Updated: {result['updated_records']}\")\n",
					"    logInfo(f\"  - Deleted: {result['deleted_records']}\")\n",
					"    logInfo(f\"  - Unchanged: {result['unchanged_records']}\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information\n",
					"    error_msg = f\"Error in SCD Type 2 processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Truncate error message if it exceeds 300 characters\n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}