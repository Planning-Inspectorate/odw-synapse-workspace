{
	"name": "appeals_folder",
	"properties": {
		"folder": {
			"name": "archive/odw-harmonised/DocumentTree"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "94b10e44-7055-468e-a68c-8a26e1fce2be"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import asyncio\n",
					"import nest_asyncio\n",
					"import tracemalloc\n",
					"tracemalloc.start()\n",
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os\n",
					"import re\n",
					"from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat,max as spark_max\n",
					"from pyspark.sql.types import StringType, DateType, TimestampType, IntegerType, FloatType, StructType, StructField\n",
					"from delta import DeltaTable"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialize Application Insight Logging functions\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"# Define variables\n",
					"target_table = 'appeals_folder'\n",
					"\n",
					"# Initialize tracking variables\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Create Table if doesn't exist"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Process appeals_folder data with App Insights logging\n",
					"try:\n",
					"    logInfo('Starting appeals_folder processing')\n",
					"    \n",
					"    db_name: str = 'odw_harmonised_db'\n",
					"    table_name: str = 'appeals_folder'\n",
					"    \n",
					"    def test_table_exists(db_name: str, table_name: str) -> bool:\n",
					"        spark.sql(f\"USE {db_name}\")\n",
					"        tables_df: DataFrame = spark.sql(\"SHOW TABLES\")\n",
					"        table_names: list = [row['tableName'] for row in tables_df.collect()]\n",
					"        return table_name in table_names\n",
					"    \n",
					"    if test_table_exists(db_name, table_name):\n",
					"        logInfo(f'Table {db_name}.{table_name} exists in harmonised, updating the harmonised layer')\n",
					"    else:\n",
					"        logInfo(f'Table {db_name}.{table_name} does not exist, creating table first.')\n",
					"        mssparkutils.notebook.run('/py_odw_harmonised_table_creation',300,{'specific_table': table_name } )\n",
					"        logInfo(f'Table {db_name}.{table_name} created')\n",
					"        \n",
					"except Exception as e:\n",
					"    logInfo(f'Error in table existence check: {str(e)}')\n",
					"    raise"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check for new, updated or deleted data\n",
					"- This script checks for new, updated or deleted data by checking the source data (horizon tables) against the target (odw_harmonised_db.casework tables)\n",
					"- **New Data:** where an main Reference in the source does not exist in the target, then NewData flag is set to 'Y'\n",
					"- **Updated data:** Comparison occurs on Reference Fields in source and in target where the row hash is different i.e. there is a change in one of the columns. NewData flag is set to 'Y'\n",
					"- **Deleted data:** where an Reference info in the target exists but the same identifyers don't exist in the source. DeletedData flag is set to 'Y'\n",
					"\n",
					"## View appeals_folder is created"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Build appeals_folder table\n",
					"-- Gets modified or deleted from source rows\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW appeals_folder_new\n",
					"\n",
					"     AS\n",
					"\n",
					"-- gets data that matches of SourceID and flags that it is modified based on a row (md5) hash. Flags as \"NewData\"\n",
					"-- gets data that is in the target but not in source. Flags as \"DeletedData\"\n",
					"\n",
					"SELECT DISTINCT\n",
					"    CASE\n",
					"        WHEN T1.id IS NULL\n",
					"        THEN T3.HorizonAppealFolderId\n",
					"        ELSE NULL\n",
					"    END                             AS HorizonAppealFolderId,\n",
					"    T1.id                           AS ID,\n",
					"    T1.casereference\t            AS CaseReference,\n",
					"    T1.displaynameenglish\t        AS DisplayNameEnglish,\n",
					"    T1.displaynamewelsh\t            AS DisplayNameWelsh,\n",
					"    T1.parentfolderid\t            AS ParentFolderID,\n",
					"    T1.casenodeid\t                AS CaseNodeId,\n",
					"    T1.casestage\t                AS CaseStage,\n",
					"    T2.SourceSystemID               AS SourceSystemID,\n",
					"    to_timestamp(T1.expected_from)  AS IngestionDate,\n",
					"    NULL                            AS ValidTo,\n",
					"    md5(\n",
					"        concat(\n",
					"            IFNULL(T1.id,'.'),\n",
					"            IFNULL(T1.casereference,'.'),\n",
					"            IFNULL(T1.displaynameenglish,'.'),\n",
					"            IFNULL(T1.displaynamewelsh,'.'),\n",
					"            IFNULL(T1.parentfolderid,'.'),\n",
					"            IFNULL(T1.casenodeid,'.'),\n",
					"            IFNULL(T1.casestage,'.')\n",
					"        ))                          AS RowID, -- this hash should contain all the defining fields\n",
					"    'Y'                             AS IsActive,\n",
					"    T3.IsActive                     AS HistoricIsActive\n",
					"\n",
					"FROM odw_standardised_db.horizon_appeals_folder T1\n",
					"LEFT JOIN odw_harmonised_db.main_sourcesystem_fact T2 \n",
					"    ON \"DocumentTree\" = T2.Description AND \n",
					"        T2.IsActive = 'Y'\n",
					"FULL JOIN odw_harmonised_db.appeals_folder T3 \n",
					"    ON T1.id = T3.ID AND \n",
					"        T3.IsActive = 'Y'\n",
					"WHERE\n",
					"    -- flags new data        \n",
					"    (CASE\n",
					"        WHEN T1.casereference = T3.CaseReference AND md5(\n",
					"            concat(\n",
					"                IFNULL(T1.id,'.'),\n",
					"                IFNULL(T1.casereference,'.'),\n",
					"                IFNULL(T1.displaynameenglish,'.'),\n",
					"                IFNULL(T1.displaynamewelsh,'.'),\n",
					"                IFNULL(T1.parentfolderid,'.'),\n",
					"                IFNULL(T1.casenodeid,'.'),\n",
					"                IFNULL(T1.casestage,'.')\n",
					"            )) <> T3.RowID  -- same record, changed data\n",
					"        THEN 'Y'\n",
					"        WHEN T3.ID IS NULL -- new record\n",
					"        THEN 'Y'\n",
					"    ELSE 'N'\n",
					"    END  = 'Y' )\n",
					"    AND T1.id IS NOT NULL\n",
					"    AND NOT(T1.id = '29309932' AND T1.casestage = 'Initial Documents') --- Hardcoded exception as per Gareth request for data consistency\n",
					"    AND T1.expected_from = (SELECT MAX(expected_from) FROM odw_standardised_db.horizon_appeals_folder)\n",
					";"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Dataset is created that contains changed data and corresponding target data\n",
					"- This script combines data that has been updated, Deleted or is new, with corresponding target data\n",
					"- View **casework_all_appeals_new** is unioned to the target data filter to only those rows where changes have been detected\n",
					"## View appeals_folder_changed_rows is created"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Create new and updated dataset\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW appeals_folder_changed_rows\n",
					"\n",
					"    AS\n",
					"\n",
					"-- gets updated, deleted and new rows \n",
					"SELECT \n",
					"    HorizonAppealFolderId,\n",
					"    ID,\n",
					"    CaseReference,\n",
					"    DisplayNameEnglish,\n",
					"    DisplayNameWelsh,\n",
					"    ParentFolderID,\n",
					"    CaseNodeId,\n",
					"    CaseStage,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"    \n",
					"\n",
					"From appeals_folder_new WHERE HistoricIsActive = 'Y' or HistoricIsActive IS NULL\n",
					"\n",
					"    UNION ALL\n",
					"\n",
					"-- gets original versions of updated rows so we can update EndDate and set IsActive flag to 'N'\n",
					"SELECT\n",
					"    \n",
					"    HorizonAppealFolderId,\n",
					"    ID,\n",
					"    CaseReference,\n",
					"    DisplayNameEnglish,\n",
					"    DisplayNameWelsh,\n",
					"    ParentFolderID,\n",
					"    CaseNodeId,\n",
					"    CaseStage,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"    \n",
					"FROM odw_harmonised_db.appeals_folder\n",
					"WHERE ID IN (SELECT ID FROM appeals_folder_new WHERE HorizonAppealFolderId IS NULL) AND IsActive = 'Y'; "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW Loading_month\n",
					"\n",
					"    AS\n",
					"\n",
					"SELECT DISTINCT\n",
					"    IngestionDate AS IngestionDate,\n",
					"    to_timestamp(date_sub(IngestionDate,1)) AS ClosingDate,\n",
					"    'Y' AS IsActive\n",
					"\n",
					"FROM appeals_folder_new;\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW appeals_folder_changed_rows_final\n",
					"\n",
					"    AS\n",
					"\n",
					"SELECT \n",
					"    HorizonAppealFolderId,\n",
					"    ID,\n",
					"    CaseReference,\n",
					"    DisplayNameEnglish,\n",
					"    DisplayNameWelsh,\n",
					"    ParentFolderID,\n",
					"    CaseNodeId,\n",
					"    CaseStage,\n",
					"    T1.SourceSystemID,\n",
					"    T1.IngestionDate,\n",
					"    T1.ValidTo,\n",
					"    T1.RowID,\n",
					"    T1.IsActive,\n",
					"    T2.ClosingDate\n",
					"FROM appeals_folder_changed_rows T1\n",
					"FULL JOIN Loading_month T2 ON T1.IsActive = T2.IsActive"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# View appeals_folder_changed_rows is used in a merge (Upsert) statement into the target table\n",
					"- **WHEN MATCHED** ON the surrogate Key (i.e. AllAppealsID), EndDate is set to today -1 day and the IsActive flag is set to 'N'\n",
					"- **WHEN NOT MATCHED** ON the surrogate Key, insert rows\n",
					"## Table odw_harmonised_db.appeals_folder is updated"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"update_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) AS update_count\n",
					"    FROM odw_harmonised_db.appeals_folder AS Target\n",
					"    JOIN appeals_folder_changed_rows_final AS Source\n",
					"        ON Source.HorizonAppealFolderId = Target.HorizonAppealFolderId\n",
					"    WHERE Target.IsActive = 'Y'\n",
					"\"\"\").collect()[0][\"update_count\"]\n",
					"print(update_count)\n",
					"insert_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) AS insert_count\n",
					"    FROM appeals_folder_changed_rows_final AS Source\n",
					"    LEFT JOIN odw_harmonised_db.appeals_folder AS Target\n",
					"        ON Source.HorizonAppealFolderId = Target.HorizonAppealFolderId AND Target.IsActive = 'Y'\n",
					"    WHERE Target.HorizonAppealFolderId IS NULL\n",
					"\"\"\").collect()[0][\"insert_count\"]\n",
					"print(insert_count)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- merge into dim table\n",
					"\n",
					"MERGE INTO odw_harmonised_db.appeals_folder AS Target\n",
					"USING appeals_folder_changed_rows_final AS Source\n",
					"\n",
					"ON Source.HorizonAppealFolderId = Target.HorizonAppealFolderId AND Target.IsActive = 'Y'\n",
					"\n",
					"-- For Updates existing rows\n",
					"\n",
					"WHEN MATCHED\n",
					"    THEN \n",
					"    UPDATE SET\n",
					"    Target.ValidTo = to_timestamp(ClosingDate),\n",
					"    Target.IsActive = 'N'\n",
					"\n",
					"-- Insert completely new rows\n",
					"\n",
					"WHEN NOT MATCHED \n",
					"    THEN INSERT (\n",
					"        HorizonAppealFolderId,\n",
					"        ID,\n",
					"        CaseReference,\n",
					"        DisplayNameEnglish,\n",
					"        DisplayNameWelsh,\n",
					"        ParentFolderID,\n",
					"        CaseNodeId,\n",
					"        CaseStage,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive    \n",
					"        )\n",
					"    VALUES (\n",
					"        Source.HorizonAppealFolderId,\n",
					"        Source.ID,\n",
					"        Source.CaseReference,\n",
					"        Source.DisplayNameEnglish,\n",
					"        Source.DisplayNameWelsh,\n",
					"        Source.ParentFolderID,\n",
					"        Source.CaseNodeId,\n",
					"        Source.CaseStage,\n",
					"        Source.SourceSystemID,\n",
					"        Source.IngestionDate,\n",
					"        Source.ValidTo,\n",
					"        Source.RowID,\n",
					"        Source.IsActive)\n",
					"     ;   "
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Fix the IDs\n",
					"- No auto-increment feature is available in delta tables, therefore we need to create new IDs for the inserted rows\n",
					"- This is done by select the target data and using INSERT OVERWRITE to re-insert the data is a new Row Number\n",
					"## Table odw_harmonised_db.appeals_folder is updated"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"-- Insert new appeals_folder\n",
					"\n",
					"INSERT OVERWRITE odw_harmonised_db.appeals_folder\n",
					"\n",
					"SELECT \n",
					"    ROW_NUMBER() OVER (ORDER BY CaseReference NULLS LAST) AS  HorizonAppealFolderId\t,\n",
					"    ID,\n",
					"    CaseReference,\n",
					"    DisplayNameEnglish,\n",
					"    DisplayNameWelsh,\n",
					"    ParentFolderID,\n",
					"    CaseNodeId,\n",
					"    CaseStage,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"FROM odw_harmonised_db.appeals_folder;\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"end_exec_time = datetime.now()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    # Calculate execution duration\n",
					"    \n",
					"    duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					" \n",
					"    # Define activity type\n",
					"    activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					" \n",
					"    # Determine status and message\n",
					"    stage = \"Success\" if not error_message else \"Failed\"\n",
					"    status_message = (\n",
					"        f\"Successfully loaded data into {target_table} table\"\n",
					"        if not error_message\n",
					"        else f\"Failed to load data from {target_table} table\"\n",
					"    )\n",
					"    status_code = \"200\" if stage == \"Success\" else \"500\"\n",
					" \n",
					"    # Prepare telemetry parameters\n",
					"    params = {\n",
					"        \"Stage\": stage,\n",
					"        \"PipelineName\": PipelineName,\n",
					"        \"PipelineRunID\": PipelineRunID,\n",
					"        \"StartTime\": start_exec_time.isoformat(),\n",
					"        \"EndTime\": end_exec_time.isoformat(),\n",
					"        \"Inserts\": insert_count,\n",
					"        \"Updates\": update_count,\n",
					"        \"Deletes\": delete_count,\n",
					"        \"ErrorMessage\": error_message,\n",
					"        \"StatusMessage\": status_message,\n",
					"        \"PipelineTriggerID\": PipelineTriggerID,\n",
					"        \"PipelineTriggerName\": PipelineTriggerName,\n",
					"        \"PipelineTriggerType\": PipelineTriggerType,\n",
					"        \"PipelineTriggeredbyPipelineName\": PipelineTriggeredbyPipelineName,\n",
					"        \"PipelineTriggeredbyPipelineRunID\": PipelineTriggeredbyPipelineRunID,\n",
					"        \"PipelineExecutionTimeInSec\": duration_seconds,\n",
					"        \"ActivityType\": activity_type,\n",
					"        \"DurationSeconds\": duration_seconds,\n",
					"        \"StatusCode\": status_code,\n",
					"        \"AppInsCustomEventName\": \"ODW_Master_Pipeline_Logs\"\n",
					"    }\n",
					" \n",
					"    # Send telemetry asynchronously\n",
					"    send_telemetry_to_app_insights(params)\n",
					" \n",
					"    # Raise error if execution failed\n",
					"    if error_message:\n",
					"        print(f\"Notebook Failed for load {table_name} : {error_message}\")\n",
					"        raise RuntimeError(f\"Notebook Failed due to error in {table_name} Table: {error_message}\")\n",
					" \n",
					"except RuntimeError as e:\n",
					"    print(str(e))\n",
					"    import sys\n",
					"    sys.exit(1)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}