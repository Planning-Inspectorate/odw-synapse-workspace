{
	"name": "Entraid_curated_mipins",
	"properties": {
		"folder": {
			"name": "odw-curated"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2feea64a-8627-4008-85a9-0b1e30c9b968"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"spark = SparkSession.builder.getOrCreate()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"CREATE OR REPLACE VIEW odw_curated_db.vw_Entraid_curated_mipins\r\n",
					"\r\n",
					"AS\r\n",
					"\r\n",
					"SELECT DISTINCT \r\n",
					"CAST(EI.EmployeeEntraId as INT)             AS EmployeeEntraId,\r\n",
					"EI.ID                                        AS ID,\r\n",
					"CAST(EI.employeeId AS INT)                  AS EmployeeID,\r\n",
					"EI.givenName                                AS GivenName,\r\n",
					"EI.surname                                  AS Surname,\r\n",
					"EI.userPrincipalName                        AS UserPrincipalName,              \r\n",
					"EI.Migrated                                 AS Migrated,\r\n",
					"EI.ODTSourceSystem                          AS ODTSourceSystem,\r\n",
					"EI.SourceSystemID                           AS SourceSystemID,\r\n",
					"EI.IngestionDate                            AS IngestionDate,\r\n",
					"EI.ValidTo                                  AS ValidTo,\r\n",
					"EI.RowID                                    AS RowID,\r\n",
					"EI.IsActive                                 AS IsActive\r\n",
					"FROM odw_harmonised_db.entraid AS EI"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"view_df = spark.sql(\"SELECT * FROM odw_curated_db.vw_Entraid_curated_mipins\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import DateType, TimestampType\r\n",
					"\r\n",
					"def apply_ancient_datetime_filter(df, cutoff=\"1900-01-01\"):\r\n",
					"    datetime_cols = [\r\n",
					"        f.name for f in df.schema.fields\r\n",
					"        if isinstance(f.dataType, (TimestampType, DateType))\r\n",
					"    ]\r\n",
					"\r\n",
					"    if not datetime_cols:\r\n",
					"        return df\r\n",
					"\r\n",
					"    filter_expr = \" AND \".join([\r\n",
					"        f\"({c} IS NULL OR {c} >= '{cutoff}')\"\r\n",
					"        for c in datetime_cols\r\n",
					"    ])\r\n",
					"\r\n",
					"    return df.filter(filter_expr)\r\n",
					"\r\n",
					"# Apply before timezone conversion\r\n",
					"filtered_df = apply_ancient_datetime_filter(view_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(f\"drop table if exists odw_curated_db.Entraid_curated_mipins;\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"filtered_df.write.mode(\"overwrite\").saveAsTable(\"odw_curated_db.Entraid_curated_mipins\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#%%sql\r\n",
					"#select * from odw_curated_db.Entraid_curated_mipins"
				],
				"execution_count": null
			}
		]
	}
}