{
	"name": "py_get_delta_table_changes",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0aa45826-f0f1-46d8-90f6-d4b2a71cecd3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"db_name=''\n",
					"table_name=''\n",
					"primary_key=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Pipeline parameters for logging\n",
					"PipelineName=''\n",
					"PipelineRunID=''\n",
					"PipelineTriggerID=''\n",
					"PipelineTriggerName=''\n",
					"PipelineTriggerType=''\n",
					"PipelineTriggeredbyPipelineName=''\n",
					"PipelineTriggeredbyPipelineRunID=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_applicationinsights"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"from delta.tables import DeltaTable\n",
					"from pyspark.sql.functions import col, lit, when, md5, concat_ws, struct, to_json\n",
					"from datetime import datetime\n",
					"import json\n",
					"\n",
					"spark: SparkSession = SparkSession.builder.getOrCreate()\n",
					"storage_account = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"table_name_full = f\"{db_name}.{table_name}\"\n",
					"\n",
					"# Initialize logging variables\n",
					"start_exec_time = datetime.now()\n",
					"error_message = ''\n",
					"changes_count = 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Utility functions\n",
					"\n",
					"`get_delta_table_path`: Gets the location/path of a delta table\n",
					"\n",
					"`get_delta_table_lastest_version`: Gets the int value of the lastes version of a delta table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_delta_table_path(table_name):\n",
					"    table_path = spark.sql(f\"DESCRIBE DETAIL {table_name}\").select(\"location\").first()[\"location\"]\n",
					"    return table_path\n",
					"    \n",
					"def get_delta_table_lastest_version(table_path):\n",
					"    delta_table = DeltaTable.forPath(spark, table_path)\n",
					"    history_df = delta_table.history()\n",
					"    version = history_df.select(\"version\").orderBy(\"version\", ascending=False).first()[\"version\"]\n",
					"    return version"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"table_path: str = get_delta_table_path(table_name_full)\n",
					"latest_version: int = get_delta_table_lastest_version(table_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# Main processing logic\n",
					"try:\n",
					"    previous_version = latest_version - 1\n",
					"\n",
					"    # Check if this is version 0 (initial table creation)\n",
					"    if previous_version < 1:\n",
					"        try:\n",
					"            print(f\"Table is at version 0. Treating all records as Create events for delta table: {table_name_full}\")\n",
					"        except Exception as e:\n",
					"            print(f\"Failed to log info: {e}\")\n",
					"        \n",
					"        # Load version 0 and mark all records as Create\n",
					"        current_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version).load(table_path)\n",
					"        changes_df = current_version_df.withColumn(\"EventType\", lit(\"Create\"))\n",
					"        \n",
					"        new_count = changes_df.count()\n",
					"        try:\n",
					"            print(f\"Found {new_count} new records in version {latest_version} of delta table: {table_name_full}\")\n",
					"        except Exception as e:\n",
					"            print(f\"Failed to log info: {e}\")\n",
					"    else:\n",
					"        try:\n",
					"            print(f\"Checking for CDF (Change Data Feed) capability on delta table: {table_name_full}\")\n",
					"        except Exception as e:\n",
					"            print(f\"Failed to log info: {e}\")\n",
					"        \n",
					"        # Check if Change Data Feed is enabled\n",
					"        delta_table = DeltaTable.forPath(spark, table_path)\n",
					"        table_properties = spark.sql(f\"SHOW TBLPROPERTIES {table_name_full}\").collect()\n",
					"        cdf_enabled = any(prop.key == 'delta.enableChangeDataFeed' and prop.value == 'true' for prop in table_properties)\n",
					"        \n",
					"        if cdf_enabled:\n",
					"            # Use native CDF - most efficient and accurate\n",
					"            try:\n",
					"                print(f\"Using Change Data Feed to read changes between version {latest_version} and {previous_version}\")\n",
					"            except Exception as e:\n",
					"                print(f\"Failed to log info: {e}\")\n",
					"            \n",
					"            changes_df = spark.read.format(\"delta\") \\\n",
					"                .option(\"readChangeFeed\", \"true\") \\\n",
					"                .option(\"startingVersion\", previous_version + 1) \\\n",
					"                .option(\"endingVersion\", latest_version) \\\n",
					"                .load(table_path)\n",
					"            \n",
					"            # Map CDF operation types to our EventType\n",
					"            # CDF operations: insert, update_preimage, update_postimage, delete\n",
					"            changes_df = changes_df.withColumn(\n",
					"                \"EventType\",\n",
					"                when(col(\"_change_type\") == \"insert\", lit(\"Create\"))\n",
					"                .when(col(\"_change_type\") == \"update_postimage\", lit(\"Update\"))\n",
					"                .when(col(\"_change_type\") == \"delete\", lit(\"Delete\"))\n",
					"                .otherwise(lit(None))\n",
					"            ).filter(col(\"EventType\").isNotNull()) \\\n",
					"             .drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\")\n",
					"            \n",
					"            changes_count = changes_df.count()\n",
					"            try:\n",
					"                print(f\"Found {changes_count} changes using CDF between version {latest_version} and {previous_version} in delta table: {table_name_full}\")\n",
					"            except Exception as e:\n",
					"                print(f\"Failed to log info: {e}\")\n",
					"        else:\n",
					"            # Fallback: Compare versions using hash-based approach\n",
					"            try:\n",
					"                print(f\"CDF not enabled. Using hash-based comparison between version {latest_version} and {previous_version}\")\n",
					"            except Exception as e:\n",
					"                print(f\"Failed to log info: {e}\")\n",
					"            \n",
					"            current_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version).load(table_path)\n",
					"            previous_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", previous_version).load(table_path)\n",
					"            \n",
					"            all_columns = current_version_df.columns\n",
					"            \n",
					"            # Create hash of entire row to handle complex types\n",
					"            current_with_hash = current_version_df.withColumn(\n",
					"                \"_row_hash\",\n",
					"                md5(to_json(struct(*all_columns)))\n",
					"            ).select(col(primary_key).cast(\"string\").alias(\"_pk\"), col(\"_row_hash\"))\n",
					"            \n",
					"            previous_with_hash = previous_version_df.withColumn(\n",
					"                \"_row_hash\",\n",
					"                md5(to_json(struct(*all_columns)))\n",
					"            ).select(col(primary_key).cast(\"string\").alias(\"_pk\"), col(\"_row_hash\"))\n",
					"            \n",
					"            # Find new records\n",
					"            create_keys = current_with_hash.select(\"_pk\").subtract(previous_with_hash.select(\"_pk\"))\n",
					"            create_df = current_version_df.join(create_keys, col(primary_key).cast(\"string\") == col(\"_pk\"), \"inner\").drop(\"_pk\").withColumn(\"EventType\", lit(\"Create\"))\n",
					"            \n",
					"            # Find deleted records\n",
					"            delete_keys = previous_with_hash.select(\"_pk\").subtract(current_with_hash.select(\"_pk\"))\n",
					"            delete_df = previous_version_df.join(delete_keys, col(primary_key).cast(\"string\") == col(\"_pk\"), \"inner\").drop(\"_pk\").withColumn(\"EventType\", lit(\"Delete\"))\n",
					"            \n",
					"            # Find updated records (same key, different hash)\n",
					"            common_keys = current_with_hash.select(\"_pk\").intersect(previous_with_hash.select(\"_pk\"))\n",
					"            current_common = current_with_hash.join(common_keys, \"_pk\", \"inner\")\n",
					"            previous_common = previous_with_hash.join(common_keys, \"_pk\", \"inner\")\n",
					"            update_keys = current_common.subtract(previous_common).select(\"_pk\")\n",
					"            update_df = current_version_df.join(update_keys, col(primary_key).cast(\"string\") == col(\"_pk\"), \"inner\").drop(\"_pk\").withColumn(\"EventType\", lit(\"Update\"))\n",
					"            \n",
					"            # Union all changes\n",
					"            changes_df = create_df.union(update_df).union(delete_df)\n",
					"            \n",
					"            changes_count = changes_df.count()\n",
					"            try:\n",
					"                print(f\"Found {changes_count} changes between version {latest_version} and {previous_version} in delta table: {table_name_full}\")\n",
					"            except Exception as e:\n",
					"                print(f\"Failed to log info: {e}\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_message = f\"Failed to read changes in delta table: {table_name_full}. Exception: {str(e)[:800]}\"\n",
					"    try:\n",
					"        print(error_message)\n",
					"    except Exception as log_e:\n",
					"        print(f\"Failed to log error: {log_e}\")\n",
					"    import traceback\n",
					"    try:\n",
					"        print(traceback.format_exc())\n",
					"    except Exception as log_e:\n",
					"        print(f\"Failed to log traceback: {log_e}\")\n",
					"    end_exec_time = datetime.now()\n",
					"\n",
					"    # Log failure to Application Insights\n",
					"    duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"    activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"    stage = \"Failed\"\n",
					"    status_message = f\"Failed to track changes for {table_name_full}\"\n",
					"    status_code = \"500\"\n",
					"    \n",
					"    log_telemetry_and_exit(\n",
					"        stage,\n",
					"        start_exec_time,\n",
					"        end_exec_time,\n",
					"        error_message,\n",
					"        table_name_full,\n",
					"        0,  # insert_count\n",
					"        0,  # update_count\n",
					"        0,  # delete_count\n",
					"        PipelineName,\n",
					"        PipelineRunID,\n",
					"        PipelineTriggerID,\n",
					"        PipelineTriggerName,\n",
					"        PipelineTriggerType,\n",
					"        PipelineTriggeredbyPipelineName,\n",
					"        PipelineTriggeredbyPipelineRunID,\n",
					"        activity_type,\n",
					"        duration_seconds,\n",
					"        status_message,\n",
					"        status_code\n",
					"    )"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check if there are any changes\n",
					"changes_count = changes_df.count()\n",
					"end_exec_time = datetime.now()\n",
					"\n",
					"if changes_count == 0:\n",
					"    try:\n",
					"        print(f\"No changes detected in delta table: {table_name_full}\")\n",
					"    except Exception as e:\n",
					"        print(f\"Failed to log info: {e}\")\n",
					"    \n",
					"    # Log success to Application Insights\n",
					"    duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"    activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"    stage = \"Success\"\n",
					"    status_message = f\"No changes detected in {table_name_full}\"\n",
					"    status_code = \"200\"\n",
					"    \n",
					"    log_telemetry_and_exit(\n",
					"        stage,\n",
					"        start_exec_time,\n",
					"        end_exec_time,\n",
					"        error_message,\n",
					"        table_name_full,\n",
					"        0,  # insert_count\n",
					"        0,  # update_count\n",
					"        0,  # delete_count\n",
					"        PipelineName,\n",
					"        PipelineRunID,\n",
					"        PipelineTriggerID,\n",
					"        PipelineTriggerName,\n",
					"        PipelineTriggerType,\n",
					"        PipelineTriggeredbyPipelineName,\n",
					"        PipelineTriggeredbyPipelineRunID,\n",
					"        activity_type,\n",
					"        duration_seconds,\n",
					"        status_message,\n",
					"        status_code\n",
					"    )"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Process changes in batches to avoid Service Bus message size limit\n",
					"# Azure Service Bus has a 256KB limit for standard tier (1MB for premium tier)\n",
					"# Use Spark partitioning for efficient batching\n",
					"\n",
					"from pyspark.sql.functions import create_map, lit, spark_partition_id\n",
					"from math import ceil\n",
					"\n",
					"BATCH_SIZE = 100  # Process 100 messages per batch\n",
					"target_folder = f\"abfss://odw-curated@{storage_account}/{table_name}/curated_to_sb\"\n",
					"\n",
					"# Clean up any existing files in the target folder\n",
					"try:\n",
					"    mssparkutils.fs.rm(target_folder, True)\n",
					"except Exception:\n",
					"    pass  # Folder might not exist\n",
					"\n",
					"mssparkutils.fs.mkdirs(target_folder)\n",
					"\n",
					"# Calculate number of partitions based on changes count and batch size\n",
					"num_partitions = max(1, ceil(changes_count / BATCH_SIZE))\n",
					"\n",
					"# Get all columns except EventType for the Body\n",
					"data_columns = [c for c in changes_df.columns if c != 'EventType']\n",
					"\n",
					"# Create the message structure with Body and UserProperties\n",
					"messages_df = changes_df.withColumn(\n",
					"    \"message\",\n",
					"    struct(\n",
					"        to_json(struct(*data_columns)).alias(\"Body\"),\n",
					"        create_map(lit(\"type\"), col(\"EventType\")).alias(\"UserProperties\")\n",
					"    )\n",
					").select(\"message\")\n",
					"\n",
					"# Repartition to create batches - Spark will distribute rows across partitions\n",
					"messages_partitioned = messages_df.repartition(num_partitions)\n",
					"\n",
					"# Add partition ID for grouping - each partition becomes a batch\n",
					"from pyspark.sql.functions import collect_list\n",
					"messages_with_partition = messages_partitioned.withColumn(\"partition_id\", spark_partition_id())\n",
					"\n",
					"# Group by partition to collect messages into arrays\n",
					"batches_df = messages_with_partition.groupBy(\"partition_id\").agg(\n",
					"    collect_list(\"message\").alias(\"messages\")\n",
					")\n",
					"\n",
					"# Convert to JSON\n",
					"batches_with_json = batches_df.withColumn(\n",
					"    \"json_content\",\n",
					"    to_json(col(\"messages\"))\n",
					").select(\"partition_id\", \"json_content\")\n",
					"\n",
					"# Write files using Spark - fully distributed\n",
					"temp_path = f\"{target_folder}_temp\"\n",
					"batches_with_json.write.mode(\"overwrite\").json(temp_path)\n",
					"\n",
					"# Read back and rename files to proper format\n",
					"temp_files = mssparkutils.fs.ls(temp_path)\n",
					"json_files = [f for f in temp_files if f.name.endswith('.json')]\n",
					"batch_count = len(json_files)\n",
					"\n",
					"for idx, file_info in enumerate(json_files):\n",
					"    # Read the JSON file\n",
					"    content = mssparkutils.fs.head(file_info.path, 100000000)  # Read up to 100MB\n",
					"    parsed = json.loads(content)\n",
					"    \n",
					"    # Extract partition_id and json_content\n",
					"    partition_id = parsed['partition_id']\n",
					"    json_content = parsed['json_content']\n",
					"    \n",
					"    # Write to final location with proper naming\n",
					"    target_file = f\"{target_folder}/sb_message_batch_{partition_id:05d}.json\"\n",
					"    mssparkutils.fs.put(target_file, json_content, True)\n",
					"    \n",
					"    try:\n",
					"        messages_in_batch = len(json.loads(json_content))\n",
					"        print(f\"Wrote batch {partition_id} with {messages_in_batch} messages\")\n",
					"    except Exception as e:\n",
					"        print(f\"Failed to log info: {e}\")\n",
					"\n",
					"# Clean up temp folder\n",
					"mssparkutils.fs.rm(temp_path, True)\n",
					"\n",
					"try:\n",
					"    print(f\"Successfully wrote {changes_count} changes across {batch_count} batch files\")\n",
					"except Exception as e:\n",
					"    print(f\"Failed to log info: {e}\")\n",
					"\n",
					"# Log success to Application Insights\n",
					"duration_seconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"activity_type = f\"{mssparkutils.runtime.context['currentNotebookName']} Notebook\"\n",
					"stage = \"Success\"\n",
					"status_message = f\"Successfully tracked {changes_count} changes for {table_name_full} in {batch_count} batches\"\n",
					"status_code = \"200\"\n",
					"\n",
					"log_telemetry_and_exit(\n",
					"    stage,\n",
					"    start_exec_time,\n",
					"    end_exec_time,\n",
					"    error_message,\n",
					"    table_name_full,\n",
					"    changes_count,  # insert_count (total changes processed)\n",
					"    0,  # update_count\n",
					"    0,  # delete_count\n",
					"    PipelineName,\n",
					"    PipelineRunID,\n",
					"    PipelineTriggerID,\n",
					"    PipelineTriggerName,\n",
					"    PipelineTriggerType,\n",
					"    PipelineTriggeredbyPipelineName,\n",
					"    PipelineTriggeredbyPipelineRunID,\n",
					"    activity_type,\n",
					"    duration_seconds,\n",
					"    status_message,\n",
					"    status_code\n",
					")"
				],
				"execution_count": null
			}
		]
	}
}