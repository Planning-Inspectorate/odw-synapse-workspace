{
	"name": "Load Test Cleanup",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "bc84bc21-757b-45e9-88a7-e17e7733ef7d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# S78/HAS Load Test Cleanup \n",
					"import uuid\n",
					"from datetime import datetime"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"DRY_RUN = True  #set False to actually apply changes\n",
					"\n",
					"STD_DB  = \"odw_standardised_db\"\n",
					"HARM_DB = \"odw_harmonised_db\"\n",
					"CUR_DB  = \"odw_curated_db\"\n",
					"\n",
					"LPA_CODE = \"LOAD1\"\n",
					"ENTITIES = [\"s78\", \"has\"]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"PATTERNS = [\n",
					"    (STD_DB,  \"sb_appeal_{entity}\"),\n",
					"    (HARM_DB, \"sb_appeal_{entity}\"),\n",
					"    (HARM_DB, \"appeal_{entity}\"),\n",
					"    (CUR_DB,  \"appeal_{entity}\"),\n",
					"    (CUR_DB,  \"appeals_{entity}_curated_mipins\"),\n",
					"]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"TABLES = [(db, p.format(entity=e)) for e in ENTITIES for db, p in PATTERNS]\n",
					"TABLES.append((CUR_DB, \"dart_api\"))\n",
					"\n",
					"def table_exists(db, table):\n",
					"    try:\n",
					"        spark.sql(f\"DESCRIBE TABLE {db}.{table}\")\n",
					"        return True\n",
					"    except Exception as e:\n",
					"        print(f\"[WARN] Missing/inaccessible table: {db}.{table} ({e})\")\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_load1_input_files(std_db, table, lpa_code, limit=50):\n",
					"    full = f\"{std_db}.{table}\"\n",
					"    query = f\"\"\"\n",
					"    SELECT input_file, COUNT(*) AS count\n",
					"    FROM {full}\n",
					"    WHERE lpaCode = '{lpa_code}'\n",
					"    GROUP BY input_file\n",
					"    ORDER BY count DESC\n",
					"    LIMIT {limit}\n",
					"    \"\"\"\n",
					"    return spark.sql(query)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def delete_abfss_file(path: str, dry_run: bool = True) -> bool:\n",
					"    dir_path = path.rsplit(\"/\", 1)[0] + \"/\"\n",
					"    filename = path.rsplit(\"/\", 1)[1]\n",
					"\n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(dir_path)\n",
					"        matches = [i.path for i in items if (not i.isDir) and i.path.endswith(\"/\" + filename)]\n",
					"\n",
					"        if not matches:\n",
					"            print(f\"[SKIP] Not found in directory listing: {path}\")\n",
					"            return False\n",
					"\n",
					"        actual_path = matches[0]\n",
					"\n",
					"        if dry_run:\n",
					"            print(f\"[DRY-RUN] Would delete: {actual_path}\")\n",
					"            return True\n",
					"\n",
					"        mssparkutils.fs.rm(actual_path, True)\n",
					"        print(f\"[DELETED] {actual_path}\")\n",
					"        return True\n",
					"\n",
					"    except Exception as e:\n",
					"        print(f\"[WARN] Could not access/delete: {path} | Error: {e}\")\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def cleanup_raw_files_from_input_file(std_db, table, lpa_code, dry_run=True, limit=50):\n",
					"    print(f\"RAW file check: {std_db}.{table}\")\n",
					"    print(f\"Predicate: lpaCode = '{lpa_code}'\")\n",
					"\n",
					"    df = get_load1_input_files(std_db, table, lpa_code, limit=limit)\n",
					"    rows = df.collect()\n",
					"\n",
					"    if len(rows) == 0:\n",
					"        print(\"[OK] No matching rows found, so no raw files to delete.\")\n",
					"        return []\n",
					"\n",
					"    print(f\"[INFO] Found {len(rows)} raw files referenced (top {limit})\")\n",
					"    df.show(limit, truncate=False)\n",
					"\n",
					"    deleted_files = []\n",
					"    for r in rows:\n",
					"        file_path = r[\"input_file\"]\n",
					"        if delete_abfss_file(file_path, dry_run=dry_run):\n",
					"            deleted_files.append(file_path)\n",
					"\n",
					"    return deleted_files\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def list_parquet_files(path: str):\n",
					"    files, stack = [], [path.rstrip(\"/\") + \"/\"]\n",
					"    while stack:\n",
					"        p = stack.pop()\n",
					"        try:\n",
					"            items = mssparkutils.fs.ls(p)\n",
					"        except Exception:\n",
					"            continue\n",
					"        for it in items:\n",
					"            if it.isDir:\n",
					"                stack.append(it.path)\n",
					"            elif it.path.lower().endswith(\".parquet\"):\n",
					"                files.append(it.path)\n",
					"    return files"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def clean_std_harm(db, table, lpa_code):\n",
					"    full = f\"{db}.{table}\"\n",
					"\n",
					"    count = spark.sql(\n",
					"        f\"SELECT COUNT(*) AS count FROM {full} WHERE lpaCode = '{lpa_code}'\"\n",
					"    ).collect()[0][\"count\"]\n",
					"\n",
					"    if count == 0:\n",
					"        print(f\"[SKIP] {full} has 0 matching rows\")\n",
					"        return 0\n",
					"\n",
					"    if DRY_RUN:\n",
					"        print(f\"[DRY-RUN] Would DELETE {count} rows from {full} WHERE lpaCode = '{lpa_code}'\")\n",
					"        return count\n",
					"\n",
					"    spark.sql(f\"DELETE FROM {full} WHERE lpaCode = '{lpa_code}'\")\n",
					"    print(f\"[DELETED] {count} rows from {full}\")\n",
					"    return count"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def clean_curated(db, table, lpa_code):\n",
					"    full = f\"{db}.{table}\"\n",
					"\n",
					"    try:\n",
					"        spark.sql(f\"REFRESH TABLE {full}\")\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"    details = spark.sql(f\"DESCRIBE DETAIL {full}\").collect()[0].asDict()\n",
					"    base_path = (details.get(\"location\") or \"\").rstrip(\"/\")\n",
					"\n",
					"    if not base_path:\n",
					"        raise Exception(f\"No LOCATION found for {full}\")\n",
					"\n",
					"    schema = spark.table(full).schema\n",
					"\n",
					"    if len(list_parquet_files(base_path)) == 0:\n",
					"        print(f\"[SKIP] {full} has no parquet files under location\")\n",
					"        return 0\n",
					"\n",
					"    df = (spark.read.format(\"parquet\")\n",
					"          .schema(schema)\n",
					"          .option(\"recursiveFileLookup\", \"true\")\n",
					"          .load(base_path))\n",
					"\n",
					"    count = df.filter(f\"lpaCode = '{lpa_code}'\").count()\n",
					"\n",
					"    if count == 0:\n",
					"        print(f\"[SKIP] {full} has 0 matching rows\")\n",
					"        return 0\n",
					"\n",
					"    if DRY_RUN:\n",
					"        print(f\"[DRY-RUN] Would remove {count} rows from {full} (overwrite-by-location)\")\n",
					"        return count\n",
					"\n",
					"    run_id = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") + \"_\" + uuid.uuid4().hex[:8]\n",
					"    tmp_path = f\"{base_path}__tmp_{run_id}\"\n",
					"\n",
					"    (df.filter(f\"lpaCode <> '{lpa_code}'\")\n",
					"       .write.format(\"parquet\")\n",
					"       .mode(\"overwrite\")\n",
					"       .save(tmp_path))\n",
					"\n",
					"    if mssparkutils.fs.exists(base_path):\n",
					"        mssparkutils.fs.rm(base_path, True)\n",
					"    mssparkutils.fs.mv(tmp_path, base_path, True)\n",
					"\n",
					"    try:\n",
					"        spark.sql(f\"REFRESH TABLE {full}\")\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"    print(f\"[OVERWRITTEN] {full} cleaned (removed {count} rows)\")\n",
					"    return count"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(\"S78/HAS Cleanup\")\n",
					"print(f\"Mode: {'DRY-RUN' if DRY_RUN else 'EXECUTE'}  |  lpaCode = '{LPA_CODE}'\\n\")\n",
					"\n",
					"deleted_raw = {}\n",
					"\n",
					"for std_table in [\"sb_appeal_s78\", \"sb_appeal_has\"]:\n",
					"    if table_exists(STD_DB, std_table):\n",
					"        deleted_raw[std_table] = cleanup_raw_files_from_input_file(\n",
					"            STD_DB, std_table, LPA_CODE, dry_run=DRY_RUN, limit=50\n",
					"        )\n",
					"\n",
					"print(\"RAW FILE DELETE SUMMARY\")\n",
					"for t, files in deleted_raw.items():\n",
					"    print(f\"{STD_DB}.{t}: {len(files)} file(s) deleted (or would delete)\")\n",
					"\n",
					"\n",
					"results = {}\n",
					"\n",
					"for db, table in TABLES:\n",
					"    full = f\"{db}.{table}\"\n",
					"\n",
					"    if not table_exists(db, table):\n",
					"        continue\n",
					"    if db == CUR_DB and table == \"dart_api\":\n",
					"        removed = clean_std_harm(db, table, LPA_CODE)\n",
					"    elif db == CUR_DB:\n",
					"        removed = clean_curated(db, table, LPA_CODE)\n",
					"    else:\n",
					"        removed = clean_std_harm(db, table, LPA_CODE)\n",
					"\n",
					"    results[full] = removed\n",
					"\n",
					"print(\"SUMMARY\")\n",
					"for k, v in results.items():\n",
					"    print(f\"{k}: {v}\")"
				],
				"execution_count": null
			}
		]
	}
}